\documentclass{article}

\usepackage{AISTATS2023_Author_Response_Pack/aistats2023_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % define colors in text
\usepackage{xspace}         % fix spacing around commands

\begin{document}

This is the author response for ``Universal Agent Mixtures and the Geometry of Intelligence''
(submission 638).

\paragraph{W.r.t. Reviewer 4 (``Accept'')}

Thank you for your insightful review. We particularly appreciate your note about the \emph{agent} vs.\ \emph{policy} terminology, and we will add some text to clarify.

\paragraph{W.r.t. Reviewer 3 (``Accept'')}

Thank you for this detailed review. We particularly appreciate your suggestion to use Bayesian priors to further illuminate the main definition. Re: Proposition 39: thank you, you're right, in Definition 38 the "either...or" ought to be "both...and".

\paragraph{W.r.t. Reviewer 2 (``Marginally below'') and Reviewer 1 (``Reject'')}

Thank you for your constructive reviews. You make a great point that our introduction/conclusion would
be improved by more elaboration on the specific results that we later prove, as well as
about how these theoretical results are actually applicable.

While we were limited in space laying out the impact and
applications
of our results, we plan to fully articulate the additional page
allowed in the camera-ready version for this purpose - and of course refining notations as suggested by Reviewer 2.

On a higher level our results are intended as a step towards more provable behaviors in reinforcement learning,
paying a tribute to its root from control theory where system stability (and in a more pragmatic sense,
both safety and guaranteed performance)
is paramount.
In our case, we have proven that reinforcement agents collaborating
in a specific way will \emph{not} exhibit emergent behavior, which
can be regarded as a certain theoretical safety guarantee where emergent behavior might be undesirable.
Due to its generalized nature, our results not only apply to artificial intelligence,
but potentially also agent-based modeling of
systems involving human behaviour such as \cite{Bouarfa2013AgentbasedMA}.
For example, corruption in organizational behavior can be considered as an unwanted emergent ``bartering'' behavior. 

%We intend to add such
%language, which will greatly improve the paper.
Moreover, with the additional page, we would like to point out the following applications of individual results.
%the introduction
%about applications:
\begin{itemize}
    \item
    By guaranteeing that ``the expected reward of a weighted mixture is the weighted
    average of the expected rewards,'' Theorem 14 offers an application to AI safety.
    Namely: it provides a way to mix reinforcement learning agents without the risk of
    unforeseen side-effects. For example, if several agents have different weaknesses,
    then, a priori, one might worry that when those agents are combined, those weaknesses
    might compound each other, leading to a combined weakness larger than the sum of the
    individual weaknesses; Theorem 14
    guarantees this does not occur with our agent mixtures.
    \item
    We already point out in Section 4 (Equivalence of Weak and Strong Symmetry)
    that the results there
    have implications for the search for inherently desirable properties of universal Turing
    machines. We will put this more front-and-center by adding language to that effect in the
    Introduction/Conclusion.
    \item
    Section 5 (Discernability and Separability) has applications for determining what
    sorts of things can or cannot be incentivized by reinforcement learning, in a formal sense 
    (analogous to how the Pumping Lemma can be used to show that, e.g., certain
    languages are not regular). Namely: if a set of agents does not satisfy the
    closure properties in this
    section, then it is hopeless to try to engineer an RL environment that ultimately rewards
    exactly that set of agents while punishing all other agents.
    \item
    Section 6 (Local Extrema and Lattice Points) has applications for the search for
    optimal reinforcement learning agents. It shows that the search space can be
    considerably narrowed down by restricting attention to deterministic agents. In concrete
    terms, the work here shows that nothing is gained by allowing agents to invoke
    genuine random number generators, e.g.\ expensive RNGs based on quantum mechanics etc.
\end{itemize}



\bibliographystyle{apalike} % AISTATS 
\bibliography{main}

\end{document}
