@article{silver2021reward,
  title={Reward is enough},
  author={Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard},
  journal={Artificial Intelligence},
  year={2021},
  publisher={Elsevier}
}

@conference{RLvsRL,
  title={Can reinforcement learning learn itself? {A} reply to `{R}eward is enough'},
  year={2021},
  author={Alexander, Samuel Allen},
  booktitle={CIFMA}
}

@inproceedings{alexander2021reward,
  title={Reward-punishment symmetric universal intelligence},
  author={Alexander, Samuel Allen and Hutter, Marcus},
  booktitle={AGI},
  year={2021},
}

@article{legg2007universal,
  title={Universal intelligence: A definition of machine intelligence},
  author={Legg, Shane and Hutter, Marcus},
  journal={Minds and machines},
  volume={17},
  number={4},
  pages={391--444},
  year={2007},
  publisher={Springer}
}

@article{hutter2009discrete,
  title={Discrete {MDL} predicts in total variation},
  author={Hutter, Marcus},
  journal={Advances in Neural Information Processing Systems},
  volume={22},
  year={2009}
}

@article{extendedenvironmentspaper,
  title={Extending Environments To Measure Self-Reflection In Reinforcement Learning},
  author={Alexander, Samuel Allen and Castaneda, Michael and Compher, Kevin
    and Martinez, Oscar},
  journal={Preprint},
  year={2022}
}

@inproceedings{leike2015bad,
  title={Bad universal priors and notions of optimality},
  author={Leike, Jan and Hutter, Marcus},
  booktitle={Conference on Learning Theory},
  pages={1244--1259},
  year={2015},
  organization={PMLR}
}

@book{li2008introduction,
  title={An introduction to Kolmogorov complexity and its applications},
  author={Li, Ming and Vit{\'a}nyi, Paul},
  year={2008},
  publisher={Springer}
}

@article{muller2010stationary,
  title={Stationary algorithmic probability},
  author={M{\"u}ller, Markus},
  journal={Theoretical Computer Science},
  volume={411},
  number={1},
  pages={113--130},
  year={2010},
  publisher={Elsevier}
}

@article{nature-sortition,
author = {Flanigan, Bailey and Gölz, Paul and Gupta, Anupam and Hennig, Brett and Procaccia, Ariel},
year = {2021},
month = {08},
pages = {},
title = {Fair algorithms for selecting citizens’ assemblies},
volume = {596},
journal = {Nature},
doi = {10.1038/s41586-021-03788-6}
}

@article{nature-crowd,
author = {Prelec, Dražen and Seung, Hyunjune and McCoy, John},
year = {2017},
month = {01},
pages = {532-535},
title = {A solution to the single-question crowd wisdom problem},
volume = {541},
journal = {Nature},
doi = {10.1038/nature21054}
}

@article{nature-starcraft,
author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John and Jaderberg, Max and Silver, David},
year = {2019},
month = {11},
pages = {},
title = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
volume = {575},
journal = {Nature},
doi = {10.1038/s41586-019-1724-z}
}

@article{nature-ATARI,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei and Veness, Joel and Bellemare, Marc and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
year = {2015},
month = {02},
pages = {529-33},
title = {Human-level control through deep reinforcement learning},
volume = {518},
journal = {Nature},
doi = {10.1038/nature14236}
}

@article{nature-go-2,
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Driessche, George and Graepel, Thore and Hassabis, Demis},
year = {2017},
month = {10},
pages = {354-359},
title = {Mastering the game of Go without human knowledge},
volume = {550},
journal = {Nature},
doi = {10.1038/nature24270}
}

@inproceedings{unified-multiagent,
  title={A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning},
  author={Marc Lanctot and Vin{\'i}cius Flores Zambaldi and Audrunas Gruslys and Angeliki Lazaridou and Karl Tuyls and Julien P{\'e}rolat and David Silver and Thore Graepel},
  booktitle={NIPS},
  year={2017}
}

@inproceedings{multiagent-2nd-cited-1998,
  title={Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm},
  author={Junling Hu and Michael P. Wellman},
  booktitle={ICML},
  year={1998}
}
￼
@inproceedings{multiagent-most-cited-1998-cooperative,
  title={The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems},
  author={Caroline Claus and Craig Boutilier},
  booktitle={AAAI/IAAI},
  year={1998}
}

@article{collective-1993,
  title={Collective learning of action sequences},
  author={Gerhard Weiss},
  journal={[1993] Proceedings. The 13th International Conference on Distributed Computing Systems},
  year={1993},
  pages={203-209}
}

@inproceedings{multiagent-1994-most-cited,
  title={Markov Games as a Framework for Multi-Agent Reinforcement Learning},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}

@inproceedings{filler,
  title={Test filler: Does unused entry get displayed in the appendix?},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}
@inproceedings{filler1,
  title={Test filler: Does unused entry get displayed in the appendix?},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}
@inproceedings{filler2,
  title={Test filler: Does unused entry get displayed in the appendix?},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}
@inproceedings{filler3,
  title={Test filler: Does unused entry get displayed in the appendix?},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}
@inproceedings{filler4,
  title={Test filler: Does unused entry get displayed in the appendix?},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}
@inproceedings{filler5,
  title={Test filler: Does unused entry get displayed in the appendix?},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}
@inproceedings{filler6,
  title={Test filler: Does unused entry get displayed in the appendix?},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}

@Inbook{multiagent-2021-most-cited,
author="Zhang, Kaiqing
and Yang, Zhuoran
and Ba{\c{s}}ar, Tamer",
editor="Vamvoudakis, Kyriakos G.
and Wan, Yan
and Lewis, Frank L.
and Cansever, Derya",
title="Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms",
bookTitle="Handbook of Reinforcement Learning and Control",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="321--384",
abstract="Recent years have witnessed significant advances in reinforcement learning (RL), which has registered tremendous success in solving various sequential decision-making problems in machine learning. Most of the successful RL applications, e.g., the games of Go and Poker, robotics, and autonomous driving, involve the participation of more than one single agent, which naturally fall into the realm of multi-agent RL (MARL), a domain with a relatively long history, and has recently re-emerged due to advances in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly within two representative frameworks, Markov/stochastic games and extensive-form games, in accordance with the types of tasks they address, i.e., fully cooperative, fully competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL, we highlight several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, (non-)convergence of policy-based methods for learning in games, etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the current state of the field on the mark, to identify fruitful future research directions on theoretical studies of MARL. We expect this chapter to serve as continuing stimulus for researchers interested in working on this exciting while challenging topic.",
isbn="978-3-030-60990-0",
doi="10.1007/978-3-030-60990-0_12",
url="https://doi.org/10.1007/978-3-030-60990-0_12"
}

@article{federated-learning-2021,
  title={Communication-efficient federated learning},
  author={Mingzhe Chen and Nir Shlezinger and H. Vincent Poor and Yonina C. Eldar and Shuguang Cui},
  journal={Proceedings of the National Academy of Sciences},
  year={2021},
  volume={118}
}
@inproceedings{feudal-1992,
  title={Feudal Reinforcement Learning},
  author={Peter Dayan and Geoffrey E. Hinton},
  booktitle={NIPS},
  year={1992}
}

@article{Johnson2020FeudalSH,
  title={Feudal Steering: Hierarchical Learning for Steering Angle Prediction},
  author={Faith Johnson and Kristin J. Dana},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year={2020},
  pages={4316-4325}
}
