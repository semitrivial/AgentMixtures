@article{brockman2016openai,
  title={Open{AI} gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={Preprint},
  year={2016}
}

@book{bromwich2005introduction,
  title={An introduction to the theory of infinite series},
  author={Bromwich, Thomas John I'Anson},
  volume={335},
  year={2005},
  publisher={American Mathematical Society}
}

@inproceedings{hernandez2011more,
  title={On more realistic environment distributions for defining, evaluating and developing intelligence},
  author={Hern{\'a}ndez-Orallo, Jos{\'e} and Dowe, David L and Espana-Cubillo, Sergio and Hern{\'a}ndez-Lloreda, M Victoria and Insa-Cabrera, Javier},
  booktitle={CAGI},
  year={2011},
}

@article{silver2021reward,
  title={Reward is enough},
  author={Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard},
  journal={Artificial Intelligence},
  year={2021},
  publisher={Elsevier}
}

@conference{RLvsRL,
  title={Can reinforcement learning learn itself? {A} reply to `{R}eward is enough'},
  year={2021},
  author={Alexander, Samuel Allen},
  booktitle={CIFMA}
}

@inproceedings{alexander2021reward,
  title={Reward-punishment symmetric universal intelligence},
  author={Alexander, Samuel Allen and Hutter, Marcus},
  booktitle={AGI},
  year={2021},
}

@article{legg2007universal,
  title={Universal intelligence: A definition of machine intelligence},
  author={Legg, Shane and Hutter, Marcus},
  journal={Minds and machines},
  volume={17},
  number={4},
  pages={391--444},
  year={2007},
  publisher={Springer}
}

@inproceedings{DBLP:conf/ijcai/LeggH05,
  author    = {Shane Legg and
               Marcus Hutter},
  editor    = {Leslie Pack Kaelbling and
               Alessandro Saffiotti},
  title     = {A Universal Measure of Intelligence for Artificial Agents},
  booktitle = {IJCAI-05, Proceedings of the Nineteenth International Joint Conference
               on Artificial Intelligence, Edinburgh, Scotland, UK, July 30 - August
               5, 2005},
  pages     = {1509--1510},
  publisher = {Professional Book Center},
  year      = {2005},
  url       = {http://ijcai.org/Proceedings/05/Papers/post-0042.pdf},
  timestamp = {Tue, 20 Aug 2019 16:18:51 +0200},
  biburl    = {https://dblp.org/rec/conf/ijcai/LeggH05.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hutter2009discrete,
  title={Discrete {MDL} predicts in total variation},
  author={Hutter, Marcus},
  journal={Advances in Neural Information Processing Systems},
  volume={22},
  year={2009}
}

@article{extendedenvironmentspaper,
  title={Extending Environments To Measure Self-Reflection In Reinforcement Learning},
  author={Alexander, Samuel Allen and Castaneda, Michael and Compher, Kevin
    and Martinez, Oscar},
  journal={Preprint},
  year={2022}
}

@inproceedings{leike2015bad,
  title={Bad universal priors and notions of optimality},
  author={Leike, Jan and Hutter, Marcus},
  booktitle={Conference on Learning Theory},
  pages={1244--1259},
  year={2015},
  organization={PMLR}
}

@book{li2008introduction,
  title={An introduction to Kolmogorov complexity and its applications},
  author={Li, Ming and Vit{\'a}nyi, Paul},
  year={2008},
  publisher={Springer}
}

@inproceedings{DBLP:conf/alt/Hutter03,
  author    = {Marcus Hutter},
  editor    = {Ricard Gavald{\`{a}} and
               Klaus P. Jantke and
               Eiji Takimoto},
  title     = {On the Existence and Convergence of Computable Universal Priors},
  booktitle = {Algorithmic Learning Theory, 14th International Conference, {ALT}
               2003, Sapporo, Japan, October 17-19, 2003, Proceedings},
  series    = {Lecture Notes in Computer Science},
  volume    = {2842},
  pages     = {298--312},
  publisher = {Springer},
  year      = {2003},
  url       = {https://doi.org/10.1007/978-3-540-39624-6\_24},
  doi       = {10.1007/978-3-540-39624-6\_24},
  timestamp = {Tue, 14 May 2019 10:00:51 +0200},
  biburl    = {https://dblp.org/rec/conf/alt/Hutter03.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{muller2010stationary,
  title={Stationary algorithmic probability},
  author={M{\"u}ller, Markus},
  journal={Theoretical Computer Science},
  volume={411},
  number={1},
  pages={113--130},
  year={2010},
  publisher={Elsevier}
}

@article{nature-sortition,
author = {Flanigan, Bailey and Gölz, Paul and Gupta, Anupam and Hennig, Brett and Procaccia, Ariel},
year = {2021},
month = {08},
pages = {},
title = {Fair algorithms for selecting citizens’ assemblies},
volume = {596},
journal = {Nature},
doi = {10.1038/s41586-021-03788-6}
}

@Book{sortition-ancient,
author = { Hansen, Mogens Herman },
title = { The Athenian democracy in the age of Demosthenes : structure, principles, and ideology / Mogens Herman Hansen ; translated by J.A. Crook },
isbn = { 0631180176 0631138226 },
publisher = { B. Blackwell Oxford, UK ; Cambridge, USA },
pages = { xvi, 410 p. : },
year = { 1991 },
type = { Book },
language = { English },
subjects = { Democracy -- History.; Athens (Greece) -- Politics and government.; Greece -- Politics and government -- To 146 B.C. },
life-dates = { 1991 -  },
catalogue-url = { https://nla.gov.au/nla.cat-vn1436590 },
}

@article{Sintomer2018FromDT,
  title={From Deliberative to Radical Democracy? Sortition and Politics in the Twenty-First Century},
  author={Yves Sintomer},
  journal={Politics \& Society},
  year={2018},
  volume={46},
  pages={337 - 357}
}

@article{sortition-2013,
author = {Bouricius, Terrill},
year = {2013},
month = {04},
pages = {11},
title = {Democracy Through Multi-Body Sortition: Athenian Lessons for the Modern Day},
volume = {9},
journal = {Journal of Deliberative Democracy},
doi = {10.16997/jdd.156}
}

@article{nature-crowd,
author = {Prelec, Dražen and Seung, Hyunjune and McCoy, John},
year = {2017},
month = {01},
pages = {532-535},
title = {A solution to the single-question crowd wisdom problem},
volume = {541},
journal = {Nature},
doi = {10.1038/nature21054}
}

@article{nature-starcraft,
author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John and Jaderberg, Max and Silver, David},
year = {2019},
month = {11},
pages = {},
title = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
volume = {575},
journal = {Nature},
doi = {10.1038/s41586-019-1724-z}
}

@article{nature-ATARI,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei and Veness, Joel and Bellemare, Marc and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
year = {2015},
month = {02},
pages = {529-33},
title = {Human-level control through deep reinforcement learning},
volume = {518},
journal = {Nature},
doi = {10.1038/nature14236}
}

@article{nature-go-2,
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Driessche, George and Graepel, Thore and Hassabis, Demis},
year = {2017},
month = {10},
pages = {354-359},
title = {Mastering the game of Go without human knowledge},
volume = {550},
journal = {Nature},
doi = {10.1038/nature24270}
}

@inproceedings{unified-multiagent,
  title={A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning},
  author={Marc Lanctot and Vin{\'i}cius Flores Zambaldi and Audrunas Gruslys and Angeliki Lazaridou and Karl Tuyls and Julien P{\'e}rolat and David Silver and Thore Graepel},
  booktitle={NIPS},
  year={2017}
}

@inproceedings{multiagent-2nd-cited-1998,
  title={Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm},
  author={Junling Hu and Michael P. Wellman},
  booktitle={ICML},
  year={1998}
}
￼
@inproceedings{multiagent-most-cited-1998-cooperative,
  title={The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems},
  author={Caroline Claus and Craig Boutilier},
  booktitle={AAAI/IAAI},
  year={1998}
}

@article{collective-1993,
  title={Collective learning of action sequences},
  author={Gerhard Weiss},
  journal={[1993] Proceedings. The 13th International Conference on Distributed Computing Systems},
  year={1993},
  pages={203-209}
}

@inproceedings{multiagent-1994-most-cited,
  title={Markov Games as a Framework for Multi-Agent Reinforcement Learning},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}

@inproceedings{filler,
  title={Test filler: Does unused entry get displayed in the appendix?},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}
@inproceedings{filler1,
  title={Test filler: Does unused entry get displayed in the appendix?},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}
@inproceedings{filler2,
  title={Test filler: Does unused entry get displayed in the appendix?},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}
@inproceedings{filler3,
  title={Test filler: Does unused entry get displayed in the appendix?},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}
@inproceedings{filler4,
  title={Test filler: Does unused entry get displayed in the appendix?},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}
@inproceedings{filler5,
  title={Test filler: Does unused entry get displayed in the appendix?},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}
@inproceedings{filler6,
  title={Test filler: Does unused entry get displayed in the appendix?},
  author={Michael L. Littman},
  booktitle={ICML},
  year={1994}
}

@Inbook{multiagent-2021-most-cited,
author="Zhang, Kaiqing
and Yang, Zhuoran
and Ba{\c{s}}ar, Tamer",
editor="Vamvoudakis, Kyriakos G.
and Wan, Yan
and Lewis, Frank L.
and Cansever, Derya",
title="Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms",
bookTitle="Handbook of Reinforcement Learning and Control",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="321--384",
abstract="Recent years have witnessed significant advances in reinforcement learning (RL), which has registered tremendous success in solving various sequential decision-making problems in machine learning. Most of the successful RL applications, e.g., the games of Go and Poker, robotics, and autonomous driving, involve the participation of more than one single agent, which naturally fall into the realm of multi-agent RL (MARL), a domain with a relatively long history, and has recently re-emerged due to advances in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly within two representative frameworks, Markov/stochastic games and extensive-form games, in accordance with the types of tasks they address, i.e., fully cooperative, fully competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL, we highlight several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, (non-)convergence of policy-based methods for learning in games, etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the current state of the field on the mark, to identify fruitful future research directions on theoretical studies of MARL. We expect this chapter to serve as continuing stimulus for researchers interested in working on this exciting while challenging topic.",
isbn="978-3-030-60990-0",
doi="10.1007/978-3-030-60990-0_12",
url="https://doi.org/10.1007/978-3-030-60990-0_12"
}

@article{federated-learning-2021,
  title={Communication-efficient federated learning},
  author={Mingzhe Chen and Nir Shlezinger and H. Vincent Poor and Yonina C. Eldar and Shuguang Cui},
  journal={Proceedings of the National Academy of Sciences},
  year={2021},
  volume={118}
}
@inproceedings{feudal-1992,
  title={Feudal Reinforcement Learning},
  author={Peter Dayan and Geoffrey E. Hinton},
  booktitle={NIPS},
  year={1992}
}

@article{Johnson2020FeudalSH,
  title={Feudal Steering: Hierarchical Learning for Steering Angle Prediction},
  author={Faith Johnson and Kristin J. Dana},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year={2020},
  pages={4316-4325}
}


@inproceedings{Debenham2005AMS,
  title={A multiagent system manages collaboration in emergent processes},
  author={John K. Debenham},
  booktitle={AAMAS '05},
  year={2005}
}



@article{Zheng2022TheAE,
  title={The AI Economist: Taxation policy design via two-level deep multiagent reinforcement learning},
  author={Stephan Zheng and Alexander Trott and Sunil Srinivasa and David C. Parkes and Richard Socher},
  journal={Science Advances},
  year={2022},
  volume={8}
}

@inbook{inbook,
author = {Barton, Sean and Waytowich, Nicholas and Zaroukian, Erin and Asher, Derrik},
year = {2018},
month = {10},
pages = {422-427},
title = {Measuring Collaborative Emergent Behavior in Multi-agent Reinforcement Learning},
isbn = {978-3-030-02052-1},
doi = {10.1007/978-3-030-02053-8_64}
}


@article{Kok2006CollaborativeMR,
  title={Collaborative Multiagent Reinforcement Learning by Payoff Propagation},
  author={Jelle R. Kok and Nikos A. Vlassis},
  journal={J. Mach. Learn. Res.},
  year={2006},
  volume={7},
  pages={1789-1828}
}

@inproceedings{Qiu2021RMIXLR,
  title={RMIX: Learning Risk-Sensitive Policies for Cooperative Reinforcement Learning Agents},
  author={Wei Qiu and Xinrun Wang and Runsheng Yu and Xu He and R. Wang and Bo An and Svetlana Obraztsova and Zinovi Rabinovich},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{collab-other,
author = {Li, Zhaoxing and Shi, Lei and Cristea, Alexandra I. and Zhou, Yunzhan},
title = {A Survey of Collaborative Reinforcement Learning: Interactive Methods and Design Patterns},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462135},
doi = {10.1145/3461778.3462135},
booktitle = {Designing Interactive Systems Conference 2021},
pages = {1579–1590},
numpages = {12},
keywords = {Interactive Methods, Design Patterns, Collaborative RL},
location = {Virtual Event, USA},
series = {DIS '21}
}


@misc{PPO-bad,
  doi = {10.48550/ARXIV.2206.10027},
  url = {https://arxiv.org/abs/2206.10027},
  author = {Aitchison, Mathew and Sweetser, Penny},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DNA: Proximal Policy Optimization with a Dual Network Architecture},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


@article{PPO,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  eprinttype = {arXiv},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{control-textbook-1,
  title={Feedback Control of Dynamic Systems (6th edition)},
  author={Gene F. Franklin and J. David Powell and Abbas Emami-Naeini},
  year={2010}
}

@inproceedings{control-textbook-2,
  title={Feedback Systems: An Introduction for Scientists and Engineers},
  author={Karl Johan {\AA}str{\"o}m and Richard M. Murray},
  year={2008}
}

@article{RL-robotics,
author = {Kober, Jens and Bagnell, J. and Peters, Jan},
year = {2013},
month = {09},
pages = {1238-1274},
title = {Reinforcement Learning in Robotics: A Survey},
volume = {32},
isbn = {978-3-642-27644-6},
journal = {The International Journal of Robotics Research},
doi = {10.1177/0278364913495721}
}

@article{Li2021ReinforcementLF,
  title={Reinforcement Learning for Robust Parameterized Locomotion Control of Bipedal Robots},
  author={Zhongyu Li and Xuxin Cheng and Xue Bin Peng and P. Abbeel and Sergey Levine and Glen Berseth and Koushil Sreenath},
  journal={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2021},
  pages={2811-2817}
}

@article{Wang2022DeepRL,
  title={Deep reinforcement learning based synthetic jet control on disturbed flow over airfoil},
  author={Yi-Zhe Wang and Yu-fei Mei and Nadine Aubry and Zhi-hua Chen and Peng Wu and Wei-Tao Wu},
  journal={Physics of Fluids},
  year={2022}
}

@article{RL-car-suspension,
  title={Reinforcement-Learning-Based Vibration Control for a Vehicle Semi-Active Suspension System via the PPO Approach},
  author={Shizhong Han and Tong Liang},
  journal={Applied Sciences},
  year={2022}
}

See actually quote at https://ai.stackexchange.com/questions/11375/what-is-the-difference-between-reinforcement-learning-and-optimal-control
@article{Sutton2005ReinforcementLA-mentions-optimal-control,
  title={Reinforcement Learning: An Introduction},
  author={Richard S. Sutton and Andrew G. Barto},
  journal={IEEE Transactions on Neural Networks},
  year={2005},
  volume={16},
  pages={285-286}
}

@article{Valena1980NyquistCF,
  title={Nyquist criterion for input/output stability of multivariable systems},
  author={J. M. E. Valença and Christopher J. Harris},
  journal={International Journal of Control},
  year={1980},
  volume={31},
  pages={917-935}
}

@article{RL-Lyapunov-safe,
author = {Perkins, Theodore and Barto, Andrew},
year = {2003},
month = {05},
pages = {803-},
title = {Lyapunov Design for Safe Reinforcement Learning Control},
volume = {3},
journal = {J. Mach. Learn. Res.},
doi = {10.1162/jmlr.2003.3.4-5.803}
}


@InProceedings{pmlr-v119-stooke20a,
  title = 	 {Responsive Safety in Reinforcement Learning by {PID} Lagrangian Methods},
  author =       {Stooke, Adam and Achiam, Joshua and Abbeel, Pieter},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9133--9143},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/stooke20a/stooke20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/stooke20a.html},
}

@inproceedings{Cheng2019EndtoEndSR,
  title={End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks},
  author={Richard Cheng and G{\'a}bor Orosz and Richard M. Murray and Joel W. Burdick},
  booktitle={AAAI},
  year={2019}
}

@article{Bouarfa2013AgentbasedMA,
  title={Agent-based modeling and simulation of emergent behavior in air transportation},
  author={Soufiane Bouarfa and Henk A. P. Blom and Richard Curran and Mariken H. C. Everdij},
  journal={Complex Adaptive Systems Modeling},
  year={2013},
  volume={1},
  pages={1-26}
}
