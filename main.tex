\documentclass{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amsthm}
% \smartqed  % flush right qed marks, e.g. at end of proof
% \usepackage{graphicx}
%\newtheorem{definition}[definition]{Definition}

% \pagenumbering{gobble}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{definition}[theorem]{Definition}
\newtheorem{principle}[theorem]{Principle}
\newtheorem{conjecture}[theorem]{Conjecture}

\begin{document}

\title{Agent mixtures and the genericness of non-deterministic intelligence}
% \titlerunning{Agent mixtures and genericness}
\author{Samuel Allen Alexander \& Len Du \& Marcus Hutter}

% \institute{The U.S.\ Securities and Exchange Commission
% \email{samuelallenalexander@gmail.com}
% \url{https://philpeople.org/profiles/samuel-alexander/publications}}

\maketitle

\begin{abstract}
    We introduce a weighted mixture operation on
    reinforcement learning agents. The mixture of several weighted agents is
    an agent with the
    following property: the expected total reward the mixture agent
    gets in any environment is the corresponding weighted average
    of the expected total rewards the original agents get in that
    environment. We use mixture agents to formalize and
    strengthen an informal result of Alexander and Hutter. We also use mixture
    agents to prove additional results, including a surprising result
    which we call the genericness of non-deterministic intelligence. Loosely:
    any particular non-deterministic action an agent takes
    can have its probabilities modified without making the agent less
    intelligent.
\end{abstract}

\section{Introduction}

In reinforcement learning (RL), an agent $\pi$ interacts with an environment $\mu$.
The agent and the environment take turns.
\begin{itemize}
\item
On $\pi$'s turn, $\pi$
outputs a probability distribution over a fixed action-set.
Based on this distribution, an action is randomly chosen
and is transmitted to $\pi$ and $\mu$.
\item
On $\mu$'s turn, $\mu$
outputs a probability distribution over a fixed percept-set,
where every percept includes an observation (thought of as
the agent's view of the world) and a numerical reward.
Based on this distribution, a percept is randomly chosen and
is transmitted to $\pi$ and $\mu$.
\end{itemize}
These turns continue forever, and the whole sequence of turns
is called an agent-environment interaction.

If $\pi$ and $\rho$ are two agents, we can informally imagine a new agent
$\sigma$ (first described in \cite{alexander2021reward})
as follows. At the beginning of every agent-environment interaction,
$\sigma$ flips a coin. If the coin lands heads, then $\sigma$ transforms into
$\pi$; otherwise, $\sigma$ transforms into $\rho$. Note that the coin is only
flipped one time, at the very start of the agent-environment interaction:
it is not repeatedly flipped every turn. Intuitively, it seems like the
expected total reward in the agent-environment interaction
when $\sigma$ interacts with $\mu$, should be the average
of the corresponding expected total rewards when $\pi$ or $\rho$ interact
with $\mu$. But this is all quite informal, as the RL
framework does not actually provide any mechanism for such an initial
coin-flip.

More generally, given agents $\vec{\pi}=(\pi^1,\ldots,\pi^n)$ and positive real
numbers
$\vec{w}=(w_1,\ldots,w_n)$ with $w_1+\cdots+w_n=1$, we could imagine an agent $\sigma$
who, at the start of each agent-environment interaction, randomly transforms
into $\pi^1$ (with probability $w_1$) or into $\pi^2$ (with probability $w_2$),
etc. If each $\pi^i$ would get total expected reward $r_i$ from an environment
$\mu$, intuitively $\sigma$ should get total expected reward
$\vec{w}\cdot \vec{r}=w_1r_1+\cdots+w_nr_n$ from that environment.
But again, the RL
framework has no mechanism for such an agent $\sigma$. We will define
an agent $\vec{w}\cdot\vec{\pi}$, called a \emph{mixture agent},
within the constraints
of the RL framework, and prove that ``the expected total reward of the
weighted mixture is the weighted average of the expected total rewards''.
We will then use this mixture operation to prove a number of interesting results.

\section{Preliminaries}

In defining agent and environment below, we attempt to follow
Legg and Hutter \cite{legg2007universal} as closely as possible,
except that we permit environments to output rewards from $\mathbb Q \cap [-1,1]$
rather than just $\mathbb Q\cap [0,1]$ (and, accordingly, we modify which well-behaved
environments to restrict our attention to).

Throughout the paper, we implicitly
fix a finite set $\mathcal A$ of \emph{actions},
a finite set $\mathcal O$ of \emph{observations},
and a finite set $\mathcal R\subseteq \mathbb Q\cap [-1,1]$ of \emph{rewards}
(so each reward is a rational number between $-1$ and $1$ inclusive),
with $|\mathcal A|>0$,
$|\mathcal O|>0$, $|\mathcal R|>0$.
We assume that $\mathcal R$ has the following property:
whenever $\mathcal R$ contains any reward $r$, then $\mathcal R$
also contains $-r$.
We assume $\mathcal A$, $\mathcal O$, and $\mathcal R$ are mutually disjoint
(i.e., no reward is an action, no reward is an observation, and no action is an
observation).
By $\langle\rangle$ we mean the empty sequence.
By $\mathcal P$ we mean $\mathcal O\times\mathcal R$ (the set of all observation-reward
pairs); elements of $\mathcal P$ are called \emph{percepts}.

\begin{definition}
\label{omnibusdefn}
    (Agents, environments, etc.)
    \begin{enumerate}
        \item
        By $(\mathcal P\mathcal A)^*$ we mean the set of
        all finite sequences
        starting with a percept, ending with an action,
        and following the pattern ``percept, action, ...''.
        We also include $\langle\rangle$ in $(\mathcal P\mathcal A)^*$.
        Nonempty elements of $(\mathcal P\mathcal A)^*$ have the
        form $x_1y_1\ldots x_ty_t$ where each $x_i$ is a percept and
        each $y_i$ is an action.
        \item
        By $(\mathcal P\mathcal A)^* \mathcal P$
        we mean the set of all sequences of the form $s\circ p$ where
        $s\in (\mathcal P\mathcal A)^*$, $p\in\mathcal P$
        ($\circ$ denotes concatenation).
        Elements of $(\mathcal P\mathcal A)^* \mathcal P$
        of length $>1$ have the form
        $x_1y_1\ldots x_{t-1}y_{t-1}x_t$
        (each $x_i$ a percept, each $y_i$ an action).
        \item
        By an \emph{agent}, we mean a function $\pi$
        with domain $(\mathcal P\mathcal A)^* \mathcal P$,
        which assigns to every sequence
        $h\in (\mathcal P\mathcal A)^* \mathcal P$ a
        $\mathbb Q$-valued probability measure,
        written $\pi(\bullet|h)$, on $\mathcal A$.
        For every such $h$ and every $a\in\mathcal A$,
        we write $\pi(a|h)$ for $(\pi(\bullet|h))(a)$.
        Intuitively, $\pi(a|h)$ is the probability that agent $\pi$
        will take action $a$ in response to history $h$.
        \item
        By an \emph{environment}, we mean a function $\mu$
        with domain $(\mathcal P\mathcal A)^*$,
        which assigns to every
        $h\in (\mathcal P\mathcal A)^*$
        a $\mathbb Q$-valued probability measure,
        written $\mu(\bullet|h)$,
        on $\mathcal P$.
        For every such $s$ and every $(o,r)\in\mathcal P$,
        we write $\mu(o,r|h)$ for $(\mu(\bullet|h))(o,r)$.
        If $p=(o,r)$ then we may also write $\mu(p|h)$ for
        $(\mu(\bullet|h))(o,r)$.
        Intuitively, $\mu(o,r|h)$ is the probability that environment
        $\mu$ will issue percept $(o,r)$ (observation $o$ and reward $r$)
        to the agent in response to history $h$.
        \item
        If $\pi$ is an agent, $\mu$ is an environment, and $t\in\mathbb N$,
        we write $V^\pi_{\mu,t}$ for the expected value of the sum of
        the rewards which would occur in the sequence
        $x_1y_1\ldots x_{t+1}y_{t+1}$
        randomly generated as follows:
        \begin{enumerate}
            \item $x_1\in \mathcal P$ is chosen randomly based
            on the probability measure $\mu(\bullet|\langle\rangle)$.
            \item $y_1\in\mathcal A$ is chosen randomly based on the probability
            measure $\pi(\bullet|x_1)$.
            \item
            For each $i>1$,
            $x_i\in\mathcal P$ is chosen randomly based on
            the probability measure
            $\mu(\bullet|x_1y_1\ldots x_{i-1}y_{i-1})$.
            \item
            For each $i>1$,
            $y_i\in\mathcal A$ is chosen randomly based on the probability measure
            $\pi(\bullet|x_1y_1\ldots x_{i-1}y_{i-1}x_i)$.
        \end{enumerate}
        \item
        If $\pi$ is an agent and $\mu$ is an environment,
        let $V^\pi_\mu=\lim_{t\to\infty}V^{\pi}_{\mu,t}$.
        Intuitively, $V^\pi_\mu$ is the expected total reward which $\pi$ would extract
        from $\mu$.
    \end{enumerate}
\end{definition}

Note that it is possible for $V^\pi_\mu$ to be undefined.
For example, if $\mu$ is an environment which always issues
reward $(-1)^t$ in response to the agent's $t$th action $y_t$,
then $V^\pi_\mu$ is undefined for every agent $\pi$.

\begin{remark}
\label{impossibleremark}
    Note that in Definition \ref{omnibusdefn} part 3, we require,
    e.g., $\pi(\bullet|x_1y_1x_2)$ to be defined even if
    $\pi(y_1|x_1)=0$, in which case in Definition \ref{omnibusdefn} part 5,
    there would be zero probability that the randomly-generated initial
    sequence would begin with this particular $x_1y_1x_2$
    (so that $\pi(\bullet|x_1y_1x_2)$ contributes nothing to
    $V^\pi_{\mu,t}$ and thus nothing to $V^\pi_\mu$).
    Intuitively, an agent must choose actions even in response to
    a history that is ``impossible'' for that agent (impossible because the history
    involves the agent having taken an action the agent never would have taken).
    This convention (in which we follow \cite{legg2007universal}) simplifies most
    definitions, though it will complicate our definition of mixture agents
    (Definition \ref{maindefn} below). It also has implications about the
    self-reflectivity of agents, explored in \cite{extendedenvironmentspaper}.
\end{remark}

\begin{definition}
    An environment $\mu$ is \emph{well-behaved} if $\mu$ is computable and the following
    condition holds: for every agent $\pi$, $V^\pi_\mu$ exists and
    $-1\leq V^\pi_\mu\leq 1$. Let $W$ be the set of all well-behaved environments.
\end{definition}

\begin{definition}
\label{performanceaveragerdefn}
    By a \emph{weighted intelligence measure}, we mean a function
    $\Upsilon$ assigning real numbers to agents, such that there
    are weights $\{w_\mu\}_{\mu\in W}$ such that for every agent
    $\pi$, $\Upsilon(\pi)=\sum_{\mu\in W}w_\mu V^\pi_\mu$
    (and such that this infinite sum is absolutely convergent
    for every agent $\pi$).
\end{definition}

The prototypical weighted intelligence measure is the Legg-Hutter intelligence
measure $\Upsilon$, where each well-behaved $\mu$ is given weight $2^{-K(\mu)}$
where $K$ denotes Kolmogorov complexity (dependent on a background universal
Turing machine) \cite{legg2007universal}.

\section{Mixture agents}

Before defining mixture agents, we need to start with some preliminary definitions
and lemmas.

\begin{definition}
    By $\mathcal H$ we mean
    $((\mathcal P\mathcal A)^*)\cup((\mathcal P\mathcal A)^*\mathcal P)$.
    We refer to elements $h$ of $\mathcal H$ as \emph{histories} (a history
    may terminate with either a percept or an action).
\end{definition}

\begin{definition}
\label{pullbackdef}
    \begin{enumerate}
        \item
        For every agent $\pi$, for every $h\in\mathcal H$, we define a real number
        $\pi_*(h)\in[0,1]$ as follows.
        \begin{itemize}
            \item
            If $h=\langle\rangle$ then $\pi_*(h)=1$.
            \item
            If $h=g\circ p$ for some $p\in\mathcal P$ then $\pi_*(h)=\pi_*(g)$.
            \item
            If $h=g\circ a$ for some $a\in\mathcal A$ then $\pi_*(h)=\pi_*(g)\pi(a|g)$.
        \end{itemize}
        The intended intuition is that $\pi_*(h)$ can be thought of as the
        conditional probability that when $\pi$
        and any environment
        $\mu$ interact, the interaction will begin with initial segment
        $h$, \emph{assuming}
        that $\mu$ produces the percepts in $h$.
        But note that $\pi_*$ is \emph{not} a probability distribution.
        \item
        For every environment $\mu$, for every $h\in\mathcal H$, we define a real number
        $\mu_*(h)\in[0,1]$ as follows.
        \begin{itemize}
            \item
            If $h=\langle\rangle$ then $\mu_*(h)=1$.
            \item
            If $h=g\circ p$ for some $p\in\mathcal P$ then $\mu_*(h)=\mu_*(g)\mu(p|g)$.
            \item
            If $s=g\circ a$ for some $a\in\mathcal A$ then $\mu_*(h)=\mu_*(g)$.
        \end{itemize}
        The intended intuition is that $\mu_*(h)$ can be thought of as the
        conditional probability that when any agent $\pi$ and $\mu$ interact,
        the interaction will begin with initial segment $h$, \emph{assuming} that
        $\pi$ produces the actions in $h$.
        But note that $\mu_*$ is \emph{not} a probability distribution.
        \item
        For every agent $\pi$, environment $\mu$, and $h\in\mathcal H$, we define a
        real number $(\pi,\mu)_*(h)\in[0,1]$ by induction as follows.
        \begin{itemize}
            \item
            If $h=\langle\rangle$ then $(\pi,\mu)_*(h)=1$.
            \item
            If $h=g\circ p$ for some $p\in\mathcal P$ then
            $(\pi,\mu)_*(h)=(\pi,\mu)_*(g)\mu(p|g)$.
            \item
            If $h=g\circ a$ for some $a\in\mathcal A$ then
            $(\pi,\mu)_*(h)=(\pi,\mu)_*(g)\pi(a|g)$.
        \end{itemize}
        The intended intuition is that $(\pi,\mu)_*(h)$ can be thought of as
        the probability that when $\pi$ and $\mu$ interact, the interaction
        will begin with initial segment $h$. But note that
        $(\pi,\mu)_*$ is \emph{not} a probability distribution.
    \end{enumerate}
\end{definition}

Some authors, such as \cite{hutter2009discrete}, would write $P(h)$ or a variation thereof
for $(\pi,\mu)_*(h)$, provided $\pi$ and $\mu$ are clear from context.

One could alternately more directly define
\[
    \pi_*(x_1y_1\ldots x_ty_t)
    = \pi(y_1|x_1)\pi(y_2|x_1y_1x_2)\cdots \pi(y_t|x_1y_1\ldots x_t),
\]
and similarly define $\pi_*(x_1y_1\ldots x_t)$,
and likewise for $\mu_*$ and for $(\pi,\mu)_*$. Surprisingly, this more
direct definition would actually be \emph{less} useful than
Definition \ref{pullbackdef}, because it would not lend itself so well
to proofs by induction.

\begin{lemma}
\label{factorizationlemma}
    For all $h$, $\pi$, $\mu$ as in Definition \ref{pullbackdef},
    \[
        (\pi,\mu)_*(h) = \pi_*(h)\mu_*(h).
    \]
\end{lemma}

\begin{proof}
    By induction on $h$.

    Case 1: $h=\langle\rangle$. Then the lemma is trivial.

    Case 2: $h=g\circ p$ for some $p\in\mathcal P$.
        Then
        \begin{align*}
            (\pi,\mu)_*(h)
                &= (\pi,\mu)_*(g)\mu(p|g)
                    &\mbox{(Definition \ref{pullbackdef} part 3)}\\
                &= \pi_*(g)\mu_*(g)\mu(p|g)
                    &\mbox{(Induction)}\\
                &= \pi_*(h)\mu_*(g)\mu(p|g)
                    &\mbox{(Definition \ref{pullbackdef} part 1)}\\
                &= \pi_*(h)\mu_*(h).
                    &\mbox{(Definition \ref{pullbackdef} part 2)}
        \end{align*}

    Case 3: $h=g\circ a$ for some $a\in\mathcal A$.
        Similar to Case 2.
\end{proof}

\begin{lemma}
\label{basicprobabilitylemma}
    $V^\pi_{\mu,t}=\sum_{h\in X}(\pi,\mu)_*(h)R(h),$
    where $X$ is the set of all $h\in\mathcal H$
    of length $2(t+1)$ (i.e., all $h\in\mathcal H$ of the form
    $x_1y_1\ldots x_{t+1}y_{t+1}$
    as in Part 5 of Definition \ref{omnibusdefn}),
    and where $R(h)$ is the total reward in $h$.
\end{lemma}

\begin{proof}
    Basic probability theory.
\end{proof}

\begin{definition}
    If $\vec\pi=(\pi^1,\ldots,\pi^n)$ are agents, $\mu$ is an environment,
    $h\in\mathcal H$, $t\in\mathbb N$, and $\Upsilon$ is a weighted
    intelligence measure, then we define:
    \begin{itemize}
        \item $\vec\pi_*(h)=(\pi^1_*(h),\ldots,\pi^n_*(h))$.
        \item $(\vec\pi,\mu)_*(h)=((\pi^1,\mu)_*(h),\ldots,(\pi^n,\mu)_*(h))$.
        \item $V^{\vec\pi}_{\mu,t}=(V^{\pi^1}_{\mu,t},\ldots,V^{\pi^n}_{\mu,t})$.
        \item If each $V^{\pi^i}_\mu$ converges, we say $V^{\vec\pi}_\mu$
            converges, and define $V^{\vec\pi}_\mu=(V^{\pi^1}_\mu,\ldots,V^{\pi^n}_\mu)$.
        \item $\Upsilon(\vec\pi)=(\Upsilon(\pi^1),\ldots,\Upsilon(\pi^n))$.
    \end{itemize}
    Recall that if $\vec u=(u_1,\ldots,u_n)$ and
    $\vec v=(v_1,\ldots,v_n)$ are any two equal-length
    vectors of real numbers, then their \emph{inner product}
    (or \emph{dot product}) is defined to be
    $\vec u\cdot \vec v=u_1v_1+\cdots+u_nv_n$.
\end{definition}

Now we are ready to define mixture agents.

\begin{definition}
\label{maindefn}
    (Mixture agents)
    Suppose $\vec\pi=(\pi^1,\ldots,\pi^n)$ are agents and $\vec w=(w_1,\ldots,w_n)$
    are positive real numbers with $w_1+\cdots+w_n=1$.
    Define the \emph{mixture agent} $\vec w\cdot\vec\pi$ as follows: for all
    $h\in (\mathcal P\mathcal A)^*\mathcal P$, let
    \[
        (\vec w\cdot\vec\pi)(a|h)
        =
        \begin{cases}
            \dfrac{\vec w\cdot \vec\pi_*(h\circ a)}{\vec w\cdot \vec\pi_*(h)}
            &\mbox{if $\vec w\cdot \vec\pi_*(h)\not=0$,}\\
            1/|\mathcal{A}| &\mbox{otherwise}
        \end{cases}
    \]
    where
    $\vec w\cdot\vec\pi_*(h)$ denotes $\vec w\cdot(\vec\pi_*(h))$ and
    $\vec w\cdot\vec\pi_*(h\circ a)$ denotes
    $\vec w\cdot(\vec\pi_*(h\circ a))$.
\end{definition}

\begin{remark}
\label{ambiguityremark}
    In Definition \ref{maindefn},
    the expression $\vec w\cdot \vec\pi_*(h)$ would have been inherently
    ambiguous if we had not specified that it denotes
    the vector dot-product $\vec w\cdot (\vec\pi_*(h))$.
    For, otherwise, it could just as well denote
    $(\vec w\cdot \vec\pi)_*(h)$.
    Likewise for $\vec w\cdot \vec\pi_*(h\circ a)$.
    In Theorem \ref{maintheorem} below, we will see that in fact
    there is no ambiguity: $\vec w\cdot(\vec\pi_*(h))=(\vec w\cdot \vec\pi)_*(h)$.
\end{remark}

\begin{remark}
    Definition \ref{maindefn} is complicated by Remark \ref{impossibleremark},
    which forces us to include the case
    $(\vec w\cdot\vec\pi)(a|h)=1/|\mathcal A|$ when
    $\vec w\cdot \vec\pi_*(h)=0$: we are obligated to specify how
    $\vec w\cdot\vec\pi$ chooses actions even in response to histories that
    are ``impossible'' for $\vec w\cdot\vec\pi$ (histories which involve
    $\vec w\cdot\vec\pi$ having taken actions which $\vec w\cdot\vec\pi$
    never would take). The value $1/|\mathcal A|$ in this case is completely arbitrary;
    any other value would be just as good,
    provided $\sum_{a\in\mathcal A}(\vec w\cdot\vec\pi)(a|h)=1$.
\end{remark}

\begin{lemma}
    If $\vec\pi$ and $\vec w$ are as in Definition \ref{maindefn}
    then $\vec w\cdot\vec\pi$ really is an agent.
\end{lemma}

\begin{proof}
    Let $h\in(\mathcal P\mathcal A)^*\mathcal P$.
    Clearly $(\vec w\cdot\vec\pi)(a|h)\geq 0$ for all $a\in\mathcal A$.
    It remains to show
    $\sum_{a\in\mathcal A}(\vec w\cdot\vec\pi)(a|h)=1$.

    Case 1: $\vec w\cdot (\vec\pi_*(h))=0$. Then
    each $(\vec w\cdot\vec\pi)(a|h)=1/|\mathcal A|$ so the
    claim is immediate.

    Case 2: $\vec w\cdot (\vec\pi_*(h))\not=0$. Then
    \begin{align*}
        &{} \sum_{a\in\mathcal A}(\vec w\cdot\vec\pi)(a|h)\\
            &= \sum_{a\in\mathcal A}
                \frac{\vec w\cdot (\vec\pi_*(h\circ a))}{\vec w\cdot (\vec\pi_*(h))}
                &\mbox{(Definition \ref{maindefn})}\\
            &= \sum_{a\in\mathcal A}
                \frac
                {w_1\pi^1_*(h)\pi^1(a|h)+\cdots+w_n\pi^n_*(h)\pi^n(a|h)}
                {\vec w\cdot(\vec\pi_*(h))}
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= \frac{
                w_1\pi^1_*(h)\left(\mbox{$\sum_{a\in\mathcal A}\pi^1(a|h)$}\right)
                +\cdots+
                w_n\pi^n_*(h)\left(\mbox{$\sum_{a\in\mathcal A}\pi^n(a|h)$}\right)
                }
                {\vec w\cdot (\vec\pi_*(h))}
                &\mbox{(Algebra)}\\
            &= \frac
                {w_1\pi^1_*(h)\cdot 1 + \cdots + w_n\pi^n_*(h)\cdot1}
                {\vec w\cdot (\vec\pi_*(h))}
                =\frac{\vec w\cdot (\vec\pi_*(h))}{\vec w\cdot (\vec\pi_*(h))}=1.
                &\mbox{($\pi^i$ are agents)}
    \end{align*}
\end{proof}

\begin{theorem}
\label{maintheorem}
    (Commutativity of $\vec w$)
    Let $\vec\pi=(\pi^1,\ldots,\pi^n)$ be agents.
    Let $\vec w=(w_1,\ldots,w_n)$ be positive reals with
    $w_1+\cdots+w_n=1$. Let $\mu$ be any environment.
    Then:
    \begin{enumerate}
        \item
        For any $h\in\mathcal H$,
        $(\vec w\cdot\vec\pi)_*(h)=\vec w\cdot(\vec\pi_*(h))$.
        \item
        For any $h\in\mathcal H$,
        $(\vec w\cdot \vec\pi,\mu)_*(h)=\vec w\cdot(\vec\pi,\mu)_*(h)$.
        \item
        For any $t\in\mathbb N$,
        $V^{\vec w\cdot \vec\pi}_{\mu,t}=\vec w\cdot V^{\vec\pi}_{\mu,t}$.
        \item
        (``The expected reward of a weighted mixture is the weighted
        average of the expected rewards'')
        If $V^{\vec\pi}_\mu$ converges, then $V^{\vec w\cdot\vec\pi}_\mu$
        converges and $V^{\vec w\cdot\vec\pi}_\mu=\vec w\cdot V^{\vec\pi}_\mu$.
        \item
        (``The intelligence of a weighted mixture is the weighted average
        of the intelligences'')
        For any weighted intelligence measure $\Upsilon$,
        $\Upsilon(\vec w\cdot\vec\pi)=\vec w\cdot\Upsilon(\vec\pi)$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    (1) By induction on $h$.

    Case 1: $h=\langle\rangle$. Then
    $(\vec w\cdot\vec\pi)_*(h)=1=w_1\cdot 1+\cdots+w_n\cdot 1
    =\vec w\cdot (\vec\pi_*(h))$.

    Case 2: $h=g\circ p$ for some $p\in\mathcal P$. Then
    \begin{align*}
        (\vec w\cdot \vec\pi)_*(h)
            &= (\vec w\cdot\vec\pi)_*(g)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= \vec w\cdot(\vec\pi_*(g))
                &\mbox{(Induction)}\\
            &= \vec w\cdot(\vec\pi_*(g\circ p))
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= \vec w\cdot(\vec\pi_*(h)).
    \end{align*}

    Case 3: $h=g\circ a$ for some $a\in\mathcal A$.

    Subcase 3.1: $(\vec w\cdot \vec\pi)_*(g)=0$.
        By induction $\vec w\cdot(\vec\pi_*(g))=0$.
        Since the $w_i$ are positive, this implies
        each $\pi^i_*(g)=0$.
        Thus each
        \begin{align*}
            w_i\pi^i_*(g\circ a)
                &= w_i\pi^i_*(g)\pi^i(a|g)
                    &\mbox{(Definition \ref{pullbackdef})}\\
                &= 0w_i\pi^i(a|g)=0,
        \end{align*}
        i.e., $\vec w\cdot(\vec\pi_*(g\circ a))=0$.
        And
        \begin{align*}
            (\vec w\cdot\vec\pi)_*(g\circ a)
                &= (\vec w\cdot\vec\pi)_*(g)(\vec w\cdot\vec\pi)(a|g)
                    &\mbox{(Definition \ref{pullbackdef})}\\
                &= 0(\vec w\cdot\vec\pi)(a|g) = 0,
        \end{align*}
        so $(\vec w\cdot\vec\pi)_*(h)=\vec w\cdot(\vec \pi_*(h))=0$.

    Subcase 3.2: $\vec w\cdot (\vec\pi_*(g))\not=0$. Then
    \begin{align*}
        (\vec w\cdot\vec\pi)_*(h)
            &= (\vec w\cdot\vec\pi)_*(g)(\vec w\cdot\vec\pi)(a|g)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= (\vec w\cdot\vec\pi)_*(g)
                \frac
                {\vec w\cdot(\vec\pi_*(g\circ a))}
                {\vec w\cdot(\vec\pi_*(g))}
                &\mbox{(Definition \ref{maindefn})}\\
            &= \vec w\cdot(\vec\pi_*(g))
                \frac
                {\vec w\cdot(\vec\pi_*(g\circ a))}
                {\vec w\cdot(\vec\pi_*(g))}
                &\mbox{(Induction)}\\
            &= \vec w\cdot(\vec\pi_*(g\circ a))
                &\mbox{(Basic Algebra)}\\
            &= \vec w\cdot(\vec\pi_*(h)).
    \end{align*}

    (2) Follows from (1) and Lemma \ref{factorizationlemma}.

    (3) Follows from (2) and Lemma \ref{basicprobabilitylemma}.

    (4) Immediate from (3).

    (5) Immediate from (4).
\end{proof}

By part 1 of Theorem \ref{maintheorem},
we may unambiguously write $\vec w\cdot \vec\pi_*(h)$
for either $(\vec w\cdot\vec\pi)_*(h)$ or $\vec w\cdot(\vec\pi_*(h))$.
We will freely do so for the remainder of the paper.

\section{Duality and Janus agents:
Strengthening and formalizing a result of Alexander and Hutter}

\begin{definition}
\label{dualagentsdefn}
(Dual Agents and Dual Environments)
\begin{enumerate}
    \item
    For each $h\in\mathcal H$, let $\overline h$ be the sequence obtained
    by replacing every percept $(o,r)$ in $h$ by $(o,-r)$ (in other words:
    replacing every reward $r$ in $h$ by $-r$).
    \item
    Suppose $\pi$ is an agent.
    We define a new agent $\overline \pi$, the \emph{dual} of $\pi$,
    as follows:
    for each $h\in (\mathcal P\mathcal A)^*\mathcal P$,
    for each action $a\in\mathcal A$,
    \[\overline\pi(a|h)=\pi(a|\overline h).\]
    \item
    Suppose $\mu$ is an environment.
    We define a new environment $\overline\mu$, the \emph{dual} of $\mu$,
    as follows:
    for each $h\in (\mathcal P\mathcal A)^*$,
    for each $(o,r)\in\mathcal P$,
    \[\overline\mu(o,r|h)=\mu(o,-r|\overline h).\]
\end{enumerate}
\end{definition}

\begin{lemma}
\label{doublenegationlemma}
    If $x$ is any agent, environment, or element of $\mathcal H$,
    then $\overline{\overline x}=x$.
\end{lemma}

\begin{proof}
    By the fact that $--r=r$ for all real $r$.
\end{proof}

\begin{lemma}
\label{asteriskcommuteswithoverlinelemma}
    For any agent $\pi$ and any $h\in(\mathcal P\mathcal A)^*\mathcal P$,
    $\pi_*(\overline h)=\overline{\pi}_*(h)$.
\end{lemma}

\begin{proof}
    By induction on $h$.
\end{proof}

The following class of agents is named after Janus, the mythological
Roman god of duality. In the myth, Janus has two faces, one facing
forward, the other facing backward. Thus in some sense, flipping Janus
around by $180^\circ$ would not change him.

\begin{definition}
    (Janus agents)
    A \emph{Janus agent} is an agent $\pi$ such that
    $\overline{\pi}=\pi$.
\end{definition}

We will use Janus agents to strengthen and formalize an informal result
from \cite{alexander2021reward}.
But first, we will use mixture agents to characterize
Janus agents (up to equivalence modulo a certain natural equivalence relation).

\begin{definition}
\label{equivdefn}
    If $\pi$ and $\rho$ are agents, we say $\pi\equiv\rho$ if the
    following conditions hold:
    \begin{enumerate}
        \item For all $h\in\mathcal H$, $\pi_*(h)=0$ iff $\rho_*(h)=0$.
        \item For all $h\in(\mathcal P\mathcal A)^*\mathcal P$,
            if $\pi_*(h)\not=0$ then for all $a\in\mathcal A$,
            $\pi(a|h)=\rho(a|h)$.
    \end{enumerate}
\end{definition}

\begin{remark}
    Intuitively, Definition \ref{equivdefn} says that $\pi\equiv\rho$
    iff the histories which are ``possible'' for $\pi$ (in the sense of
    Remark \ref{impossibleremark}) are exactly the histories which are
    ``possible'' for $\rho$, and $\pi=\rho$ on those histories.
\end{remark}

\begin{lemma}
\label{equivrelationlemma}
    The relation $\equiv$ of Definition \ref{equivdefn} is an equivalence
    relation.
\end{lemma}

\begin{proof}
    Straightforward.
\end{proof}

\begin{lemma}
\label{piopluspilemma}
    Let $\vec w=(w_1,\ldots,w_n)$ be positive reals,
    $w_1+\cdots+w_n=1$. For every agent $\pi$,
    $\pi\equiv\vec w\cdot (\pi,\ldots,\pi)$ (where
    $(\pi,\ldots,\pi)$ has length $n$).
\end{lemma}

\begin{proof}
    Write $\vec\pi$ for $(\pi,\ldots,\pi)$.
    We prove conditions 1 and 2 of Definition \ref{equivdefn}
    simultaneously by induction on $h$.
 
    Case 1: $h=\langle\rangle$. Then
    $\pi_*(h)=\vec w\cdot\vec\pi_*(h)=1\not=0$, so
    vacuously $\pi_*(h)=0$ iff $\vec w\cdot\vec\pi_*(h)=0$
    (proving condition 1).
    For condition 2, there is nothing to check, since
    $\langle\rangle\not\in(\mathcal P\mathcal A)^*\mathcal P$.

    Case 2: $h=h_0\circ a_0$ for some
        $h_0\in(\mathcal P\mathcal A)^*\mathcal P$, $a_0\in\mathcal A$.
        For condition 2, there is nothing to prove, since
        $h\not\in(\mathcal P\mathcal A)^*\mathcal P$.
        For condition 1, we consider two cases.

        Subcase 2.1: $\pi_*(h_0)=0$.
        By induction, condition 1 holds for $h_0$, so
        $\vec w\cdot\vec\pi_*(h_0)=0$.
        By Definition \ref{pullbackdef},
        $\pi_*(h)=0\pi(a_0|h_0)=0$
        and $(\vec w\cdot\vec\pi)_*(h)=0(\vec w\cdot\vec\pi)(a_0|h_0)=0$.
        So $\pi_*(h)=0$ iff $(\vec w\cdot\vec\pi)_*(h)=0$.

        Subcase 2.2: $\pi_*(h_0)\not=0$.
        Then
        \begin{align*}
            (\vec w\cdot \vec\pi)_*(h)
                &= \vec w\cdot(\vec\pi_*(h))
                    &\mbox{(Theorem \ref{maintheorem})}\\
                &= (w_1\pi_*(h)+\cdots+w_n\pi_*(h))
                    &\mbox{(Def.\ of $\vec w$ and $\vec\pi$)}\\
                &= \pi_*(h)
                    &\mbox{($w_1+\cdots+w_n=1$)}\\
                &= \pi_*(h_0)\pi(a_0|h_0).
                    &\mbox{(Definition \ref{pullbackdef})}
        \end{align*}
        Since $\pi_*(h_0)\not=0$, and since $\mathbb R$ has the zero-product property,
        it follows that
        $(\vec w\cdot\vec\pi)_*(h)=0$ iff $\pi_*(h)=0$ iff $\pi(a_0|h_0)=0$.

    Case 3: $h=h_0\circ p$ for some $h_0\in (\mathcal P\mathcal A)^*$,
        $p\in\mathcal P$.
        By induction, conditions 1 and 2 hold for $h_0$.
        By Definition \ref{pullbackdef},
        $\pi_*(h)=\pi_*(h_0)$ and $(\vec w\cdot\vec\pi)_*(h)=(\vec w\cdot\vec\pi)_*(h_0)$,
        so condition 1 for $h$ follows.

        For condition 2,
        assume $\pi_*(h)\not=0$ and let $a\in\mathcal A$.
        By choice of $\vec w$ and $\vec\pi$,
        $\vec w\cdot\vec\pi_*(h)=w_1\pi_*(h)+\cdots+w_n\pi_*(h)=\pi_*(h)$.
        So, since $\pi_*(h)\not=0$, $\vec w\cdot\vec\pi_*(h)\not=0$.
        Thus
        \begin{align*}
            (\vec w\cdot\vec\pi)(a|h)
                &= \frac{\vec w\cdot \vec\pi_*(h\circ a)}{\vec w\cdot\vec\pi_*(h)}
                    &\mbox{(Definition \ref{maindefn})}\\
                &= \frac{w_1\pi_*(h\circ a)+\cdots+w_n\pi_*(h\circ a)}
                    {w_1\pi_*(h)+\cdots+w_n\pi_*(h)}
                    &\mbox{(Def.\ of $\vec w$ and $\vec\pi$)}\\
                &= \frac{w_1\pi_*(h)+\cdots+w_n\pi_*(h)}{w_1\pi_*(h)+\cdots+w_n\pi_*(h)}
                    \pi(a|h)
                    &\mbox{(Definition \ref{pullbackdef})}\\
                &= \pi(a|h).
        \end{align*}
\end{proof}

\begin{lemma}
\label{reflectionmakesjanuslemma}
    For any agent $\pi$,
    $(\frac12,\frac12)\cdot(\pi,\overline\pi)$ is a Janus agent.
\end{lemma}

\begin{proof}
    Let $\vec w=(\frac12,\frac12)$.
    For any $h\in(\mathcal P\mathcal A)^*\mathcal P$ and $a\in\mathcal A$,
    we claim
    $\overline{\vec w\cdot(\pi,\overline{\pi})}(a|h)
    =(\vec w\cdot(\pi,\overline{\pi}))(a|h)$.
    If $\vec w\cdot(\pi,\overline\pi)_*(h)=0$
    then it follows by Lemmas \ref{doublenegationlemma} and
    \ref{asteriskcommuteswithoverlinelemma} that
    $\overline{\vec w\cdot(\pi,\overline\pi)}_*(h)=0$,
    so by Definition \ref{maindefn},
    $\overline{\vec w\cdot(\pi,\overline{\pi})}(a|h)
    =(\vec w\cdot(\pi,\overline{\pi}))(a|h)=1/|\mathcal A|$.
    But assume $\vec w\cdot(\pi,\overline\pi)_*(h)\not=0$.
    Then:
    \begin{align*}
        \overline{\vec w\cdot(\pi,\overline{\pi})}(a|h)
        &= (\vec w\cdot(\pi,\overline{\pi}))(a|\overline h)
            &\mbox{(Definition \ref{dualagentsdefn})}\\
        &= \frac
            {\frac12\pi_*(\overline h\circ a)+\frac12\overline\pi_*(\overline h\circ a)}
            {\frac12\pi_*(\overline h)+\frac12\overline\pi_*(\overline h)}
            &\mbox{(Definition \ref{maindefn})}\\
        &= \frac
            {\frac12\pi_*(\overline{h\circ a})+\frac12\overline\pi_*(\overline{h\circ a})}
            {\frac12\pi_*(\overline h)+\frac12\overline\pi_*(\overline h)}
            &\mbox{(Clearly $\overline h\circ a=\overline{h\circ a}$)}\\
        &= \frac
            {\frac12\overline\pi_*(h\circ a)+\frac12\overline{\overline\pi}_*(h\circ a)}
            {\frac12\overline\pi_*(h)+\frac12\overline{\overline\pi}_*(h)}
            &\mbox{(Lemma \ref{asteriskcommuteswithoverlinelemma})}\\
        &= (\vec w\cdot(\overline{\overline\pi},\overline\pi))(a|h)
            &\mbox{(Definition \ref{maindefn})}\\
        &= (\vec w\cdot(\pi,\overline\pi))(a|h).
            &\mbox{(Lemma \ref{doublenegationlemma})}
    \end{align*}
\end{proof}

\begin{proposition}
    (Characterization of Janus agents modulo $\equiv$)
    For any agent $\pi$, the following are equivalent:
    \begin{enumerate}
        \item $\pi\equiv\rho$ for some Janus agent $\rho$.
        \item $\pi\equiv(\frac12,\frac12)\cdot(\rho,\overline{\rho})$
            for some agent $\rho$.
    \end{enumerate}
\end{proposition}

\begin{proof}
    ($\Rightarrow$)
    Assume $\pi\equiv\rho$ for some Janus agent $\rho$.
    Then $\rho=\overline{\rho}$, so
    $\rho\equiv (\frac12,\frac12)\cdot(\rho,\overline{\rho})$ by
    Lemma \ref{piopluspilemma}, thus, by Lemma \ref{equivrelationlemma},
    $\pi\equiv(\frac12,\frac12)\cdot(\rho,\overline{\rho})$.

    ($\Leftarrow$)
    Assume $\pi\equiv(\frac12,\frac12)\cdot(\rho,\overline{\rho})$ for some agent $\rho$.
    By Lemma \ref{reflectionmakesjanuslemma},
    $(\frac12,\frac12)\cdot(\rho,\overline{\rho})$ is a Janus agent, so we are done.
\end{proof}

The following theorem formalizes and strengthens an informal result
from \cite{alexander2021reward}.

\begin{theorem}
    Suppose $\Upsilon$ is a weighted intelligence measure.
    If $\Upsilon(\pi)=0$ for every Janus agent $\pi$,
    then $\Upsilon(\overline{\pi})=-\Upsilon(\pi)$
    for every agent $\pi$.
\end{theorem}

\begin{proof}
    Let $\pi$ be any agent.
    By Lemma \ref{reflectionmakesjanuslemma},
    $(\frac12,\frac12)\cdot(\pi,\overline\pi)$ is a Janus agent.
    So by assumption,
    $\Upsilon((\frac12,\frac12)\cdot(\pi,\overline\pi))=0$.
    Thus by Theorem \ref{maintheorem},
    \[
        (\mbox{$\frac12$},\mbox{$\frac12$})\cdot\Upsilon((\pi,\overline\pi))
        =\mbox{$\frac12$}\Upsilon(\pi)+\mbox{$\frac12$}\Upsilon(\overline\pi)=0.
    \]
    So $\Upsilon(\overline{\pi})=-\Upsilon(\pi)$.
\end{proof}


\section{Detectability of sets of agents}

In this section, we give another application of mixture agents.

\begin{definition}
\label{incentivizabilitydefn}
    A set $\Pi$ of agents is \emph{detectable} if there exists
    an environment $\mu$ such that for every agent $\pi$:
    \begin{enumerate}
        \item
        $V^\pi_\mu$ exists.
        \item
        $V^\pi_\mu>0$ iff $\pi\in\Pi$.
    \end{enumerate}
\end{definition}

\begin{definition}
\label{mixtureclosuredef}
    A set $\Pi$ is \emph{closed under mixtures} if the following
    condition holds: for all positive reals $\vec w=(w_1,\ldots,w_n)$
    with $w_1+\cdots+w_n=1$,
    for all agents $\vec\pi=(\pi^1,\ldots,\pi^n)$,
    if $\pi^i\in \Pi$ for every $i=1,\ldots,n$, then
    $\vec w\cdot\vec\pi\in\Pi$.
\end{definition}

\begin{theorem}
\label{closuretheorem}
    (Necessary Conditition for Detectability)
    Let $\Pi$ be any set of agents.
    If $\Pi$ is detectable, then $\Pi$ is closed under mixtures, and so
    is its complement $\Pi^c$.
\end{theorem}

\begin{proof}
    Assume $\Pi$ is detectable.
    Let $\mu$ be as in
    Definition \ref{incentivizabilitydefn}.
    To see $\Pi$ is closed under mixtures, let $\vec w$, $\vec\pi=(\pi^1,\ldots,\pi^n)$ be
    as in Definition \ref{mixtureclosuredef}.
    Assume $\pi^i\in\Pi$ for all $i=1,\ldots,n$.
    By choice of $\mu$, $V^{\pi^i}_\mu>0$ for all $i=1,\ldots,n$.
    Thus
    \begin{align*}
        V^{\vec w\cdot\vec\pi}_\mu
            &= \vec w\cdot V^{\vec\pi}_\mu
                &\mbox{(Theorem \ref{maintheorem})}\\
            &> w_1\cdot 0 + \cdots + w_n\cdot 0 = 0,
    \end{align*}
    so by choice of $\mu$, $\vec w\cdot \vec\pi\in \Pi$.
    A similar argument shows that $\Pi^c$ is closed under mixtures.
\end{proof}


\section{Genericness of non-deterministic agents}

\begin{definition}
\label{modifyagentatoneplace}
    If $\pi$ is an agent, $h_0\in(\mathcal P\mathcal A)^*\mathcal P$,
    and $m$ is a probability distribution on $\mathcal A$,
    we write $\pi^{h_0\mapsto m}$ for the agent which is identical to $\pi$
    except that it maps $h_0$ to $m$, in other words:
    \[
        \pi^{h_0\mapsto m}(a|h)
        =
        \begin{cases}
            \pi(a|h) &\mbox{if $h\not=h_0$,}\\
            m(a) &\mbox{if $h=h_0$.}
        \end{cases}
    \]
\end{definition}

The following three lemmas are technical lemmas needed to prove
some interesting results.

\begin{lemma}
\label{firsttechlemmaforgenericity}
    Suppose $\pi$, $h_0$, $m$ are as in Definition \ref{modifyagentatoneplace}.
    Let $h\in\mathcal H$ be such that
    for every $a\in\mathcal A$,
    $h_0\circ a$ is not an initial segment of $h$.
    Then $\pi^{h_0\mapsto m}_*(h)=\pi_*(h)$.
\end{lemma}

\begin{proof}
    By induction on $h$.
\end{proof}

\begin{lemma}
\label{thirdtechlemmaforgenericity}
    Suppose $\pi$, $h_0$, $m$ are as in Definition \ref{modifyagentatoneplace}.
    For any $a\in\mathcal A$,
    $\pi^{h_0\mapsto m}_*(h_0\circ a)=\pi_*(h_0)m(a)$.
\end{lemma}

\begin{proof}
    Immediate by Definition \ref{pullbackdef} and Lemma \ref{firsttechlemmaforgenericity}.
\end{proof}

\begin{lemma}
\label{secondtechlemmaforgenericity}
    Suppose $\pi$, $h_0$, $m$ are as in Definition \ref{modifyagentatoneplace}.
    Assume $h\in\mathcal H$, $a_0\in\mathcal A$, and $h_0\circ a_0$ is
    an initial segment of $h$. Assume $\pi(a_0|h_0)\not=0$. Then
    $\pi^{h_0\mapsto m}_*(h) = \frac{\pi_*(h)m(a_0)}{\pi(a_0|h_0)}$.
\end{lemma}

\begin{proof}
    By induction on $h$.

    Case 1: $h=h_0\circ a_0$. Then
    \begin{align*}
        \pi^{h_0\mapsto m}_*(h)
        &= \pi^{h_0\mapsto m}_*(h_0)\pi^{h_0\mapsto m}(a_0|h_0)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi_*(h_0)\pi^{h_0\mapsto m}(a_0|h_0)
            &\mbox{(Lemma \ref{firsttechlemmaforgenericity})}\\
        &= \pi_*(h_0)m(a_0)
            &\mbox{(Definition \ref{modifyagentatoneplace})}\\
        &= \pi_*(h_0)\pi(a_0|h_0)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Basic Algebra)}\\
        &= \pi_*(h)m(a_0)/\pi(a_0|h_0).
            &\mbox{(Definition \ref{pullbackdef})}
    \end{align*}

    Case 2: $h=h_0\circ a_0\circ h_1\circ p$ for some $h_1\in\mathcal H$
        and $p\in\mathcal P$. Then
    \begin{align*}
        \pi^{h_0\mapsto m}_*(h)
        &= \pi^{h_0\mapsto m}_*(h_0\circ a_0\circ h_1)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi_*(h_0\circ a_0\circ h_1)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Induction)}\\
        &= \pi_*(h_0\circ a_0\circ h_1\circ p)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi_*(h)m(a_0)/\pi(a_0|h_0).
    \end{align*}

    Case 3: $h=h_0\circ a_0\circ h_1\circ a$ for some $h_1\in\mathcal H$ and
        $a\in\mathcal A$. Then
    \begin{align*}
        \pi^{h_0\mapsto m}_*(h)
        &= \pi^{h_0\mapsto m}_*(h_0\circ a_0\circ h_1)
            \pi^{h_0\mapsto m}(a|h_0\circ a\circ h_1)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi^{h_0\mapsto m}_*(h_0\circ a_0\circ h_1)
            \pi(a|h_0\circ a\circ h_1)
            &\mbox{(Definition \ref{modifyagentatoneplace})}\\
        &= \pi_*(h_0\circ a_0\circ h_1)\pi(a|h_0\circ a\circ h_1)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Induction)}\\
        &= \pi_*(h_0\circ a_0\circ h_1\circ a)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi_*(h)m(a_0)/\pi(a_0|h_0).
    \end{align*}
\end{proof}

\begin{definition}
\label{sumofdistros}
    Suppose $\vec m=(m_1,\ldots,m_n)$ are probability distributions on $\mathcal A$
    and $\vec w=(w_1,\ldots,w_n)$ are positive reals with
    $w_1+\cdots+w_n=1$. By $\vec w\cdot\vec m$ we mean the probability distribution
    on $\mathcal A$ defined by
    \[
        (\vec w\cdot\vec m)(a) = w_1m_1(a) + \cdots + w_nm_n(a).
    \]
\end{definition}

\begin{lemma}
    If $\vec m$, $\vec w$ are as in Definition \ref{sumofdistros}
    then $\vec w\cdot\vec m$ really is a probability distribution on $\mathcal A$.
\end{lemma}

\begin{proof}
    Clearly for every $a\in\mathcal A$,
    $(\vec w\cdot\vec m)(a) = w_1m_1(a) + \cdots + w_nm_n(a)$ is a nonnegative
    real. It remains to show $\sum_{a\in\mathcal A}(\vec w\cdot\vec m)(a)=1$.
    We compute:
    \begin{align*}
        &{} \sum_{a\in\mathcal A}(\vec w\cdot\vec m)(a)\\
        &=
        \sum_{a\in\mathcal A} w_1m_1(a) + \cdots + w_nm_n(a)
            &\mbox{(Definition \ref{sumofdistros})}\\
        &=
        w_1\sum_{a\in\mathcal A} m_1(a) + \cdots + w_n\sum_{a\in\mathcal A}m_n(a)
            &\mbox{(Basic Algebra)}\\
        &= w_1 + \cdots + w_n
            &\mbox{($m_1,\ldots,m_n$ are probability distr's)}\\
        &= 1.
    \end{align*}
\end{proof}

\begin{definition}
    For any agent $\pi$, for any $h\in(\mathcal P\mathcal A)^*\mathcal P$,
    for any probability distributions $\vec m=(m_1,\ldots,m_n)$ on $\mathcal A$,
    let $\pi^{h\mapsto \vec m}=(\pi^{h\mapsto m_1},\ldots,\pi^{h\mapsto m_n})$.
\end{definition}

The following proposition shows that
for any particular history $h$ and agent $\pi$,
for any decomposition of $\pi(\bullet|h)$ into a weighted sum
of probability distributions $m_1,\ldots,m_n$,
$\pi$ has the same intelligence as the weighted mixture of the corresponding $n$ agents
$\pi^{h\mapsto \vec m}$.

\begin{proposition}
\label{longproposition}
    Let $\Upsilon$ be any weighted intelligence measure, let $\pi$ be any agent,
    and let $h\in(\mathcal P\mathcal A)^*\mathcal P$.
    Suppose $\vec m$ and $\vec w$ are as in Definition \ref{sumofdistros}.
    If $\vec w\cdot\vec m = \pi(\bullet|h)$, then
    $
        \Upsilon(\pi)
        =
        \Upsilon(\vec w\cdot \pi^{h\mapsto \vec m}).
    $
\end{proposition}

\begin{proof}
    For brevity, write $\vec\pi$ for $\pi^{h\mapsto \vec m}$. It suffices to
    show that for every well-behaved $\mu$ and every $t\in\mathbb N$,
    $
        V^{\pi}_{\mu,t}
        =
        V^{\vec w\cdot \vec\pi}_{\mu,t}.
    $
    By Lemma \ref{basicprobabilitylemma}, it suffices to show that for every
    well-behaved $\mu$ and every $g\in\mathcal H$,
    $
    (\pi,\mu)_*(g)
    =
    (\vec w\cdot\vec\pi,\mu)_*(g)
    $.
    By Lemma \ref{factorizationlemma}, it suffices
    to show that for every $g\in\mathcal H$,
    $\pi_*(g)=(\vec w\cdot\vec\pi)_*(g)$.
    We prove this by induction on $g$.

    Case 1: $g=\langle\rangle$.
    Then $\pi_*(g)=1
    =(\vec w\cdot\vec\pi)_*(g)$.

    Case 2: $g=f\circ p$ for some $p\in\mathcal P$.
    Then
    \begin{align*}
        \pi_*(g)
            &= \pi_*(f)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= (\vec w\cdot\vec\pi)_*(f)
                &\mbox{(Induction)}\\
            &= (\vec w\cdot\vec\pi)_*(g).
                &\mbox{(Definition \ref{pullbackdef})}
    \end{align*}

    Case 3: $g=f\circ a$ for some $a\in\mathcal A$.

    Subcase 3.1: $\pi_*(f)=0$.
    Then
    \begin{align*}
        (\vec w\cdot\vec\pi)_*(g)
            &= (\vec w\cdot\vec\pi)_*(f)
            (\vec w\cdot\vec\pi)(a|f)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= \pi_*(f)(\vec w\cdot\vec\pi)(a|f)
                &\mbox{(Induction)}\\
            &= 0.
    \end{align*}
    Similarly, $\pi_*(g)=0$. So $(\vec w\cdot\vec\pi)_*(g)=\pi_*(g)$.

    Subcase 3.2: $\pi_*(f)\not=0$ and $f=h$. Then:
    \begin{align*}
        (\vec w\cdot\vec\pi)_*(g)
            &= (\vec w\cdot\vec\pi)_*(h\circ a)\\
            &= \vec w\cdot\vec\pi_*(h\circ a)
                    &\mbox{(Theorem \ref{maintheorem})}\\
            &= w_1\pi_*(h)m_1(a)+\cdots+w_n\pi_*(h)m_n(a)
                    &\mbox{(Lemma \ref{thirdtechlemmaforgenericity})}\\
            &= \pi_*(h)\pi(a|h)
                    &\mbox{($\vec w\cdot\vec m=\pi(\bullet|h)$)}\\
            &= \pi_*(h\circ a) = \pi_*(g).
                    &\mbox{(Definition \ref{pullbackdef})}
    \end{align*}

    Subcase 3.3: $\pi_*(f)\not=0$, $f\not=h$, and
    $f$ has an initial segment $h\circ a_0$ ($a_0\in\mathcal A$).

    Then $\pi(a_0|h)\not=0$, lest
    we would have $\pi_*(f)=0$. Thus:
    \begin{align*}
        (\vec w\cdot\vec\pi)_*(g)
            &= (\vec w\cdot \vec\pi)_*(f\circ a)\\
            &= \vec w\cdot\vec\pi_*(f\circ a)
                    &\mbox{(Theorem \ref{maintheorem})}\\
            &= w_1\frac{\pi_*(f\circ a)m_1(a_0)}{\pi(a_0|h)}
                +\cdots+w_n\frac{\pi_*(f\circ a)m_n(a_0)}{\pi(a_0|h)}
                    &\mbox{(Lemma \ref{secondtechlemmaforgenericity})}\\
            &= \frac{\pi_*(f\circ a)}{\pi(a_0|h)}(w_1m_1(a_0)+\cdots+w_nm_n(a_0))
                    &\mbox{(Basic Algebra)}\\
            &= \frac{\pi_*(f\circ a)}{\pi(a_0|h)}\pi(a_0|h)
                    &\mbox{($\vec w\cdot\vec m=\pi(\bullet|h)$)}\\
            &= \pi_*(f\circ a) = \pi_*(g).
    \end{align*}

    Subcase 3.4: $\pi_*(f)\not=0$, $f\not=h$, and $f$ has no initial segment
        of the form $h\circ a_0$. Then:
    \begin{align*}
        (\vec w\cdot\vec\pi)_*(g)
            &= (\vec w\cdot\vec\pi)_*(f\circ a)\\
            &= \vec w\cdot\vec\pi_*(f\circ a)
                    &\mbox{(Theorem \ref{maintheorem})}\\
            &= w_1\pi_*(f\circ a)+\cdots+w_n\pi_*(f\circ a)
                    &\mbox{(Lemma \ref{firsttechlemmaforgenericity})}\\
            &= \pi_*(f\circ a) = \pi_*(g),
                    &\mbox{($w_1+\cdots+w_n=1$)}
    \end{align*}
    as desired.
\end{proof}

\begin{theorem}
\label{pointwisegenericnessthm}
    (Pointwise Genericness of Non-Deterministic Intelligence)
    Let $\Upsilon$ be any weighted intelligence measure and let
    $\pi$ be an agent.
    Let $h\in (\mathcal P\mathcal A)^*\mathcal P$.
    For any probability distributions $\vec m=(m_1,\ldots,m_n)$ on $\mathcal A$,
    for any positive reals $\vec w=(w_1,\ldots,w_n)$ with $w_1+\cdots+w_n=1$,
    if $\vec w\cdot\vec m=\pi(\bullet|h)$,
    then one of the following is true:
    \begin{enumerate}
        \item For each $i=1,\ldots,n$, $\Upsilon(\pi^{h\mapsto m_i})=\Upsilon(\pi)$.
        \item For some $i=1,\ldots,n$, $\Upsilon(\pi^{h\mapsto m_i})>\Upsilon(\pi)$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    If not, then for each $i=1,\ldots,n$, $\Upsilon(\pi^{h\mapsto m_i})\leq\Upsilon(\pi)$,
    and for some $i$, $\Upsilon(\pi^{h\mapsto m_i})<\Upsilon(\pi)$.
    This implies
    \begin{align*}
        \Upsilon(\pi)
            &= \Upsilon(\vec w\cdot\pi^{h\mapsto\vec m})
                &\mbox{(Proposition \ref{longproposition})}\\
            &= \vec w\cdot\Upsilon(\pi^{h\mapsto\vec m})
                &\mbox{(Theorem \ref{maintheorem})}\\
            &< w_1\Upsilon(\pi)+\cdots+w_n\Upsilon(\pi)
                &\mbox{(Assumption)}\\
            &= \Upsilon(\pi),
                &\mbox{($w_1+\cdots+w_n=1$)}
    \end{align*}
    absurd.
\end{proof}

Theorem \ref{pointwisegenericnessthm} is interesting because it implies that for
any intelligent agent $\pi$, for any history $h$, if $\pi(\bullet|h)$ is not deterministic,
then the optimality of $\pi$ does not critically depend on the specific probabilities
which $\pi(\bullet|h)$ assigns to different actions. For any decomposition
$\pi(\bullet|h)=\vec w\cdot\vec m$ of $\pi(\bullet|h)$ into competing probability
distributions $\vec m$
(with $w_1+\cdots+w_n=1$), either $\pi^{h\mapsto m_i}$ is more intelligent
than $\pi$ for some $i$, or else every $\pi^{h\mapsto m_i}$ has the same intelligence as
$\pi$. This is counter-intuitive because one might imagine that the specific probability
distribution $\pi(\bullet|h)$ was carefully chosen and optimized to maximize the intelligence
of $\pi$.

\begin{example}
\label{genericnessexample}
    Suppose $\pi$ is an agent and $h_0\in(\mathcal P\mathcal A)^*\mathcal P$.
    Assume $\mathcal A=\{a_1,a_2,a_3\}$.
    Suppose $\pi(a|h_0)=\frac13$ for every $a\in\mathcal A$.
    Define probability distributions $\vec m=(m_1,m_2,m_3)$ on $\mathcal A$ by
    \begin{align*}
        m_1(a) &=
        \begin{cases}
            \frac16 &\mbox{if $a\in\{a_1,a_2\}$}\\
            \frac23 &\mbox{if $a=a_3$}
        \end{cases}\\
        m_2(a) &=
        \begin{cases}
            \frac16 &\mbox{if $a\in\{a_1,a_3\}$}\\
            \frac23 &\mbox{if $a=a_2$}
        \end{cases}\\
        m_3(a) &=
        \begin{cases}
            \frac16 &\mbox{if $a\in\{a_2,a_3\}$}\\
            \frac23 &\mbox{if $a=a_1$}
        \end{cases}
    \end{align*}
    Let $\vec w=(w_1,w_2,w_3)=(\frac13,\frac13,\frac13)$. Then $w_1+w_2+w_3=1$ and
    it is easy to check
    $(\vec w\cdot\vec m)(a)=\frac13$ for every $a\in\mathcal A$,
    i.e., $\vec w\cdot\vec m=\pi(\bullet|h_0)$.
    By Theorem \ref{pointwisegenericnessthm},
    for any weighted intelligence measure $\Upsilon$,
    one of the following statements is true:
    \begin{enumerate}
        \item
        For all $i=1,2,3$, $\pi$'s intelligence would not change if we
        changed the value of $\pi(\bullet|h_0)$ to $m_i$. Or,
        \item
        For some $i=1,2,3$, $\pi$'s intelligence would increase if
        we changed the value of $\pi(\bullet|h_0)$ to $m_i$.
    \end{enumerate}
\end{example}

In Example \ref{genericnessexample}, Theorem \ref{pointwisegenericnessthm} shows
that the optimality of $\pi$ cannot crucially depend on the specific values
$\pi(a_1|h_0)=\pi(a_2|h_0)=\pi(a_3|h_0)=\frac13$ of $\pi$.
This is counter-intuitive because, a priori, it seems
plausible that these values could have been chosen
in order to maximize $\pi$'s intelligence. We could imagine ourselves
saying: ``These particular values $\pi(a_1|h_0)=\pi(a_2|h_0)=\pi(a_3|h_0)=\frac13$ are
critical to $\pi$'s
performance. Any other values would make $\pi$ sub-optimal.''
Theorem \ref{pointwisegenericnessthm} shows that no such statement can be true.

\begin{corollary}
\label{nondeterminismcorollary}
    (Pointwise Unnecessariness of Non-Determinism)
    Suppose $\Upsilon$ is a weighted intelligence measure, $\pi$ is an agent,
    and $h_0\in(\mathcal P\mathcal A)^*\mathcal P$.
    Let $a_1,\ldots,a_n$ enumerate $\{a\in\mathcal A\,:\,\pi(a|h_0)\not=0\}$.
    For each $i=1,\ldots,n$, let $m_i$ be the
    deterministic $\mathcal A$-probability distribution
    \[
        m_i(a) = \begin{cases}
            1 &\mbox{if $a=a_i$,}\\
            0 &\mbox{if $a\not=a_i$.}
        \end{cases}
    \]
    Then one of the following is true:
    \begin{enumerate}
        \item
        For each $i=1,\ldots,n$, $\Upsilon(\pi^{h_0\mapsto m_i})=\Upsilon(\pi)$.
        \item
        For some $i=1,\ldots,n$, $\Upsilon(\pi^{h_0\mapsto m_i})>\Upsilon(\pi)$.
    \end{enumerate}
\end{corollary}

\begin{proof}
    By Theorem \ref{pointwisegenericnessthm} with
    $\vec m=(m_1,\ldots,m_n)$ and $\vec w=(w_1,\ldots,w_n)$ where each
    $w_i=\pi(a_i|h_0)$.
\end{proof}

Corollary \ref{nondeterminismcorollary} shows that for any individual
history $h_0$, the optimality of $\pi$ cannot crucially depend on $\pi(\bullet|h_0)$
being non-deterministic. This is counter-intuitive because we could imagine saying:
``In response to such-and-such history, it would be optimal for our agent
to assume the environment is playing Paper-Rock-Scissors and therefore
assign uniform probabilities of $1/3$ to each of Paper, Rock, and Scissors;
certainly, it would be sub-optimal for our agent to instead assign probability $1$ to
any of Paper, Rock, or Scissors.'' Corollary \ref{nondeterminismcorollary} shows
that no such statement can be true.

The ``pointwise'' nature of Corollary \ref{nondeterminismcorollary}
is essential: by applying the corollary repeatedly, one can remove non-determinism
from $\pi$ at any finite number of points without ever decreasing $\pi$'s intelligence,
but one cannot conclude from this that non-determinism can be removed at infinitely
many points without decreasing $\pi$'s intelligence. The following example makes this
clear.

\begin{example}
    (Intelligence Discontinuity)
    Fix distinct actions $a_0,a_1\in\mathcal A$.
    Let $\mu$ be an environment such that in every agent-environment interaction:
    \begin{enumerate}
        \item If the agent always takes action $a_0$, then the agent gets total reward $-1$.
        \item If the agent initially takes action $a_0$ exactly $k$ times and
            then takes a different action, then the agent either
            gets total reward $1$ (with probability $1-2^{-k}$)
            or total reward $0$ (with probability $2^{-k}$)---so in expected
            value, the agent gets total reward $1-2^{-k}$.
    \end{enumerate}
    Let $\Upsilon$ be a weighted intelligence measure in which $\mu$ has weight $2$,
    $\overline\mu$ has weight $1$, and such that for every
    environment $\nu\not\in\{\mu,\overline{\mu}\}$, $\nu$ and $\overline{\nu}$ have
    equal weight.
    By Corollary 6 of \cite{alexander2021reward}, for any Janus agent $\pi$ and
    any environment $\nu$, $V^\pi_{\overline\nu}=-V^\pi_{\nu}$, thus for any Janus agent $\pi$,
    in the sum $\Upsilon(\pi)$, for any environment $\nu\not\in\{\mu,\overline{\mu}\}$,
    the contributions from $\nu$ and $\overline\nu$ are equal-weight multiples of
    $V^\pi_{\nu}$ and $-V^\pi_{\nu}$,
    respectively, and so cancel each other
    (terms of the infinite series can be regrouped due the absolute convergence
    in Definition \ref{performanceaveragerdefn}).
    Thus for any Janus agent $\pi$,
    $\Upsilon(\pi)$ consists solely of the contributions from $\mu$ and $\overline\mu$,
    i.e.,
    \[
        \Upsilon(\pi)=2V^\pi_\mu+1V^\pi_{\overline\mu}
        =2V^\pi_\mu-1V^\pi_{\mu}=V^\pi_\mu.
    \]
    For every $k\in\mathbb N$, let $\pi_k$ be the agent which ignores the environment,
    blindly taking action $a_0$ exactly $k$ times, and then forever thereafter,
    randomly taking action $a_0$ with probability $1/2$ or action $a_1$ with probability
    $1/2$.
    For every $k$,
    \begin{align*}
        V^{\pi_k}_\mu
            &= (1-2^{-(k+0)})\cdot \mbox{$\frac12$}
                +(1-2^{-(k+1)})\cdot\mbox{$\frac14$} + \cdots
                    &\mbox{(Basic probability)}\\
            &= 1-2^{-k},
                    &\mbox{(Geometric series)}
    \end{align*}
    so $\Upsilon(\pi_k)=1-2^{-k}$ (as $\pi_k$ is a Janus agent).
    Thus:
    \begin{enumerate}
        \item For each $k$, transforming $\pi_k$ into $\pi_{k+1}$
            (by changing finitely many $50\%$-probability-$a_0$
            actions into $100\%$-probability-$a_0$ actions)
            \emph{increases} intelligence.
        \item However, if we start this process with $\pi_0$ and
            repeat it to infinity, even though each individual step
            \emph{increases} intelligence, we finally end up with
            the completely deterministic
            Janus agent $\pi_{\infty}$ who always takes action $a_0$ and who is
            \emph{less} intelligent than each $\pi_k$:
            $\Upsilon(\pi_{\infty})=V^{\pi_\infty}_\mu=-1$.
    \end{enumerate}
\end{example}

\bibliographystyle{alpha}
\bibliography{main}

\end{document}