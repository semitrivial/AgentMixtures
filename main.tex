\documentclass[runningheads]{llncs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathrsfs}
% \smartqed  % flush right qed marks, e.g. at end of proof
% \usepackage{graphicx}

\newcommand{\myclaim}[1]{\textbf{Claim #1:}}
\def\myulcorner{\mathord{\ulcorner}}
\def\myurcorner{\mathord{\urcorner}}
\def\HD{\textsc{HD}}
\def\R{\mathbb{R}}
\def\N{\mathbb{N}}
%\newtheorem{definition}[definition]{Definition}

% \pagenumbering{gobble}

\begin{document}

\title{Environments cannot detect Markov agents}
\titlerunning{Environments cannot detect Markov agents}
\author{Samuel Allen
Alexander\inst{1}\orcidID{0000-0002-7930-110X}}

\institute{The U.S.\ Securities and Exchange Commission
\email{samuelallenalexander@gmail.com}
\url{https://philpeople.org/profiles/samuel-alexander/publications}}

\maketitle

\begin{abstract}
    We introduce a method for combining two reinforcement learning agents
    (equivalently: two decision theoretical agents) into a new agent
    with the property that the expected total reward the new agent gets in
    each environment is the average of the expected total rewards the
    two original agents get in that environment. Using this, we formalize
    and strengthen an informal result of Alexander and Hutter, and
    we prove a surprising additional result. Call an agent ``Markov''
    if that agent ignores all training data. We show that no environment can
    reward agents for being Markov. More precisely: if an environment
    gives positive expected total reward to every Markov agent, then it
    must give positive expected total reward to some non-Markov agent.
    We informally argue that this result casts doubt on an informal
    conjecture of Silver et al.
\end{abstract}

\section{Introduction}

In reinforcement learning, an agent $\pi$ interacts with an environment $\mu$.
The agent and the environment take turns.
\begin{itemize}
\item
On $\pi$'s turn, $\pi$
outputs a probability distribution over a fixed action-set.
Based on this distribution, an action is randomly chosen
and is transmitted to $\pi$ and $\mu$.
\item
On $\mu$'s turn, $\mu$
outputs a probability distribution over a fixed percept-set,
where every percept includes an observation (thought of as
the agent's view of the world) and a numerical reward.
Based on this distribution, a percept is randomly chosen and
is transmitted to $\pi$ and $\mu$.
\end{itemize}
These turns continue forever, and the whole sequence of turns
is called an agent-environment interaction.

If $\pi$ and $\rho$ are two agents, we can informally imagine a new agent
$\sigma$ (first described in \cite{alexander2021reward})
as follows. At the beginning of every agent-environment interaction,
$\sigma$ flips a coin. If the coin lands heads, then $\sigma$ transforms into
$\pi$; otherwise, $\sigma$ transforms into $\rho$. Note that the coin is only
flipped one time, at the very start of the agent-environment interaction:
it is not repeatedly flipped every turn. Intuitively, it seems like the
expected total reward in the agent-environment interaction
when $\sigma$ interacts with $\mu$, should be the average
of the corresponding expected total rewards when $\pi$ or $\rho$ interact
with $\mu$. But this is all quite informal, as the reinforcement learning
framework does not actually provide any mechanism for such an initial
coin-flip.

We will show that there is a way to combine $\pi$ and $\rho$ into a new
agent $\pi\oplus\rho$, within the formal framework, such that the expected
total reward when $\pi\oplus\rho$ interacts with $\mu$ is the average of the
expected total rewards when $\pi$ or $\rho$ interact with $\mu$.
Thus, at least up to expected total reward, $\pi\oplus\rho$ has the exact
same performance as the above informal coin-flipping agent.

The structure of this paper is as follows.

...

\section{Preliminaries}

(Define RL)

\section{The $\oplus$ operator}

\begin{definition}
\label{pullbackdef}
    Suppose $s$ is a finite sequence of alternating percepts and actions.
    Let $\pi$ be an agent. Let $\mu$ be an environment.
    \begin{enumerate}
        \item
        The \emph{probability of $s$ according to $\pi$}, written
        $\pi_*(s)$, is the probability that when $\pi$ and $\mu$ interact,
        the interaction begins with $s$, given that all
        percepts are as in $s$. Formally we define $\pi_*(s)$ by induction:
        \begin{enumerate}
            \item
            If $s$ contains no action, then $\pi_*(s)=1$.
            \item
            If $s=t\frown a$ (where $a$ is an action) or $s=t\frown a\frown p$
            (where $a$ is an action and $p$ is a percept),
            then $\pi_*(s)=\pi_*(t)\pi(a|t)$.
        \end{enumerate}
        \item
        The \emph{probability of $s$ according to $\mu$}, written
        $\mu_*(s)$, is the probability that when $\pi$ and $\mu$ interact,
        the interaction begins with $s$, given that all
        actions are as in $s$. Formally we define $\mu_*(s)$ by induction:
        \begin{enumerate}
            \item
            If $s$ has length $0$, then $\mu_*(s)=1$.
            \item
            If $s=t\frown p$ (where $p$ is a percept) or $s=t\frown p\frown a$
            (where $a$ is an action and $p$ is a percept),
            then $\mu_*(s)=\mu_*(t)\mu(p|t)$.
        \end{enumerate}
        \item
        The \emph{probability of $s$ according to $\pi$ and $\mu$},
        written $(\pi,\mu)_*(s)$, is the probability that when $\pi$ and
        $\mu$ interact, the interactin begins with $s$.
        Formally, we define $(\pi,\mu)_*(s)$ by induction:
        \begin{enumerate}
            \item
            If $s$ has length $0$, then $(\pi,\mu)_*(s)=1$.
            \item
            If $s=t\frown p$ (where $p$ is a percept),
            then $(\pi,\mu)_*(s)=(\pi,\mu)_*(t)\mu(p|t)$.
            \item
            If $s=t\frown a$ (where $a$ is an action),
            then $(\pi,\mu)_*(s)=(\pi,\mu)_*(t)\pi(a|t)$.
        \end{enumerate}
    \end{enumerate}
\end{definition}

\begin{lemma}
\label{factorizationlemma}
    For all $s$, $\pi$, $\mu$ as in Definition \ref{pullbackdef},
    \[
        (\pi,\mu)_*(s) = \pi_*(s)\mu_*(s).
    \]
\end{lemma}

\begin{proof}
    By induction.
\end{proof}

\begin{lemma}
\label{basicprobabilitylemma}
    $V^\pi_{\mu,n}=\sum_{s\in S_n}(\pi,\mu)_*(s)r(s),$
    where $S_n$ is the set of all length-$n$ initial
    percept-action sequences and $r(s)$ is the total reward in $s$.
\end{lemma}

\begin{proof}
    Basic probability theory.
\end{proof}

\begin{definition}
\label{maindefn}
    Assume $\pi$ and $\rho$ are agents.
    Define the new agent $\pi\oplus\rho$ by
    \[
        (\pi\oplus\rho)(a|s)
        =
        \frac{\pi_*(s\frown a) + \rho_*(s\frown a)}{\pi_*(s)+\rho_*(s)}
    \]
    provided $\pi_*(s)+\rho_*(s)>0$; otherwise,
    let $(\pi\oplus\rho)(a|s)=1/|\mathcal{A}|$.
\end{definition}

\begin{lemma}
    If $\pi$ and $\rho$ are agents then $\pi\oplus\rho$ really is an agent.
\end{lemma}

\begin{proof}
    We must show $\sum_{a\in\mathcal A}(\pi\oplus\rho)(a|s)=1$ for every
    percept-action sequence $s$ ending in a percept. Fix some such $s$.

    Case 1: $\pi_*(s)=\rho_*(s)=0$. Then by definition each
    $(\pi\oplus\rho)(a|s)=1/|\mathcal A|$ so the claim is immediate.

    Case 2: $\pi_*(s)+\rho_*(s)>0$. Then
    \begin{align*}
        \sum_{a\in\mathcal A}(\pi\oplus\rho)(a|s)
            &= \sum_{a\in\mathcal A}
                \frac{\pi_*(s\frown a)+\rho_*(s\frown a)}{\pi_*(s)+\rho_*(s)}
                &\mbox{(Definition \ref{maindefn})}\\
            &= \sum_{a\in\mathcal A}
                \frac{\pi_*(s)\pi(a|s)+\rho_*(s)\rho(a|s)}{\pi_*(s)+\rho_*(s)}
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= \frac{
                \pi_*(s)\left(\mbox{$\sum_{a\in\mathcal A}\pi(a|s)$}\right)
                +
                \rho_*(s)\left(\mbox{$\sum_{a\in\mathcal A}\rho(a|s)$}\right)
                }{\pi_*(s)+\rho_*(s)}
                &\mbox{(Algebra)}\\
            &= \frac{\pi_*(s)+\rho_*(s)}{\pi_*(s)+\rho_*(s)}=1.
                &\mbox{($\pi$, $\rho$ are agents)}
    \end{align*}
\end{proof}

\begin{theorem}
\label{maintheorem}
    For all agents $\pi$ and $\rho$, for every environment $\mu$,
    if $V^\pi_\mu$ and $V^\rho_\mu$ converge, then
    \[
        V^{\pi\oplus\rho}_\mu = \frac12(V^\pi_\mu+V^\rho_\mu).
    \]
\end{theorem}

\begin{proof}
    It suffices to show that for every $n\in\mathbb N$,
    $V^{\pi\oplus\rho}_{\mu,n}=\frac12(V^\pi_{\mu,n}+V^\rho_{\mu,n})$.
    Let $n\in\mathbb N$. Let $S_n$ be the set of all length-$n$ percept-action
    sequences, and for each $s\in S_n$, let $r(s)$ be the sum of rewards in $s$.
    By Lemma \ref{basicprobabilitylemma}, it suffices to show that
    $(\pi\oplus\rho,\mu)_*(s)=\frac12((\pi,\mu)_*(s)+(\rho,\mu)_*(s))$ for each
    $s\in S_n$. Fix $s\in S_n$.

    Case 1: $s=\langle\rangle$. Then
    $(\pi\oplus\rho,\mu)_*(s)=1=\frac12(1+1)=\frac12((\pi,\mu)_*(s)+(\rho,\mu)_*(s))$.

    Case 2: $s=t\frown p$ for some percept $p$. Then
    \begin{align*}
        (\pi\oplus\rho,\mu)_*(s) &= (\pi\oplus\rho,\mu)_*(t\frown p)\\
            &= (\pi\oplus\rho,\mu)_*(t)\mu(p|t)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= ((\pi,\mu)_*(t)+(\rho,\mu)_*(t))\mu(p|t)/2
                &\mbox{(Induction)}\\
            &= ((\pi,\mu)_*(t)\mu(p|t) + (\rho,\mu)_*(t)\mu(p|t))/2
                &\mbox{(Algebra)}\\
            &= ((\pi,\mu)_*(t\frown p) + (\rho,\mu)_*(t\frown p))/2
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= ((\pi,\mu)_*(s) + (\rho,\mu)_*(s))/2,
    \end{align*}
    as desired.

    Case 3: $s=t\frown a$ for some action $a$.

    Subcase 3.1: $\pi_*(t)=\rho_*(t)=0$.
        By Lemma \ref{factorizationlemma}, $(\pi,\mu)_*(t)=(\rho,\mu)_*(t)=0$.
        So by induction, $(\pi\oplus\rho,\mu)_*(t)=\frac12(0+0)=0$.
        By Definition \ref{pullbackdef},
        \[
            (\pi,\mu)_*(s)=(\pi,\mu)_*(t\frown a)=(\pi,\mu)_*(t)\pi(a|t)=0\pi(a|t)=0,
        \]
        and similarly $(\rho,\mu)_*(s)=0$ and $(\pi\oplus\rho,\mu)_*(s)=0$.
        Thus $(\pi\oplus\rho,\mu)_*(s)=\frac12((\pi,\mu)_*(s)+(\rho,\mu)_*(s))$
        as desired.

    Subcase 3.2: $\pi_*(t)+\rho_*(t)>0$. Then
    \begin{align*}
        (\pi\oplus\rho,\mu)_*(s) &= (\pi\oplus\rho,\mu)_*(t\frown a)\\
            &= (\pi\oplus\rho,\mu)_*(t)(\pi\oplus\rho)(a|t)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= (\pi\oplus\rho,\mu)_*(t)\frac{\pi_*(t\frown a)+\rho_*(t\frown a)}
                {\pi_*(t)+\rho_*(t)}
                &\mbox{(Definition \ref{maindefn})}\\
            &= (\pi\oplus\rho,\mu)_*(t)\frac{\pi_*(t)\pi(a|t)+\rho_*(t)\rho(a|t)}
                {\pi_*(t)+\rho_*(t)}
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= \frac{(\pi,\mu)_*(t)+(\rho,\mu)_*(t)}2
                \frac{\pi_*(t)\pi(a|t)+\rho_*(t)\rho(a|t)}{\pi_*(t)+\rho_*(t)}
                &\mbox{(Induction)}\\
            &= \frac{(\pi_*(t)+\rho_*(t))\mu_*(t)}2
                \frac{\pi_*(t)\pi(a|t)+\rho_*(t)\rho(a|t)}{\pi_*(t)+\rho_*(t)}
                &\mbox{(Lemma \ref{factorizationlemma})}\\
            &= (\pi_*(t)\mu_*(t)\pi(a|t) + \rho_*(t)\mu_*(t)\rho(a|t))/2
                &\mbox{(Algebra)}\\
            &= ((\pi,\mu)_*(t)\pi(a|t) + (\rho,\mu)_*(t)\rho(a|t))/2
                &\mbox{(Lemma \ref{factorizationlemma})}\\
            &= ((\pi,\mu)_*(t\frown a) + (\rho,\mu)_*(t\frown a))/2,
                &\mbox{(Definition \ref{pullbackdef})}
    \end{align*}
    as desired.
\end{proof}

\section{Duality and Janus agents:
Strengthening and formalizing a result of Alexander and Hutter}

\begin{definition}
    (Duality) Define $\overline s$, $\overline \pi$, $\overline \mu$
    as in Reward-Punishment Symmetric Universal Intelligence.
\end{definition}

\begin{definition}
    By a \emph{weighted performance averager}, we mean a function
    $\Upsilon$ assigning real numbers to agents, such that there
    are weights $\{w_\mu\}_{\mu\in W}$ such that for every agent
    $\pi$, $\Upsilon(\pi)=\sum_{\mu\in W}w_\mu V^\pi_\mu$.
\end{definition}

\begin{theorem}
\label{averageperformancelemma}
    For any weighted performance averager $\mu$, for any agents
    $\pi$ and $\rho$, $\Upsilon(\pi\oplus\rho)\frac12(\Upsilon(\pi)+\Upsilon(\rho))$.
\end{theorem}

\begin{proof}
    Follows from Theorem \ref{maintheorem}.
\end{proof}

The following class of agents are named after Janus, the Roman god of duality,
who features two faces, one facing forward, one facing backward.

\begin{definition}
    (Janus agents)
    By a \emph{Janus agent}, we mean an agent $\pi$ such that
    $\overline{\pi}=\pi$.
\end{definition}

\begin{definition}
\label{equivdefn}
    If $\pi$ and $\rho$ are agents, we say $\pi\equiv\rho$ if the
    following conditions hold:
    \begin{enumerate}
        \item For every history $h$, $\pi_*(h)=0$ iff $\rho_*(h)=0$.
        \item For every history $h$ ending with a percept,
            for every action $a$, if $\pi_*(h)\not=0$,
            then $\rho(a|h)=\pi(a|h)$.
    \end{enumerate}
\end{definition}

\begin{lemma}
\label{equivrelationlemma}
    The relation $\equiv$ of Definition \ref{equivdefn} is an equivalence
    relation.
\end{lemma}

\begin{proof}
    Straightforward.
\end{proof}

\begin{lemma}
\label{piopluspilemma}
    For any agent $\pi$, $\pi\equiv\pi\oplus\pi$.
\end{lemma}

\begin{proof}
    We prove the two conditions of Definition \ref{equivdefn} by
    induction on history $h$.
 
    Case 1: $h$ contains no actions. Then $\pi_*(h)=(\pi\oplus\pi)_*(h)=1$, so
    certainly $\pi_*(h)=0$ iff $\rho_*(h)=0$ (proving condition 1 of
    Definition \ref{equivdefn}).
    For condition 2, we check:
    \begin{align*}
        (\pi\oplus\pi)(a|h)
            &= \frac{\pi_*(h\frown a) + \pi_*(h\frown a)}{\pi_*(h)+\pi_*(h)}\\
            &= \frac{\pi_*(h)+\pi_*(h)}{\pi_*(h)+\pi_*(h)}\pi(a|h)\\
            &= \pi(a|h),
    \end{align*}
    as desired.

    Case 2: $h$ ends with an action, say $h=h_0\frown a_0$ where $h_0$ ends with
        a percept.
        By induction, we may assume (condition 1)
        $\pi_*(h_0)=0$ iff $(\pi\oplus\pi)_*(h_0)=0$,
        and (condition 2) if $\pi_*(h_0)\not=0$ then
        $(\pi\oplus\pi)(a_0|h_0)=\pi(a_0|h_0)$.
        If $\pi_*(h_0)=0$ (and hence $(\pi\oplus\pi)_*(h_0)=0$) then clearly
        $\pi_*(h)=0$ and $(\pi\oplus\pi)_*(h)=0$. But assume $\pi_*(h_0)\not=0$.
        Then by a similar calculation as in Case 1,
        $(\pi\oplus\pi)(a_0|h_0)=\pi(a_0|h_0)$.
        This implies $\pi_*(h)=0$ iff $(\pi\oplus\pi)_*(h)=0$, proving condition 1.
        For condition 2 there is nothing to prove since $h$ does not end with a percept.

    Case 3: $h$ ends with a percept, say $h=h_0\frown p$, and $h$ contains at least one action.
        By induction, conditions 1 and 2 hold for $h_0$.
        Since $\pi_*(h_0\frown p)=\pi_*(h_0)$ and
        $(\pi\oplus\pi)_*(h_0\frown p)=(\pi\oplus\pi)_*(h_0)$,
        condition 1 for $h$ follows.
        For condition 2, let $a$ be any action and assume
        $\pi_*(h)\not=0$. Then $(\pi\oplus\pi)(a|h)=\pi(a|h)$ by the
        same calculation as in Case 1.
\end{proof}

\begin{proposition}
    (Characterization of Janus agents)
    For any agent $\pi$, the following are equivalent:
    \begin{enumerate}
        \item $\pi\equiv\rho$ for some Janus agent $\rho$.
        \item $\pi\equiv\rho\oplus\overline{\rho}$ for some agent $\rho$.
    \end{enumerate}
\end{proposition}

\begin{proof}
    ($\Rightarrow$)
    Assume $\pi\equiv\rho$ for some Janus agent $\rho$.
    Then $\rho=\overline{\rho}$, so $\rho\equiv \rho\oplus\overline{\rho}$ by
    Lemma \ref{piopluspilemma}, thus, by Lemma \ref{equivrelationlemma},
    $\pi\equiv\rho\oplus\overline{\rho}$.

    ($\Leftarrow$)
    Assume $\pi\equiv\rho\oplus\overline{\rho}$ for some agent $\rho$.
    Clearly
    \[
        \overline{\rho\oplus\overline{\rho}}
        =
        \overline{\rho}\oplus\overline{\overline{\rho}}
        =
        \overline{\rho}\oplus\rho
        =
        \rho\oplus\overline{\rho},
    \]
    so $\rho\oplus\overline{\rho}$ is a Janus agent, proving that
    $\pi$ is $\equiv$ some Janus agent.
\end{proof}

\begin{theorem}
    Suppose $\Upsilon$ is a weighted performance averager.
    If $\Upsilon(\pi)=0$ for every Janus agent $\pi$,
    then $\Upsilon(\overline{\pi})=-\Upsilon(\pi)$
    for every agent $\pi$.
\end{theorem}

\begin{proof}
    Let $\pi$ be any agent.
    By assumption, $\Upsilon(\pi\oplus\overline{\pi})=0$.
    Thus by Theorem \ref{averageperformancelemma},
    $0=\frac12(\Upsilon(\pi)+\Upsilon(\overline{\pi})$,
    so $\Upsilon(\overline{\pi})=-\Upsilon(\pi)$.
\end{proof}


\section{Detectability of sets of agents}

\begin{definition}
\label{incentivizabilitydefn}
    A set $\Pi$ of agents is \emph{detectable} if there exists
    an environment $\mu$ such that for every agent $\pi$:
    \begin{enumerate}
        \item
        $V^\pi_\mu$ exists.
        \item
        $V^\pi_\mu>0$ if $\pi\in\Pi$.
        \item
        $V^\pi_\mu\leq 0$ if $\pi\not\in\Pi$.
    \end{enumerate}
\end{definition}

\begin{definition}
    A set $\Pi$ is \emph{closed under $\oplus$} if the following
    condition holds: whenever $\pi\in\Pi$ and $\rho\in\Pi$,
    then $\pi\oplus\rho\in\Pi$.
\end{definition}

\begin{theorem}
\label{closuretheorem}
    Let $\Pi$ be any set of agents.
    If $\Pi$ is detectable, then $\Pi$ is closed under $\oplus$, and so
    is its complement $\Pi^c$.
\end{theorem}

\begin{proof}
    Assume $\Pi$ is detectable.
    Let $\mu$ be as in
    Definition \ref{incentivizabilitydefn}.
    To see $\Pi$ is closed under $\oplus$, let $\pi,\rho\in\Pi$.
    Then $V^\pi_\mu>0$ and $V^\rho_\mu>0$, thus
    $V^{\pi\oplus\rho}_\mu>0$ since
    $V^{\pi\oplus\rho}_\mu=\frac12(V^\pi_\mu+V^\rho_\mu)$ by Theorem
    \ref{maintheorem}.
    So by choice of $\mu$, $\pi\oplus\rho\in \Pi$.
    A similar argument shows that $\Pi^c$ is closed under $\oplus$.
\end{proof}

\begin{definition}
    An agent $\pi$ is \emph{Markov} if
    $\pi(a|s)$ only depends on the most recent observation in $s$.
\end{definition}

We will not use the following Lemma, but we state it in order to
make it clearer what exactly a Markov agent is.

\begin{lemma}
    By a \emph{policy} we mean a function $f$ which takes a single observation
    $o\in\mathcal O$ as input, and outputs a probability distribution
    $a\mapsto f(a|o)$ on $\mathcal A$.
    For each policy $f$, let $\hat f$, the \emph{agent defined by $f$},
    be the agent defined as follows:
    for every $s$ with most recent observation $o$,
    $\hat f(a|s)=f(a|o)$.
    Then:
    The set of Markov agents is exactly $\{\hat f\,:\,\mbox{$f$ is a policy}\}$.
\end{lemma}

\begin{proof}
    Straightforward.
\end{proof}

\begin{theorem}
\label{markovcorollary}
    The set of Markov agents is not detectable.
\end{theorem}

\begin{proof}
    Let $\Pi$ be the set of Markov agents.
    By Theorem \ref{closuretheorem}, it suffices to show $\Pi$ is not
    closed under $\oplus$.
    Since $|\mathcal A|>1$, we may choose two distinct $a_1,a_2\in\mathcal A$.
    Define agents $\pi,\rho$ by
    \begin{align*}
        \pi(a|s)
        &=
        \begin{cases}
            .9 & \mbox{if $a=a_1$,}\\
            .1 & \mbox{if $a=a_2$,}\\
            0 & \mbox{otherwise;}
        \end{cases}\\
        \rho(a|s)
        &=
        \begin{cases}
            .1 & \mbox{if $a=a_1$,}\\
            .9 & \mbox{if $a=a_2$,}\\
            0 & \mbox{otherwise.}
        \end{cases}
    \end{align*}
    Let $p$ be any percept,
    let $s_1=\langle p\rangle$, and let $s_2=\langle p,a_1,p\rangle$.
    Then
    \[
        (\pi\oplus\rho)(a_1|s_1)
        = \frac{\pi_*(s_1\frown a_1) + \rho_*(s_1\frown a_1)}{\pi_*(s_1)+\rho_*(s_1)}
        = \frac{0.9 + 0.1}{1+1} = \frac12
    \]
    and
    \[
        (\pi\oplus\rho)(a_1|s_2)
        = \frac{\pi_*(s_2\frown a_1) + \rho_*(s_2\frown a_1)}{\pi_*(s_2)+\rho_*(s_2)}
        = \frac{0.9\cdot 0.9 + 0.1\cdot 0.1}{0.9+0.1}=\frac{82}{100}\not=\frac12.
    \]
    So $(\pi\oplus\rho)(a|s)$ depends on more than just the most recent
    observation in $s$, so $\pi\oplus\rho$ is not Markov.
\end{proof}


\section{Genericness of non-deterministic agents}

\begin{definition}
\label{modifyagentatoneplace}
    If $\pi$ is an agent, $h_0$ is a history ending in a percept,
    and $m$ is a probability distribution on $\mathcal A$,
    we write $\pi^{h_0\mapsto m}$ for the agent which is identical to $\pi$
    except that it maps $h_0$ to $m$, in other words:
    \[
        \pi^{h\mapsto m}(a|h)
        =
        \begin{cases}
            \pi(a|h) &\mbox{if $h\not=h_0$,}\\
            m(a) &\mbox{if $h=h_0$.}
        \end{cases}
    \]
\end{definition}

\begin{lemma}
\label{firsttechlemmaforgenericity}
    Suppose $\pi$, $h_0$, $m$ are as in Definition \ref{modifyagentatoneplace}.
    Suppose $h$ is a history such that
    for every $a\in\mathcal A$,
    $h_0\frown a$ is not an initial segment of $h$.
    Then $\pi^{h_0\mapsto m}_*(h)=\pi_*(h)$.
\end{lemma}

\begin{proof}
    By induction on $h$.
\end{proof}

\begin{lemma}
\label{thirdtechlemmaforgenericity}
    Suppose $\pi$, $h_0$, $m$ are as in Definition \ref{modifyagentatoneplace}.
    For any action $a\in\mathcal A$,
    $\pi^{h_0\mapsto m}_*(h_0\frown a)=\pi_*(h_0)m(a)$.
\end{lemma}

\begin{proof}
    Immediate by Definition \ref{pullbackdef} and Lemma \ref{firsttechlemmaforgenericity}.
\end{proof}

\begin{lemma}
\label{secondtechlemmaforgenericity}
    Suppose $\pi$, $h_0$, $m$ are as in Definition \ref{modifyagentatoneplace}.
    Suppose $h$ is a history and $a_0$ is an action and that $h_0\frown a_0$ is
    an initial segment of $h$. Assume $\pi(a_0|h_0)\not=0$. Then
    $\pi^{h_0\mapsto m}_*(h) = \frac{\pi_*(h)m(a_0)}{\pi(a_0|h_0)}$.
\end{lemma}

\begin{proof}
    By induction on $h$.

    Case 1: $h=h_0\frown a$. Then
    \begin{align*}
        \pi^{h_0\mapsto m}_*(h)
        &= \pi^{h_0\mapsto m}_*(h_0)\pi^{h_0\mapsto m}(a_0|h_0)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi_*(h_0)\pi^{h_0\mapsto m}(a_0|h_0)
            &\mbox{(Lemma \ref{firsttechlemmaforgenericity})}\\
        &= \pi_*(h_0)m(a_0)
            &\mbox{(Definition \ref{modifyagentatoneplace})}\\
        &= \pi_*(h_0)\pi(a_0|h_0)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Basic Algebra)}\\
        &= \pi_*(h)m(a_0)/\pi(a_0|h_0).
            &\mbox{(Definition \ref{pullbackdef})}
    \end{align*}

    Case 2: $h=h_0\frown a_0\frown h_1\frown p$ for some history $h_1$
        and some percept $p$. Then
    \begin{align*}
        \pi^{h_0\mapsto m}_*(h)
        &= \pi^{h_0\mapsto m}_*(h_0\frown a_0\frown h_1)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi_*(h_0\frown a_0\frown h_1)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Induction)}\\
        &= \pi_*(h_0\frown a_0\frown h_1\frown p)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi_*(h)m(a_0)/\pi(a_0|h_0).
    \end{align*}

    Case 3: $h=h_0\frown a_0\frown h_1\frown a$ for some history $h_1$
        and some action $a$. Then
    \begin{align*}
        \pi^{h_0\mapsto m}_*(h)
        &= \pi^{h_0\mapsto m}_*(h_0\frown a_0\frown h_1)
            \pi^{h_0\mapsto m}(a|h_0\frown a\frown h_1)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi^{h_0\mapsto m}_*(h_0\frown a_0\frown h_1)
            \pi(a|h_0\frown a\frown h_1)
            &\mbox{(Definition \ref{modifyagentatoneplace})}\\
        &= \pi_*(h_0\frown a_0\frown h_1)\pi(a|h_0\frown a\frown h_1)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Induction)}\\
        &= \pi_*(h_0\frown a_0\frown h_1\frown a)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi_*(h)m(a_0)/\pi(a_0|h_0).
    \end{align*}
\end{proof}

\begin{definition}
\label{sumofdistros}
    Suppose $m$ and $m_1,\ldots,m_n$ are probability distributions on $\mathcal A$
    and $r_1,\ldots,r_n$ are nonnegative real numbers with
    $r_1+\cdots+r_n=1$. By $r_1m_1+\cdots+r_nm_n$ we mean the probability distribution
    on $\mathcal A$ defined by
    \[
        (r_1m_1+\cdots+r_nm_n)(a) = r_1m_1(a) + \cdots + r_nm_n(a).
    \]
\end{definition}

\begin{lemma}
    If $m$, $m_1,\ldots,m_n$, $r_1,\ldots,r_n$ are as in Definition \ref{sumofdistros}
    then $r_1m_1+\cdots+r_nm_n$ really is a probability distribution on $\mathcal A$.
\end{lemma}

\begin{proof}
    Clearly for every $a\in\mathcal A$,
    $(r_1m_1+\cdots+r_nm_n)(a) = r_1m_1(a) + \cdots + r_nm_n(a)$ is a nonnegative
    real number. It remains to show $\sum_{a\in\mathcal A}(r_1m_1+\cdots+r_nm_n)(a)=1$.
    We compute:
    \begin{align*}
        &{} \sum_{a\in\mathcal A}(r_1m_1+\cdots+r_nm_n)(a)\\
        &=
        \sum_{a\in\mathcal A} r_1m_1(a) + \cdots + r_nm_n(a)
            &\mbox{(Definition \ref{sumofdistros})}\\
        &=
        r_1\sum_{a\in\mathcal A} m_1(a) + \cdots + r_n\sum_{a\in\mathcal A}m_n(a)
            &\mbox{(Basic Algebra)}\\
        &= r_1 + \cdots + r_n
            &\mbox{($m_1,\ldots,m_n$ are probability distr's)}\\
        &= 1.
            &\mbox{(By assumption)}
    \end{align*}
\end{proof}

\begin{proposition}
\label{longproposition}
    Let $\Upsilon$ be any weighted performance averager, let $\pi$ be any agent,
    let $h$ be any history ending in a percept.
    Suppose $m_1,\ldots,m_n$ are probability distributions on $\mathcal A$
    and $r_1,\ldots,r_n$ are nonnegative reals such that $r_1+\cdots+r_n=1$
    and $r_1m_1+\cdots+r_nm_n=\pi(\bullet|h)$.
    Then
    \[
        \Upsilon(\pi)
        =
        \Upsilon(r_1\pi^{h\mapsto m_1} \oplus \cdots \oplus r_n\pi^{h\mapsto m_n}).
    \]
\end{proposition}

\begin{proof}
    For brevity, write $\pi^i$ for $\pi^{h\mapsto m_i}$.

    It suffices to show that for every well-behaved $\mu$ and every $n\in\mathbb N$,
    \[
        V^{\pi}_{\mu,n}
        =
        V^{r_1\pi^1 \oplus \cdots \oplus r_n\pi^n}_{\mu,n}.
    \]
    By Lemma \ref{basicprobabilitylemma}, it suffices to show that for every
    well-behaved $\mu$ and every finite percept-action sequence $s$,
    $
    (\pi,\mu)_*(s)
    =
    (r_1\pi^1 \oplus \cdots \oplus r_n\pi^n,\mu)_*(s)
    $.
    We will show even more. We will show that for every percept-action sequence
    $s$, $\pi_*(s)=(r_1\pi^1 \oplus \cdots \oplus r_n\pi^n)_*(s)$.
    We prove this by induction on $s$.

    Case 1: $s=\langle\rangle$.
    Then $\pi_*(s)=1
    =(r_1\pi^1 \oplus \cdots \oplus r_n\pi^n)_*(s)$.

    Case 2: $s=t\frown p$ for some percept $p$.
    Then
    \begin{align*}
        \pi_*(s)
            &= \pi_*(t\frown p)\\
            &= \pi_*(t)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= (r_1\pi^1 \oplus \cdots \oplus r_n\pi^n)_*(t)\mu(p|t)
                &\mbox{(Induction)}\\
            &= (r_1\pi^1 \oplus \cdots \oplus r_n\pi^n)_*(t\frown p)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= (r_1\pi^1 \oplus \cdots \oplus r_n\pi^n)_*(s).
    \end{align*}

    Case 3: $s=t\frown a$ for some action $a$.

    Subcase 3.1: $\pi_*(t)=0$.
    Then
    \begin{align*}
        &{} (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(s)\\
            &= (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(t)
            (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|t)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= \pi_*(t)(r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|t)
                &\mbox{(Induction)}\\
            &= 0,
    \end{align*}
    and similarly $\pi_*(s)=0$.

    Subcase 3.2: $\pi_*(t)\not=0$ and $t=h$.

    We would like to use Definition \ref{maindefn} to rewrite
    $(r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|h)$ but first we must check
    that the resulting denominator does not vanish.
    Compute ($*$):
    \begin{align*}
        &{} r_1\pi^1_*(h)+\cdots+r_n\pi^n_*(h)\\
            &= r_1\pi_*(h)+\cdots+r_n\pi_*(h)
                &\mbox{(Lemma \ref{firsttechlemmaforgenericity})}\\
            &= \pi_*(h)\not=0.
                &\mbox{($r_1+\cdots+r_n=1$)}
    \end{align*}
    Thus:
    \begin{align*}
        &{} (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(h\frown a)\\
            &= (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(h)
                (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|h)
                    &\mbox{(Definition \ref{pullbackdef})}\\
            &= \pi_*(h)(r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|h)
                    &\mbox{(Induction)}\\
            &= \pi_*(h)
                \frac
                {r_1\pi^1_*(h\frown a)+\cdots+r_n\pi^n_*(h\frown a)}
                {r_1\pi^1_*(h)+\cdots+r_n\pi^n_*(h)}
                    &\mbox{(Definition \ref{maindefn})}\\
            &= r_1\pi^1_*(h\frown a)+\cdots+r_n\pi^n_*(h\frown a)
                    &\mbox{(By $*$)}\\
            &= r_1\pi_*(h)m_1(a)+\cdots+r_n\pi_*(h)m_n(a)
                    &\mbox{(Lemma \ref{thirdtechlemmaforgenericity})}\\
            &= \pi_*(h)\pi(a|h)
                    &\mbox{($r_1m_1+\cdots+r_nm_n=\pi(\bullet|h)$)}\\
            &= \pi_*(h\frown a),
                    &\mbox{(Definition \ref{pullbackdef})}
    \end{align*}
    as desired.

    Subcase 3.3: $\pi_*(t)\not=0$, $t\not=h$, and
    $t$ has initial segment $h\frown a_0$ for some action $a_0$.

    It follows that $\pi(a_0|h)\not=0$ because otherwise clearly
    we would have $\pi_*(t)=0$.

    We would like to use Definition \ref{maindefn} to rewrite
    $(r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|t)$ but first we must
    check that the resulting denominator does not vanish. We compute ($**$):
    \begin{align*}
        &{} r_1\pi^1_*(t) + \cdots + r_n\pi^n_*(t)\\
            &= \frac{r_1\pi_*(t)m_1(a_0)}{\pi(a_0|h)}
                + \cdots +
                \frac{r_n\pi_*(t)m_n(a_0)}{\pi(a_0|h)}
                    &\mbox{(Lemma \ref{secondtechlemmaforgenericity})}\\
            &= \frac{\pi_*(t)}{\pi(a_0|h)}
                (r_1m_1(a_0) + \cdots + r_nm_n(a_0))
                    &\mbox{(Basic Algebra)}\\
            &= \frac{\pi_*(t)}{\pi(a_0|h)}\pi(a_0|h)
                    &\mbox{($r_1m_1+\cdots+r_nm_n=\pi(\bullet|h)$)}\\
            &= \pi_*(t)\not=0.
                    &\mbox{(Basic Algebra)}
    \end{align*}
    Thus
    \begin{align*}
        &{} (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(s)\\
            &= (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(t)
                (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|t)
                    &\mbox{(Definition \ref{pullbackdef})}\\
            &= \pi_*(t)(r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|t)
                    &\mbox{(Induction)}\\
            &= \pi_*(t)\frac{r_1\pi^1_*(t\frown a)+\cdots+r_n\pi^n(t\frown a)}
                {r_1\pi^1_*(t)+\cdots+r_n\pi^n_*(t)}
                    &\mbox{(Definition \ref{maindefn})}\\
            &= r_1\pi^1_*(t\frown a)+\cdots+r_n\pi^n(t\frown a)
                    &\mbox{(By $**$)}\\
            &= r_1\frac{\pi_*(t\frown a)m_1(a_0)}{\pi(a_0|h)}
                +\cdots+r_n\frac{\pi_*(t\frown a)m_n(a_0)}{\pi(a_0|h)}
                    &\mbox{(Lemma \ref{secondtechlemmaforgenericity})}\\
            &= \frac{\pi_*(t\frown a)}{\pi(a_0|h)}(r_1m_1(a_0)+\cdots+r_nm_n(a_0))
                    &\mbox{(Basic Algebra)}\\
            &= \frac{\pi_*(t\frown a)}{\pi(a_0|h)}\pi(a_0|h)
                    &\mbox{($r_1m_1+\cdots+r_nm_n=\pi(\bullet|h)$)}\\
            &= \pi_*(t\frown a) = \pi_*(s),
    \end{align*}
    as desired.

    Subcase 3.4: $\pi_*(t)\not=0$, $t\not=h$, and $t$ has no initial segment
        of the form $h\frown a_0$.

    We would like to use Definition \ref{maindefn} to rewrite
    $(r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|h)$ but first we must
    check that the resulting denominator does not vanish:
    by the same reasoning as in Subcase 3.2, we have
    ($*$) $(r_1\pi^1_*(t)+\cdots+r_n\pi^n_*(t))=\pi_*(t)\not=0$.
    Thus
    \begin{align*}
        &{} (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(t\frown a)\\
            &= (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(t)
                (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|t)
                    &\mbox{(Definition \ref{pullbackdef})}\\
            &= \pi_*(t)(r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|t)
                    &\mbox{(Induction)}\\
            &= \pi_*(t)\frac{r_1\pi^1_*(t\frown a)+\cdots+r_n\pi^n(t\frown a)}
                {r_1\pi^1_*(t)+\cdots+r_n\pi^n_*(t)}
                    &\mbox{(Definition \ref{maindefn})}\\
            &= r_1\pi^1_*(t\frown a)+\cdots+r_n\pi^n(t\frown a)
                    &\mbox{(By $*$)}\\
            &= r_1\pi_*(t\frown a)+\cdots+r_n\pi_*(t\frown a)
                    &\mbox{(Lemma \ref{firsttechlemmaforgenericity})}\\
            &= \pi_*(t\frown a),
                    &\mbox{($r_1+\cdots+r_n=1$)}
    \end{align*}
    as desired.
\end{proof}

\begin{theorem}
\label{pointwisegenericnessthm}
    (Pointwise Genericness of Non-Deterministic Intelligence)
    Let $\Upsilon$ be any weighted performance averager and let
    $\pi$ be an agent.
    Let $h$ be a history ending in a percept.
    For any probability distributions $m_1,\ldots,m_n$ on $\mathcal A$,
    for any nonnegative reals $r_1,\ldots,r_n$ with $r_1+\cdots+r_n=1$,
    if $r_1m_1+\cdots+r_nm_n=\pi(\bullet|h)$,
    then one of the following is true:
    \begin{enumerate}
        \item For each $i=1,\ldots,n$, $\Upsilon(\pi^{h\mapsto m_i})=\Upsilon(\pi)$.
        \item For some $i=1,\ldots,n$, $\Upsilon(\pi^{h\mapsto m_i})>\Upsilon(\pi)$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    If not, then for each $i=1,\ldots,n$, $\Upsilon(\pi^{h\mapsto m_i})\leq\Upsilon(\pi)$,
    and for some $i$, $\Upsilon(\pi^{h\mapsto m_i})<\Upsilon(\pi)$.
    This implies
    \begin{align*}
        \Upsilon(\pi)
            &= \Upsilon(r_1\pi^{h\mapsto m_1}\oplus\cdots\oplus r_n\pi^{h\mapsto m_n})
                &\mbox{(Proposition \ref{longproposition})}\\
            &= r_1\Upsilon(\pi^{h\mapsto m_1})+\cdots+r_n\Upsilon(\pi^{h\mapsto m_n})
                &\mbox{(Theorem \ref{averageperformancelemma})}\\
            &< r_1\Upsilon(\pi)+\cdots+r_n\Upsilon(\pi)
                &\mbox{(Assumption)}\\
            &= \Upsilon(\pi),
                &\mbox{($r_1+\cdots+r_n=1$)}
    \end{align*}
    absurd.
\end{proof}

Theorem \ref{pointwisegenericnessthm} is interesting because it implies that for
any intelligent agent $\pi$, for any history $h$, if $\pi(\bullet|h)$ is not deterministic,
then the intelligence of $\pi$ does not critically depend on the specific probabilities
which $\pi(\bullet|h)$ assigns to different actions. For any decomposition
$\pi(\bullet|h)=r_1m_1+\cdots+r_nm_n$ of $\pi(\bullet|h)$ into competing probability
distributions $m_i$ (with $r_1+\cdots+r_m=1$), either $\pi^{h\mapsto m_i}$ is more intelligent
than $\pi$ for some $i$, or else every $\pi^{h\mapsto m_i}$ has the same intelligence as
$\pi$. This is counter-intuitive because one might imagine that the specific probability
distribution $\pi(\bullet|h)$ was carefully chosen and optimized to maximize the intelligence
of $\pi$.

\begin{example}
\label{genericnessexample}
    Suppose $\pi$ is an agent and $h_0$ is some percept-action history ending in a
    percept. Assume $\mathcal A=\{a_1,a_2,a_3\}$.
    Suppose $\pi(a|h_0)=\frac13$ for every $a\in\mathcal A$.
    Define probability distributions $m_1,m_2,m_3$ on $\mathcal A$ by
    \begin{align*}
        m_1(a) &=
        \begin{cases}
            \frac16 &\mbox{if $a\in\{a_1,a_2\}$}\\
            \frac23 &\mbox{if $a=a_3$}
        \end{cases}\\
        m_2(a) &=
        \begin{cases}
            \frac16 &\mbox{if $a\in\{a_1,a_3\}$}\\
            \frac23 &\mbox{if $a=a_2$}
        \end{cases}\\
        m_3(a) &=
        \begin{cases}
            \frac16 &\mbox{if $a\in\{a_2,a_3\}$}\\
            \frac23 &\mbox{if $a=a_1$}
        \end{cases}
    \end{align*}
    Let $r_1=r_2=r_3=\frac13$. Then $r_1+r_2+r_3=1$ and
    it is easy to check
    $(r_1m_1+r_2m_2+r_3m_3)(a)=\frac13$ for every $a\in\mathcal A$,
    i.e., $r_1m_1+r_2m_2+r_3m_3=\pi(\bullet|h_0)$.
    By Theorem \ref{pointwisegenericnessthm},
    for any weighted performance averager $\Upsilon$,
    one of the following statements is true:
    \begin{enumerate}
        \item
        For all $i=1,2,3$, $\pi$'s intelligence would not change if we
        changed the value of $\pi(\bullet|h_0)$ to $m_i$. Or,
        \item
        For some $i=1,2,3$, $\pi$'s intelligence would increase if
        we changed the value of $\pi(\bullet|h_0)$ to $m_i$.
    \end{enumerate}
\end{example}

In Example \ref{genericnessexample}, Theorem \ref{pointwisegenericnessthm} shows
that the intelligence of $\pi$ does not crucially depend on the specific value
of $\pi(\bullet|h_0)$. This is counter-intuitive because, a priori, it seems
plausible that the value of $\pi(\bullet|h_0)$ could have been determined
experimentally in order to maximize $\pi$'s intelligence. We could imagine ourselves
saying: ``This particular value of $\pi(\bullet|h_0)$ is critical to $\pi$'s
performance. Any other value would make $\pi$ sub-optimal.''
Theorem \ref{pointwisegenericnessthm} shows that no such statement can be true.

\begin{corollary}
\label{nondeterminismcorollary}
    (Pointwise Unnecessariness of Non-Determinism)
    Suppose $\Upsilon$ is a weighted performance averager, $\pi$ is an agent,
    and $h_0$ is a percept-action history ending in a percept.
    Let $a_1,\ldots,a_n$ enumerate $\{a\in\mathcal A\,:\,\pi(a|h_0)\not=0\}$.
    For each $i=1,\ldots,n$, let $m_i$ be the
    deterministic $\mathcal A$-probability distribution
    \[
        m_i(a) = \begin{cases}
            1 &\mbox{if $a=a_i$,}\\
            0 &\mbox{if $a\not=a_i$.}
        \end{cases}
    \]
    Then one of the following is true:
    \begin{enumerate}
        \item
        For each $i=1,\ldots,n$, $\Upsilon(\pi^{h_0\mapsto m_i})=\Upsilon(\pi)$.
        \item
        For some $i=1,\ldots,n$, $\Upsilon(\pi^{h_0\mapsto m_i})>\Upsilon(\pi)$.
    \end{enumerate}
\end{corollary}

\begin{proof}
    By Theorem \ref{pointwisegenericnessthm} with
    $r_i=\pi(a_i|h_0)$ for $i=1,\ldots,n$.
\end{proof}

Corollary \ref{nondeterminismcorollary} shows that for any individual
history $h_0$, the intelligence of $\pi$ cannot crucially depend on $\pi(\bullet|h_0)$
being non-deterministic. This is counter-intuitive because we could imagine saying:
``In response to such-and-such history, it would be optimal for our agent
to assume the environment is playing Paper-Rock-Scissors and therefore
assign uniform probabilities of $1/3$ to each of Paper, Rock, and Scissors;
certainly, it would be sub-optimal for our agent to instead assign probability $1$ to
any of Paper, Rock, or Scissors.'' Corollary \ref{nondeterminismcorollary} shows
that no such statement can be true.

The ``pointwise'' nature of Corollary \ref{nondeterminismcorollary}
is essential: by applying the corollary repeatedly, one can remove non-determinism
from $\pi$ at any finite number of points without ever decreasing $\pi$'s intelligence,
but one cannot conclude from this that non-determinism can be removed at infinitely
many points without decreasing $\pi$'s intelligence. The following example makes this
clear.

\begin{example}
    Fix distinct actions $a_0,a_1\in\mathcal A$.
    Let $\mu$ be an environment such that in every agent-environment interaction:
    \begin{enumerate}
        \item If the agent always takes action $a_0$, then the agent gets total reward $-1$.
        \item If the agent initially takes action $a_0$ exactly $k$ times and
            then takes a different action, then the agent either
            gets total reward $1$ (with probability $1-2^{-k}$)
            or total reward $0$ (with probability $2^{-k}$)---so in expected
            value, the agent gets total reward $1-2^{-k}$.
    \end{enumerate}
    Let $\Upsilon$ be a weighted performance averager in which $\mu$ has weight $2$,
    $\overline\mu$ has weight $1$, and such that for every
    environment $\nu\not\in\{\mu,\overline{\mu}\}$, $\nu$ and $\overline{\nu}$ have
    equal weight.
    Choose these weights so that $V^\pi_\mu$ is absolutely convergent for every $\pi$.
    By Corollary 6 of \cite{alexander2021reward}, for any Janus agent $\pi$ and
    any environment $\nu$, $V^\pi_{\overline\nu}=-V^\pi_{\nu}$, thus for any Janus agent $\pi$,
    in the sum $\Upsilon(\pi)$, for any environment $\nu\not\in\{\mu,\overline{\mu}\}$,
    the contributions from $\nu$ and $\overline\nu$ are $V^\pi_{\nu}$ and $-V^\pi_{\nu}$,
    respectively, and so cancel each other
    (terms of the infinite series can be rearranged due to absolute convergence).
    Thus for any Janus agent $\pi$,
    $\Upsilon(\pi)$ consists solely of the contributions from $\mu$ and $\overline\mu$,
    i.e.,
    \[
        \Upsilon(\pi)=2V^\pi_\mu+1V^\pi_{\overline\mu}
        =2V^\pi_\mu-1V^\pi_{\mu}=V^\pi_\mu.
    \]
    For every $k\in\mathbb N$, let $\pi_k$ be the agent which ignores the environment,
    blindly taking action $a_0$ exactly $k$ times, and then forever thereafter,
    randomly taking action $a_0$ with probability $1/2$ or action $a_1$ with probability
    $1/2$.
    For every $k$,
    \begin{align*}
        V^{\pi_k}_\mu
            &= (1-2^{-(k+0)})\cdot \mbox{$\frac12$}
                +(1-2^{-(k+1)})\cdot\mbox{$\frac14$} + \cdots
                    &\mbox{(Basic probability)}\\
            &= 1-2^{-k},
                    &\mbox{(Geometric series)}
    \end{align*}
    so $\Upsilon(\pi_k)=1-2^{-k}$ (as $\pi_k$ is a Janus agent).
    Thus:
    \begin{enumerate}
        \item For each $k$, transforming $\pi_k$ into $\pi_{k+1}$
            (by changing finitely many $50\%$-probability-$a_0$
            actions into $100\%$-probability-$a_0$ actions)
            \emph{increases} intelligence.
        \item However, if we start this process with $\pi_0$ and
            repeat it to infinity, even though each individual step
            \emph{increases} intelligence, we finally end up with
            the completely deterministic
            Janus agent $\pi_{\infty}$ who always takes action $a_0$ and who is
            \emph{less} intelligent than all $\pi_k$'s:
            $\Upsilon(\pi_{\infty})=V^{\pi_\infty}_\mu=-1$.
    \end{enumerate}
\end{example}

\bibliographystyle{alpha}
\bibliography{main}

\end{document}