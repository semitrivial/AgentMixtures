\documentclass[runningheads]{llncs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathrsfs}
% \smartqed  % flush right qed marks, e.g. at end of proof
% \usepackage{graphicx}

\newcommand{\myclaim}[1]{\textbf{Claim #1:}}
\def\myulcorner{\mathord{\ulcorner}}
\def\myurcorner{\mathord{\urcorner}}
\def\HD{\textsc{HD}}
\def\R{\mathbb{R}}
\def\N{\mathbb{N}}
%\newtheorem{definition}[definition]{Definition}

% \pagenumbering{gobble}

\begin{document}

\title{Agent mixtures and the genericness of non-deterministic intelligence}
\titlerunning{Agent mixtures and genericness}
\author{Samuel Allen
Alexander\inst{1}\orcidID{0000-0002-7930-110X}}

\institute{The U.S.\ Securities and Exchange Commission
\email{samuelallenalexander@gmail.com}
\url{https://philpeople.org/profiles/samuel-alexander/publications}}

\maketitle

\begin{abstract}
    We introduce a weighted mixture operation on
    reinforcement learning agents. The mixture of several weighted agents is
    an agent with the
    following property: the expected total reward the mixture agent
    gets in any environment is the corresponding weighted average
    of the expected total rewards the original agents get in that
    environment. We use mixture agents to formalize and
    strengthen an informal result of Alexander and Hutter. We also use mixture
    agents to prove additional results, including a surprising result,
    which we call the genericness of non-deterministic intelligence. Loosely:
    any particular non-deterministic action an agent takes
    can have its probabilities modified without making the agent less
    intelligent.
\end{abstract}

\section{Introduction}

In reinforcement learning (RL), an agent $\pi$ interacts with an environment $\mu$.
The agent and the environment take turns.
\begin{itemize}
\item
On $\pi$'s turn, $\pi$
outputs a probability distribution over a fixed action-set.
Based on this distribution, an action is randomly chosen
and is transmitted to $\pi$ and $\mu$.
\item
On $\mu$'s turn, $\mu$
outputs a probability distribution over a fixed percept-set,
where every percept includes an observation (thought of as
the agent's view of the world) and a numerical reward.
Based on this distribution, a percept is randomly chosen and
is transmitted to $\pi$ and $\mu$.
\end{itemize}
These turns continue forever, and the whole sequence of turns
is called an agent-environment interaction.

If $\pi$ and $\rho$ are two agents, we can informally imagine a new agent
$\sigma$ (first described in \cite{alexander2021reward})
as follows. At the beginning of every agent-environment interaction,
$\sigma$ flips a coin. If the coin lands heads, then $\sigma$ transforms into
$\pi$; otherwise, $\sigma$ transforms into $\rho$. Note that the coin is only
flipped one time, at the very start of the agent-environment interaction:
it is not repeatedly flipped every turn. Intuitively, it seems like the
expected total reward in the agent-environment interaction
when $\sigma$ interacts with $\mu$, should be the average
of the corresponding expected total rewards when $\pi$ or $\rho$ interact
with $\mu$. But this is all quite informal, as the RL
framework does not actually provide any mechanism for such an initial
coin-flip.

More generally, given agents $\vec{\pi}=(\pi^1,\ldots,\pi^n)$ and positive reals
$\vec{w}=(w_1,\ldots,w_n)$ with $w_1+\cdots+w_n=1$, we could imagine an agent $\sigma$
who, at the start of each agent-environment interaction, randomly transforms
into $\pi^1$ (with probability $w_1$) or into $\pi^2$ (with probability $w_2$),
etc. If each $\pi^i$ would get total expected reward $r_i$ from an environment
$\mu$, intuitively $\sigma$ should get total expected reward
$\vec{w}\cdot \vec{r}=w_1r_1+\cdots+w_nr_n$ from that environment.
But again, the RL
framework has no mechanism for such an agent $\sigma$. We will define
an agent $\vec{w}\cdot\vec{\pi}$, called a \emph{mixture agent},
within the constraints
of the RL framework, and prove that ``the expected total reward of the
weighted mixture is the weighted average of the expected total rewards''.
We will then use this mixture operation to prove a number of interesting results.

\section{Preliminaries}

In defining agent and environment below, we attempt to follow
Legg and Hutter \cite{legg2007universal} as closely as possible,
except that we permit environments to output rewards from $\mathbb Q \cap [-1,1]$
rather than just $\mathbb Q\cap [0,1]$ (and, accordingly, we modify which well-behaved
environments to restrict our attention to).

Throughout the paper, we implicitly
fix a finite set $\mathcal A$ of \emph{actions},
a finite set $\mathcal O$ of \emph{observations},
and a finite set $\mathcal R\subseteq \mathbb Q\cap [-1,1]$ of \emph{rewards}
(so each reward is a rational number between $-1$ and $1$ inclusive),
with $|\mathcal A|>0$,
$|\mathcal O|>0$, $|\mathcal R|>0$.
We assume that $\mathcal R$ has the following property:
whenever $\mathcal R$ contains any reward $r$, then $\mathcal R$
also contains $-r$.
We assume $\mathcal A$, $\mathcal O$, and $\mathcal R$ are mutually disjoint
(i.e., no reward is an action, no reward is an observation, and no action is an
observation).
By $\langle\rangle$ we mean the empty sequence.
By $\mathcal P$ we mean $\mathcal O\times\mathcal R$ (the set of all observation-reward
pairs); elements of $\mathcal P$ are called \emph{percepts}.

\begin{definition}
\label{omnibusdefn}
    (Agents, environments, etc.)
    \begin{enumerate}
        \item
        By $(\mathcal P\mathcal A)^*$ we mean the set of
        all finite sequences starting with a percept, ending with an action,
        and following the pattern ``percept, action, ...''.
        We include $\langle\rangle$ in this set.
        \item
        By $(\mathcal P\mathcal A)^* \mathcal P$
        we mean the set of all sequences of the form $s\frown p$ where
        $s\in (\mathcal P\mathcal A)^*$, $p\in\mathcal P$
        ($\frown$ denotes concatenation).
        \item
        By an \emph{agent}, we mean a function $\pi$
        with domain $(\mathcal P\mathcal A)^* \mathcal P$,
        which assigns to every sequence
        $s\in (\mathcal P\mathcal A)^* \mathcal P$ a
        $\mathbb Q$-valued probability measure,
        written $\pi(\bullet|s)$, on $\mathcal A$.
        For every such $s$ and every $a\in\mathcal A$,
        we write $\pi(a|s)$ for $(\pi(\bullet|s))(a)$.
        Intuitively, $\pi(a|s)$ is the probability that agent $\pi$
        will take action $a$ in response to history $s$.
        \item
        By an \emph{environment}, we mean a function $\mu$
        with domain $(\mathcal P\mathcal A)^*$,
        which assigns to every
        $s\in (\mathcal P\mathcal A)^*$
        a $\mathbb Q$-valued probability measure,
        written $\mu(\bullet|s)$,
        on $\mathcal P$.
        For every such $s$ and every $(o,r)\in\mathcal P$,
        we write $\mu(o,r|s)$ for $(\mu(\bullet|s))(o,r)$.
        If $p=(o,r)$ then we may also write $\mu(p|s)$ for
        $(\mu(\bullet|s))(o,r)$.
        Intuitively, $\mu(o,r|s)$ is the probability that environment
        $\mu$ will issue percept $(o,r)$ (observation $o$ and reward $r$)
        to the agent in response to history $s$.
        \item
        If $\pi$ is an agent, $\mu$ is an environment, and $n\in\mathbb N$,
        we write $V^\pi_{\mu,n}$ for the expected value of the sum of
        the rewards which would occur in the sequence
        $(p_0,a_0,\ldots,p_n,a_n)$ randomly generated as follows:
        \begin{enumerate}
            \item $p_0\in \mathcal P$ is chosen randomly based
            on the probability measure $\mu(\bullet|\langle\rangle)$.
            \item $a_0\in\mathcal A$ is chosen randomly based on the probability
            measure $\pi(\bullet|p_0)$.
            \item
            For each $i>0$,
            $p_i\in\mathcal P$ is chosen randomly based on
            the probability measure
            $\mu(\bullet|p_0,a_0,\ldots,p_{i-1},a_{i-1})$.
            \item
            For each $i>0$,
            $a_i\in\mathcal A$ is chosen randomly based on the probability measure
            $\pi(\bullet|p_0,a_0,\ldots,p_{i-1},a_{i-1},p_i)$.
        \end{enumerate}
        \item
        If $\pi$ is an agent and $\mu$ is an environment,
        let $V^\pi_\mu=\lim_{n\to\infty}V^{\pi}_{\mu,n}$.
        Intuitively, $V^\pi_\mu$ is the expected total reward which $\pi$ would extract
        from $\mu$.
    \end{enumerate}
\end{definition}

Note that it is possible for $V^\pi_\mu$ to be undefined.
For example, if $\mu$ is an environment which always issues
reward $(-1)^n$ in response to the agent's $n$th action,
then $V^\pi_\mu$ is undefined for every agent $\pi$.
This would not be the case if rewards were required to be $\geq 0$,
so this is one way in which allowing
punishments complicates the resulting theory.

\begin{definition}
    An environment $\mu$ is \emph{well-behaved} if $\mu$ is computable and the following
    condition holds: for every agent $\pi$, $V^\pi_\mu$ exists and
    $-1\leq V^\pi_\mu\leq 1$. Let $W$ be the set of all well-behaved environments.
\end{definition}

\begin{definition}
\label{performanceaveragerdefn}
    By a \emph{weighted performance averager}, we mean a function
    $\Upsilon$ assigning real numbers to agents, such that there
    are weights $\{w_\mu\}_{\mu\in W}$ such that for every agent
    $\pi$, $\Upsilon(\pi)=\sum_{\mu\in W}w_\mu V^\pi_\mu$
    (and such that this infinite sum is absolutely convergent
    for every agent $\pi$).
\end{definition}

The prototypical weighted performance averager is the Legg-Hutter intelligence
measure $\Upsilon$, where each well-behaved $\mu$ is given weight $2^{-K(\mu)}$
where $K$ denotes Kolmogorov complexity (dependent on a background universal
Turing machine) \cite{legg2007universal}.

\section{Mixture agents}

Before defining mixture agents, we need to start with some preliminary definitions
and lemmas.

\begin{definition}
    By $\mathcal H$ we mean
    $((\mathcal P\mathcal A)^*)\cup((\mathcal P\mathcal A)^*\mathcal P)$.
    We refer to elements of $\mathcal H$ as \emph{histories} (a history
    may terminate with either a percept or an action).
\end{definition}

\begin{definition}
\label{pullbackdef}
    Let $\pi$ be an agent. Let $\mu$ be an environment. Let $s\in\mathcal H$.
    \begin{enumerate}
        \item
        The \emph{probability of $s$ according to $\pi$}, written
        $\pi_*(s)$, is defined by induction as follows.
        \begin{itemize}
            \item
            If $s=\langle\rangle$ then $\pi_*(s)=1$.
            \item
            If $s=t\frown p$ for some $p\in\mathcal P$ then $\pi_*(s)=\pi_*(t)$.
            \item
            If $s=t\frown a$ for some $a\in\mathcal A$ then $\pi_*(s)=\pi_*(t)\pi(a|t)$.
        \end{itemize}
        Informally, $\pi_*(s)$ is the conditional probability that when $\pi$
        and $\mu$ interact, the interaction begins with $s$, assuming
        that $\mu$ produces the percepts in $s$.
        \item
        The \emph{probability of $s$ according to $\mu$}, written
        $\mu_*(s)$, is defined by induction as follows.
        \begin{itemize}
            \item
            If $s=\langle\rangle$ then $\mu_*(s)=1$.
            \item
            If $s=t\frown p$ for some $p\in\mathcal P$ then $\mu_*(s)=\mu_*(t)\mu(p|t)$.
            \item
            If $s=t\frown a$ for some $a\in\mathcal A$ then $\mu_*(s)=\mu_*(t)$.
        \end{itemize}
        Informally, $\mu_*(s)$ is the conditional probability that when $\pi$
        and $\mu$ interact, the interaction begins with $s$, assuming
        that $\pi$ produces the actions in $s$.
        \item
        The \emph{probability of $s$ according to $\pi$ and $\mu$},
        written $(\pi,\mu)_*(s)$, is defined by induction as follows.
        \begin{itemize}
            \item
            If $s=\langle\rangle$ then $(\pi,\mu)_*(s)=1$.
            \item
            If $s=t\frown p$ for some $p\in\mathcal P$ then
            $(\pi,\mu)_*(s)=(\pi,\mu)_*(t)\mu(p|t)$.
            \item
            If $s=t\frown a$ for some $a\in\mathcal A$ then
            $(\pi,\mu)_*(s)=(\pi,\mu)_*(t)\pi(a|t)$.
        \end{itemize}
        Informally, $(\pi,\mu)_*(s)$ is the probability that when $\pi$ and $\mu$
        interact, the interaction begins with $s$.
    \end{enumerate}
\end{definition}

\begin{lemma}
\label{factorizationlemma}
    For all $s$, $\pi$, $\mu$ as in Definition \ref{pullbackdef},
    \[
        (\pi,\mu)_*(s) = \pi_*(s)\mu_*(s).
    \]
\end{lemma}

\begin{proof}
    By induction.
\end{proof}

\begin{lemma}
\label{basicprobabilitylemma}
    $V^\pi_{\mu,n}=\sum_{s\in S_n}(\pi,\mu)_*(s)r(s),$
    where $S_n$ is the set of all $s\in\mathcal H$
    of length $2(n+1)$ (i.e., all $s\in\mathcal H$ of the form
    $(p_0,a_0,\ldots,p_n,a_n)$ as in Part 5 of Definition \ref{omnibusdefn}),
    and $r(s)$ is the total reward in $s$.
\end{lemma}

\begin{proof}
    Basic probability theory.
\end{proof}

\begin{definition}
    If $\vec\pi=(\pi^1,\ldots,\pi^n)$ are agents, $\mu$ is an environment,
    $s\in\mathcal H$, $m\in\mathbb N$, and $\Upsilon$ is a weighted
    performance averager, then we define:
    \begin{itemize}
        \item $\vec\pi_*(s)=(\pi^1_*(s),\ldots,\pi^n_*(s))$.
        \item $(\vec\pi,\mu)_*(s)=((\pi^1,\mu)_*(s),\ldots,(\pi^n,\mu)_*(s))$.
        \item $V^{\vec\pi}_{\mu,m}=(V^{\pi^1}_{\mu,m},\ldots,V^{\pi^n}_{\mu,m})$.
        \item If each $V^{\pi^i}_\mu$ converges, we say $V^{\vec\pi}_\mu$
            converges, and define $V^{\vec\pi}_\mu=(V^{\pi^1}_\mu,\ldots,V^{\pi^n}_\mu)$.
        \item $\Upsilon(\vec\pi)=(\Upsilon(\pi^1),\ldots,\Upsilon(\pi^n))$.
    \end{itemize}
    Recall that if $\vec u=(u_1,\ldots,u_n)$ and
    $\vec v=(v_1,\ldots,v_n)$ are any two equal-length
    vectors of real numbers, then their \emph{inner product}
    (or \emph{dot product}) is defined to be
    $\vec u\cdot \vec v=u_1v_1+\cdots+u_nv_n$.
\end{definition}

Now we are ready to define mixture agents.

\begin{definition}
\label{maindefn}
    (Mixture agents)
    Suppose $\vec\pi=(\pi^1,\ldots,\pi^n)$ are agents and $\vec w=(w_1,\ldots,w_n)$
    are positive real numbers with $w_1+\cdots+w_n=1$.
    Define the \emph{mixture agent}
    $\vec w\cdot\vec\pi$ by:
    \[
        (\vec w\cdot\vec\pi)(a|s)
        =
        \begin{cases}
            \dfrac{\vec w\cdot (\vec\pi_*(s\frown a))}{\vec w\cdot (\vec\pi_*(s))}
            &\mbox{if $\vec w\cdot (\vec\pi_*(s))\not=0$,}\\
            1/|\mathcal{A}| &\mbox{otherwise}
        \end{cases}
    \]
    (see Remark \ref{ambiguityremark} below).
\end{definition}

\begin{remark}
\label{ambiguityremark}
    Theorem \ref{maintheorem} below will allow us to unimbiguously
    rewrite Definition \ref{maindefn} as
    \[
        (\vec w\cdot\vec\pi)(a|s)
        =
        \begin{cases}
            \dfrac{\vec w\cdot \vec\pi_*(s\frown a)}{\vec w\cdot \vec\pi_*(s)}
            &\mbox{if $\vec w\cdot \vec\pi_*(s)\not=0$,}\\
            1/|\mathcal{A}| &\mbox{otherwise.}
        \end{cases}
    \]
\end{remark}

\begin{lemma}
    If $\vec\pi$ and $\vec w$ are as in Definition \ref{maindefn}
    then $\vec w\cdot\vec\pi$ really is an agent.
\end{lemma}

\begin{proof}
    We must show
    $\sum_{a\in\mathcal A}(\vec w\cdot\vec\pi)(a|s)=1$ for every
    $s\in(\mathcal P\mathcal A)^*\mathcal P$. Fix some such $s$.

    Case 1: $\vec w\cdot (\vec\pi_*(s))=0$. Then
    each $(\vec w\cdot\vec\pi)(a|s)=1/|\mathcal A|$ so the
    claim is immediate.

    Case 2: $\vec w\cdot (\vec\pi_*(s))\not=0$. Then
    \begin{align*}
        &{} \sum_{a\in\mathcal A}(\vec w\cdot\vec\pi)(a|s)\\
            &= \sum_{a\in\mathcal A}
                \frac{\vec w\cdot (\vec\pi_*(s\frown a))}{\vec w\cdot (\vec\pi_*(s))}
                &\mbox{(Definition \ref{maindefn})}\\
            &= \sum_{a\in\mathcal A}
                \frac
                {w_1\pi^1_*(s)\pi^1(a|s)+\cdots+w_n\pi^n_*(s)\pi^n(a|s)}
                {\vec w\cdot(\vec\pi_*(s))}
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= \frac{
                w_1\pi^1_*(s)\left(\mbox{$\sum_{a\in\mathcal A}\pi^1(a|s)$}\right)
                +\cdots+
                w_n\pi^n_*(s)\left(\mbox{$\sum_{a\in\mathcal A}\pi^n(a|s)$}\right)
                }
                {\vec w\cdot (\vec\pi_*(s))}
                &\mbox{(Algebra)}\\
            &= \frac
                {w_1\pi^1_*(s)\cdot 1 + \cdots + w_n\pi^n_*(s)\cdot1}
                {\vec w\cdot (\vec\pi_*(s))}
                =\frac{\vec w\cdot (\vec\pi_*(s))}{\vec w\cdot (\vec\pi_*(s))}=1.
                &\mbox{($\pi^i$ are agents)}
    \end{align*}
\end{proof}

\begin{theorem}
\label{maintheorem}
    (Commutativity of $\vec w$)
    Let $\vec\pi=(\pi^1,\ldots,\pi^n)$ be agents.
    Let $\vec w=(w_1,\ldots,w_n)$ be positive reals with
    $w_1+\cdots+w_n=1$. Let $\mu$ be any environment.
    Then:
    \begin{enumerate}
        \item
        For any $s\in\mathcal H$,
        $(\vec w\cdot\vec\pi)_*(s)=\vec w\cdot(\vec\pi_*(s))$.
        \item
        For any $s\in\mathcal H$,
        $(\vec w\cdot \vec\pi,\mu)_*(s)=\vec w\cdot(\vec\pi,\mu)_*(s)$.
        \item
        For any $m\in\mathbb N$,
        $V^{\vec w\cdot \vec\pi}_{\mu,m}=\vec w\cdot V^{\vec\pi}_{\mu,m}$.
        \item
        (``The expected reward of a weighted mixture is the weighted
        average of the expected rewards'')
        If $V^{\vec\pi}_\mu$ converges, then $V^{\vec w\cdot\vec\pi}_\mu$
        converges and $V^{\vec w\cdot\vec\pi}_\mu=\vec w\cdot V^{\vec\pi}_\mu$.
        \item
        (``The intelligence of a weighted mixture is the weighted average
        of the intelligences'')
        For any weighted performance averager $\Upsilon$,
        $\Upsilon(\vec w\cdot\vec\pi)=\vec w\cdot\Upsilon(\vec\pi)$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    (1) By induction on $s$.

    Case 1: $s=\langle\rangle$. Then
    $(\vec w\cdot\vec\pi)_*(s)=1=w_1\cdot 1+\cdots+w_n\cdot 1
    =\vec w\cdot (\vec\pi_*(s))$.

    Case 2: $s=t\frown p$ for some percept $p$. Then
    \begin{align*}
        (\vec w\cdot \vec\pi)_*(s)
            &= (\vec w\cdot\vec\pi)_*(t)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= \vec w\cdot(\vec\pi_*(t))
                &\mbox{(Induction)}\\
            &= \vec w\cdot(\vec\pi_*(t\frown p))
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= \vec w\cdot(\vec\pi_*(s)).
    \end{align*}

    Case 3: $s=t\frown a$ for some action $a$.

    Subcase 3.1: $(\vec w\cdot \vec\pi)_*(t)=0$.
        By induction $\vec w\cdot(\vec\pi_*(t))=0$.
        Since the $w_i$ are positive, this implies
        each $\pi^i_*(t)=0$.
        Thus each
        \begin{align*}
            w_i\pi^i_*(t\frown a)
                &= w_i\pi^i_*(t)\pi^i(a|t)
                    &\mbox{(Definition \ref{pullbackdef})}\\
                &= 0w_i\pi^i(a|t)=0,
        \end{align*}
        i.e., $\vec w\cdot(\vec\pi_*(t\frown a))=0$.
        And
        \begin{align*}
            (\vec w\cdot\vec\pi)_*(t\frown a)
                &= (\vec w\cdot\vec\pi)_*(t)(\vec w\cdot\vec\pi)(a|t)
                    &\mbox{(Definition \ref{pullbackdef})}\\
                &= 0(\vec w\cdot\vec\pi)(a|t) = 0,
        \end{align*}
        so $(\vec w\cdot\vec\pi)_*(s)=\vec w\cdot(\vec \pi_*(s))=0$.

    Subcase 3.2: $\vec w\cdot (\vec\pi_*(t))\not=0$. Then
    \begin{align*}
        (\vec w\cdot\vec\pi)_*(s)
            &= (\vec w\cdot\vec\pi)_*(t)(\vec w\cdot\vec\pi)(a|t)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= (\vec w\cdot\vec\pi)_*(t)
                \frac
                {\vec w\cdot(\vec\pi_*(t\frown a))}
                {\vec w\cdot(\vec\pi_*(t))}
                &\mbox{(Definition \ref{maindefn})}\\
            &= \vec w\cdot(\vec\pi_*(t))
                \frac
                {\vec w\cdot(\vec\pi_*(t\frown a))}
                {\vec w\cdot(\vec\pi_*(t))}
                &\mbox{(Induction)}\\
            &= \vec w\cdot(\vec\pi_*(t\frown a))
                &\mbox{(Basic Algebra)}\\
            &= \vec w\cdot(\vec\pi_*(s)).
    \end{align*}
    as desired.

    (2) Follows from (1) and Lemma \ref{factorizationlemma}.

    (3) Follows from (2) and Lemma \ref{basicprobabilitylemma}.

    (4) Immediate from (3).

    (5) Immediate from (4).
\end{proof}

By part 1 of Theorem \ref{maintheorem},
we may unambiguously write $\vec w\cdot \vec\pi_*(s)$
for either $(\vec w\cdot\vec\pi)_*(s)$ or $\vec w\cdot(\vec\pi_*(s))$.
We will freely do so for the remainder of the paper.

\section{Duality and Janus agents:
Strengthening and formalizing a result of Alexander and Hutter}

\begin{definition}
    (Duality) Define $\overline s$, $\overline \pi$, $\overline \mu$
    as in Reward-Punishment Symmetric Universal Intelligence.
\end{definition}

The following class of agents are named after Janus, the Roman god of duality,
who features two faces, one facing forward, one facing backward.

\begin{definition}
    (Janus agents)
    By a \emph{Janus agent}, we mean an agent $\pi$ such that
    $\overline{\pi}=\pi$.
\end{definition}

\begin{definition}
\label{equivdefn}
    If $\pi$ and $\rho$ are agents, we say $\pi\equiv\rho$ if the
    following conditions hold:
    \begin{enumerate}
        \item For every history $h$, $\pi_*(h)=0$ iff $\rho_*(h)=0$.
        \item For every history $h$ ending with a percept,
            for every action $a$, if $\pi_*(h)\not=0$,
            then $\rho(a|h)=\pi(a|h)$.
    \end{enumerate}
\end{definition}

\begin{lemma}
\label{equivrelationlemma}
    The relation $\equiv$ of Definition \ref{equivdefn} is an equivalence
    relation.
\end{lemma}

\begin{proof}
    Straightforward.
\end{proof}

\begin{lemma}
\label{piopluspilemma}
    For any agent $\pi$, $\pi\equiv\pi\oplus\pi$.
\end{lemma}

\begin{proof}
    We prove the two conditions of Definition \ref{equivdefn} by
    induction on history $h$.
 
    Case 1: $h$ contains no actions. Then $\pi_*(h)=(\pi\oplus\pi)_*(h)=1$, so
    certainly $\pi_*(h)=0$ iff $\rho_*(h)=0$ (proving condition 1 of
    Definition \ref{equivdefn}).
    For condition 2, we check:
    \begin{align*}
        (\pi\oplus\pi)(a|h)
            &= \frac{\pi_*(h\frown a) + \pi_*(h\frown a)}{\pi_*(h)+\pi_*(h)}\\
            &= \frac{\pi_*(h)+\pi_*(h)}{\pi_*(h)+\pi_*(h)}\pi(a|h)\\
            &= \pi(a|h),
    \end{align*}
    as desired.

    Case 2: $h$ ends with an action, say $h=h_0\frown a_0$ where $h_0$ ends with
        a percept.
        By induction, we may assume (condition 1)
        $\pi_*(h_0)=0$ iff $(\pi\oplus\pi)_*(h_0)=0$,
        and (condition 2) if $\pi_*(h_0)\not=0$ then
        $(\pi\oplus\pi)(a_0|h_0)=\pi(a_0|h_0)$.
        If $\pi_*(h_0)=0$ (and hence $(\pi\oplus\pi)_*(h_0)=0$) then clearly
        $\pi_*(h)=0$ and $(\pi\oplus\pi)_*(h)=0$. But assume $\pi_*(h_0)\not=0$.
        Then by a similar calculation as in Case 1,
        $(\pi\oplus\pi)(a_0|h_0)=\pi(a_0|h_0)$.
        This implies $\pi_*(h)=0$ iff $(\pi\oplus\pi)_*(h)=0$, proving condition 1.
        For condition 2 there is nothing to prove since $h$ does not end with a percept.

    Case 3: $h$ ends with a percept, say $h=h_0\frown p$, and $h$ contains at least one action.
        By induction, conditions 1 and 2 hold for $h_0$.
        Since $\pi_*(h_0\frown p)=\pi_*(h_0)$ and
        $(\pi\oplus\pi)_*(h_0\frown p)=(\pi\oplus\pi)_*(h_0)$,
        condition 1 for $h$ follows.
        For condition 2, let $a$ be any action and assume
        $\pi_*(h)\not=0$. Then $(\pi\oplus\pi)(a|h)=\pi(a|h)$ by the
        same calculation as in Case 1.
\end{proof}

\begin{proposition}
    (Characterization of Janus agents)
    For any agent $\pi$, the following are equivalent:
    \begin{enumerate}
        \item $\pi\equiv\rho$ for some Janus agent $\rho$.
        \item $\pi\equiv\rho\oplus\overline{\rho}$ for some agent $\rho$.
    \end{enumerate}
\end{proposition}

\begin{proof}
    ($\Rightarrow$)
    Assume $\pi\equiv\rho$ for some Janus agent $\rho$.
    Then $\rho=\overline{\rho}$, so $\rho\equiv \rho\oplus\overline{\rho}$ by
    Lemma \ref{piopluspilemma}, thus, by Lemma \ref{equivrelationlemma},
    $\pi\equiv\rho\oplus\overline{\rho}$.

    ($\Leftarrow$)
    Assume $\pi\equiv\rho\oplus\overline{\rho}$ for some agent $\rho$.
    Clearly
    \[
        \overline{\rho\oplus\overline{\rho}}
        =
        \overline{\rho}\oplus\overline{\overline{\rho}}
        =
        \overline{\rho}\oplus\rho
        =
        \rho\oplus\overline{\rho},
    \]
    so $\rho\oplus\overline{\rho}$ is a Janus agent, proving that
    $\pi$ is $\equiv$ some Janus agent.
\end{proof}

\begin{theorem}
    Suppose $\Upsilon$ is a weighted performance averager.
    If $\Upsilon(\pi)=0$ for every Janus agent $\pi$,
    then $\Upsilon(\overline{\pi})=-\Upsilon(\pi)$
    for every agent $\pi$.
\end{theorem}

\begin{proof}
    Let $\pi$ be any agent.
    By assumption, $\Upsilon(\pi\oplus\overline{\pi})=0$.
    Thus by Theorem \ref{maintheorem},
    $0=\frac12(\Upsilon(\pi)+\Upsilon(\overline{\pi})$,
    so $\Upsilon(\overline{\pi})=-\Upsilon(\pi)$.
\end{proof}


\section{Detectability of sets of agents}

\begin{definition}
\label{incentivizabilitydefn}
    A set $\Pi$ of agents is \emph{detectable} if there exists
    an environment $\mu$ such that for every agent $\pi$:
    \begin{enumerate}
        \item
        $V^\pi_\mu$ exists.
        \item
        $V^\pi_\mu>0$ if $\pi\in\Pi$.
        \item
        $V^\pi_\mu\leq 0$ if $\pi\not\in\Pi$.
    \end{enumerate}
\end{definition}

\begin{definition}
    A set $\Pi$ is \emph{closed under $\oplus$} if the following
    condition holds: whenever $\pi\in\Pi$ and $\rho\in\Pi$,
    then $\pi\oplus\rho\in\Pi$.
\end{definition}

\begin{theorem}
\label{closuretheorem}
    Let $\Pi$ be any set of agents.
    If $\Pi$ is detectable, then $\Pi$ is closed under $\oplus$, and so
    is its complement $\Pi^c$.
\end{theorem}

\begin{proof}
    Assume $\Pi$ is detectable.
    Let $\mu$ be as in
    Definition \ref{incentivizabilitydefn}.
    To see $\Pi$ is closed under $\oplus$, let $\pi,\rho\in\Pi$.
    Then $V^\pi_\mu>0$ and $V^\rho_\mu>0$, thus
    $V^{\pi\oplus\rho}_\mu>0$ since
    $V^{\pi\oplus\rho}_\mu=\frac12(V^\pi_\mu+V^\rho_\mu)$ by Theorem
    \ref{maintheorem}.
    So by choice of $\mu$, $\pi\oplus\rho\in \Pi$.
    A similar argument shows that $\Pi^c$ is closed under $\oplus$.
\end{proof}


\section{Genericness of non-deterministic agents}

\begin{definition}
\label{modifyagentatoneplace}
    If $\pi$ is an agent, $h_0$ is a history ending in a percept,
    and $m$ is a probability distribution on $\mathcal A$,
    we write $\pi^{h_0\mapsto m}$ for the agent which is identical to $\pi$
    except that it maps $h_0$ to $m$, in other words:
    \[
        \pi^{h\mapsto m}(a|h)
        =
        \begin{cases}
            \pi(a|h) &\mbox{if $h\not=h_0$,}\\
            m(a) &\mbox{if $h=h_0$.}
        \end{cases}
    \]
\end{definition}

\begin{lemma}
\label{firsttechlemmaforgenericity}
    Suppose $\pi$, $h_0$, $m$ are as in Definition \ref{modifyagentatoneplace}.
    Suppose $h$ is a history such that
    for every $a\in\mathcal A$,
    $h_0\frown a$ is not an initial segment of $h$.
    Then $\pi^{h_0\mapsto m}_*(h)=\pi_*(h)$.
\end{lemma}

\begin{proof}
    By induction on $h$.
\end{proof}

\begin{lemma}
\label{thirdtechlemmaforgenericity}
    Suppose $\pi$, $h_0$, $m$ are as in Definition \ref{modifyagentatoneplace}.
    For any action $a\in\mathcal A$,
    $\pi^{h_0\mapsto m}_*(h_0\frown a)=\pi_*(h_0)m(a)$.
\end{lemma}

\begin{proof}
    Immediate by Definition \ref{pullbackdef} and Lemma \ref{firsttechlemmaforgenericity}.
\end{proof}

\begin{lemma}
\label{secondtechlemmaforgenericity}
    Suppose $\pi$, $h_0$, $m$ are as in Definition \ref{modifyagentatoneplace}.
    Suppose $h$ is a history and $a_0$ is an action and that $h_0\frown a_0$ is
    an initial segment of $h$. Assume $\pi(a_0|h_0)\not=0$. Then
    $\pi^{h_0\mapsto m}_*(h) = \frac{\pi_*(h)m(a_0)}{\pi(a_0|h_0)}$.
\end{lemma}

\begin{proof}
    By induction on $h$.

    Case 1: $h=h_0\frown a$. Then
    \begin{align*}
        \pi^{h_0\mapsto m}_*(h)
        &= \pi^{h_0\mapsto m}_*(h_0)\pi^{h_0\mapsto m}(a_0|h_0)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi_*(h_0)\pi^{h_0\mapsto m}(a_0|h_0)
            &\mbox{(Lemma \ref{firsttechlemmaforgenericity})}\\
        &= \pi_*(h_0)m(a_0)
            &\mbox{(Definition \ref{modifyagentatoneplace})}\\
        &= \pi_*(h_0)\pi(a_0|h_0)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Basic Algebra)}\\
        &= \pi_*(h)m(a_0)/\pi(a_0|h_0).
            &\mbox{(Definition \ref{pullbackdef})}
    \end{align*}

    Case 2: $h=h_0\frown a_0\frown h_1\frown p$ for some history $h_1$
        and some percept $p$. Then
    \begin{align*}
        \pi^{h_0\mapsto m}_*(h)
        &= \pi^{h_0\mapsto m}_*(h_0\frown a_0\frown h_1)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi_*(h_0\frown a_0\frown h_1)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Induction)}\\
        &= \pi_*(h_0\frown a_0\frown h_1\frown p)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi_*(h)m(a_0)/\pi(a_0|h_0).
    \end{align*}

    Case 3: $h=h_0\frown a_0\frown h_1\frown a$ for some history $h_1$
        and some action $a$. Then
    \begin{align*}
        \pi^{h_0\mapsto m}_*(h)
        &= \pi^{h_0\mapsto m}_*(h_0\frown a_0\frown h_1)
            \pi^{h_0\mapsto m}(a|h_0\frown a\frown h_1)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi^{h_0\mapsto m}_*(h_0\frown a_0\frown h_1)
            \pi(a|h_0\frown a\frown h_1)
            &\mbox{(Definition \ref{modifyagentatoneplace})}\\
        &= \pi_*(h_0\frown a_0\frown h_1)\pi(a|h_0\frown a\frown h_1)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Induction)}\\
        &= \pi_*(h_0\frown a_0\frown h_1\frown a)m(a_0)/\pi(a_0|h_0)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= \pi_*(h)m(a_0)/\pi(a_0|h_0).
    \end{align*}
\end{proof}

\begin{definition}
\label{sumofdistros}
    Suppose $m$ and $m_1,\ldots,m_n$ are probability distributions on $\mathcal A$
    and $r_1,\ldots,r_n$ are nonnegative real numbers with
    $r_1+\cdots+r_n=1$. By $r_1m_1+\cdots+r_nm_n$ we mean the probability distribution
    on $\mathcal A$ defined by
    \[
        (r_1m_1+\cdots+r_nm_n)(a) = r_1m_1(a) + \cdots + r_nm_n(a).
    \]
\end{definition}

\begin{lemma}
    If $m$, $m_1,\ldots,m_n$, $r_1,\ldots,r_n$ are as in Definition \ref{sumofdistros}
    then $r_1m_1+\cdots+r_nm_n$ really is a probability distribution on $\mathcal A$.
\end{lemma}

\begin{proof}
    Clearly for every $a\in\mathcal A$,
    $(r_1m_1+\cdots+r_nm_n)(a) = r_1m_1(a) + \cdots + r_nm_n(a)$ is a nonnegative
    real number. It remains to show $\sum_{a\in\mathcal A}(r_1m_1+\cdots+r_nm_n)(a)=1$.
    We compute:
    \begin{align*}
        &{} \sum_{a\in\mathcal A}(r_1m_1+\cdots+r_nm_n)(a)\\
        &=
        \sum_{a\in\mathcal A} r_1m_1(a) + \cdots + r_nm_n(a)
            &\mbox{(Definition \ref{sumofdistros})}\\
        &=
        r_1\sum_{a\in\mathcal A} m_1(a) + \cdots + r_n\sum_{a\in\mathcal A}m_n(a)
            &\mbox{(Basic Algebra)}\\
        &= r_1 + \cdots + r_n
            &\mbox{($m_1,\ldots,m_n$ are probability distr's)}\\
        &= 1.
            &\mbox{(By assumption)}
    \end{align*}
\end{proof}

\begin{proposition}
\label{longproposition}
    Let $\Upsilon$ be any weighted performance averager, let $\pi$ be any agent,
    let $h$ be any history ending in a percept.
    Suppose $m_1,\ldots,m_n$ are probability distributions on $\mathcal A$
    and $r_1,\ldots,r_n$ are nonnegative reals such that $r_1+\cdots+r_n=1$
    and $r_1m_1+\cdots+r_nm_n=\pi(\bullet|h)$.
    Then
    \[
        \Upsilon(\pi)
        =
        \Upsilon(r_1\pi^{h\mapsto m_1} \oplus \cdots \oplus r_n\pi^{h\mapsto m_n}).
    \]
\end{proposition}

\begin{proof}
    For brevity, write $\pi^i$ for $\pi^{h\mapsto m_i}$.

    It suffices to show that for every well-behaved $\mu$ and every $n\in\mathbb N$,
    \[
        V^{\pi}_{\mu,n}
        =
        V^{r_1\pi^1 \oplus \cdots \oplus r_n\pi^n}_{\mu,n}.
    \]
    By Lemma \ref{basicprobabilitylemma}, it suffices to show that for every
    well-behaved $\mu$ and every finite percept-action sequence $s$,
    $
    (\pi,\mu)_*(s)
    =
    (r_1\pi^1 \oplus \cdots \oplus r_n\pi^n,\mu)_*(s)
    $.
    We will show even more. We will show that for every percept-action sequence
    $s$, $\pi_*(s)=(r_1\pi^1 \oplus \cdots \oplus r_n\pi^n)_*(s)$.
    We prove this by induction on $s$.

    Case 1: $s=\langle\rangle$.
    Then $\pi_*(s)=1
    =(r_1\pi^1 \oplus \cdots \oplus r_n\pi^n)_*(s)$.

    Case 2: $s=t\frown p$ for some percept $p$.
    Then
    \begin{align*}
        \pi_*(s)
            &= \pi_*(t\frown p)\\
            &= \pi_*(t)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= (r_1\pi^1 \oplus \cdots \oplus r_n\pi^n)_*(t)\mu(p|t)
                &\mbox{(Induction)}\\
            &= (r_1\pi^1 \oplus \cdots \oplus r_n\pi^n)_*(t\frown p)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= (r_1\pi^1 \oplus \cdots \oplus r_n\pi^n)_*(s).
    \end{align*}

    Case 3: $s=t\frown a$ for some action $a$.

    Subcase 3.1: $\pi_*(t)=0$.
    Then
    \begin{align*}
        &{} (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(s)\\
            &= (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(t)
            (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|t)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= \pi_*(t)(r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|t)
                &\mbox{(Induction)}\\
            &= 0,
    \end{align*}
    and similarly $\pi_*(s)=0$.

    Subcase 3.2: $\pi_*(t)\not=0$ and $t=h$.

    We would like to use Definition \ref{maindefn} to rewrite
    $(r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|h)$ but first we must check
    that the resulting denominator does not vanish.
    Compute ($*$):
    \begin{align*}
        &{} r_1\pi^1_*(h)+\cdots+r_n\pi^n_*(h)\\
            &= r_1\pi_*(h)+\cdots+r_n\pi_*(h)
                &\mbox{(Lemma \ref{firsttechlemmaforgenericity})}\\
            &= \pi_*(h)\not=0.
                &\mbox{($r_1+\cdots+r_n=1$)}
    \end{align*}
    Thus:
    \begin{align*}
        &{} (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(h\frown a)\\
            &= (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(h)
                (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|h)
                    &\mbox{(Definition \ref{pullbackdef})}\\
            &= \pi_*(h)(r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|h)
                    &\mbox{(Induction)}\\
            &= \pi_*(h)
                \frac
                {r_1\pi^1_*(h\frown a)+\cdots+r_n\pi^n_*(h\frown a)}
                {r_1\pi^1_*(h)+\cdots+r_n\pi^n_*(h)}
                    &\mbox{(Definition \ref{maindefn})}\\
            &= r_1\pi^1_*(h\frown a)+\cdots+r_n\pi^n_*(h\frown a)
                    &\mbox{(By $*$)}\\
            &= r_1\pi_*(h)m_1(a)+\cdots+r_n\pi_*(h)m_n(a)
                    &\mbox{(Lemma \ref{thirdtechlemmaforgenericity})}\\
            &= \pi_*(h)\pi(a|h)
                    &\mbox{($r_1m_1+\cdots+r_nm_n=\pi(\bullet|h)$)}\\
            &= \pi_*(h\frown a),
                    &\mbox{(Definition \ref{pullbackdef})}
    \end{align*}
    as desired.

    Subcase 3.3: $\pi_*(t)\not=0$, $t\not=h$, and
    $t$ has initial segment $h\frown a_0$ for some action $a_0$.

    It follows that $\pi(a_0|h)\not=0$ because otherwise clearly
    we would have $\pi_*(t)=0$.

    We would like to use Definition \ref{maindefn} to rewrite
    $(r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|t)$ but first we must
    check that the resulting denominator does not vanish. We compute ($**$):
    \begin{align*}
        &{} r_1\pi^1_*(t) + \cdots + r_n\pi^n_*(t)\\
            &= \frac{r_1\pi_*(t)m_1(a_0)}{\pi(a_0|h)}
                + \cdots +
                \frac{r_n\pi_*(t)m_n(a_0)}{\pi(a_0|h)}
                    &\mbox{(Lemma \ref{secondtechlemmaforgenericity})}\\
            &= \frac{\pi_*(t)}{\pi(a_0|h)}
                (r_1m_1(a_0) + \cdots + r_nm_n(a_0))
                    &\mbox{(Basic Algebra)}\\
            &= \frac{\pi_*(t)}{\pi(a_0|h)}\pi(a_0|h)
                    &\mbox{($r_1m_1+\cdots+r_nm_n=\pi(\bullet|h)$)}\\
            &= \pi_*(t)\not=0.
                    &\mbox{(Basic Algebra)}
    \end{align*}
    Thus
    \begin{align*}
        &{} (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(s)\\
            &= (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(t)
                (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|t)
                    &\mbox{(Definition \ref{pullbackdef})}\\
            &= \pi_*(t)(r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|t)
                    &\mbox{(Induction)}\\
            &= \pi_*(t)\frac{r_1\pi^1_*(t\frown a)+\cdots+r_n\pi^n(t\frown a)}
                {r_1\pi^1_*(t)+\cdots+r_n\pi^n_*(t)}
                    &\mbox{(Definition \ref{maindefn})}\\
            &= r_1\pi^1_*(t\frown a)+\cdots+r_n\pi^n(t\frown a)
                    &\mbox{(By $**$)}\\
            &= r_1\frac{\pi_*(t\frown a)m_1(a_0)}{\pi(a_0|h)}
                +\cdots+r_n\frac{\pi_*(t\frown a)m_n(a_0)}{\pi(a_0|h)}
                    &\mbox{(Lemma \ref{secondtechlemmaforgenericity})}\\
            &= \frac{\pi_*(t\frown a)}{\pi(a_0|h)}(r_1m_1(a_0)+\cdots+r_nm_n(a_0))
                    &\mbox{(Basic Algebra)}\\
            &= \frac{\pi_*(t\frown a)}{\pi(a_0|h)}\pi(a_0|h)
                    &\mbox{($r_1m_1+\cdots+r_nm_n=\pi(\bullet|h)$)}\\
            &= \pi_*(t\frown a) = \pi_*(s),
    \end{align*}
    as desired.

    Subcase 3.4: $\pi_*(t)\not=0$, $t\not=h$, and $t$ has no initial segment
        of the form $h\frown a_0$.

    We would like to use Definition \ref{maindefn} to rewrite
    $(r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|h)$ but first we must
    check that the resulting denominator does not vanish:
    by the same reasoning as in Subcase 3.2, we have
    ($*$) $(r_1\pi^1_*(t)+\cdots+r_n\pi^n_*(t))=\pi_*(t)\not=0$.
    Thus
    \begin{align*}
        &{} (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(t\frown a)\\
            &= (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)_*(t)
                (r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|t)
                    &\mbox{(Definition \ref{pullbackdef})}\\
            &= \pi_*(t)(r_1\pi^1\oplus\cdots\oplus r_n\pi^n)(a|t)
                    &\mbox{(Induction)}\\
            &= \pi_*(t)\frac{r_1\pi^1_*(t\frown a)+\cdots+r_n\pi^n(t\frown a)}
                {r_1\pi^1_*(t)+\cdots+r_n\pi^n_*(t)}
                    &\mbox{(Definition \ref{maindefn})}\\
            &= r_1\pi^1_*(t\frown a)+\cdots+r_n\pi^n(t\frown a)
                    &\mbox{(By $*$)}\\
            &= r_1\pi_*(t\frown a)+\cdots+r_n\pi_*(t\frown a)
                    &\mbox{(Lemma \ref{firsttechlemmaforgenericity})}\\
            &= \pi_*(t\frown a),
                    &\mbox{($r_1+\cdots+r_n=1$)}
    \end{align*}
    as desired.
\end{proof}

\begin{theorem}
\label{pointwisegenericnessthm}
    (Pointwise Genericness of Non-Deterministic Intelligence)
    Let $\Upsilon$ be any weighted performance averager and let
    $\pi$ be an agent.
    Let $h$ be a history ending in a percept.
    For any probability distributions $m_1,\ldots,m_n$ on $\mathcal A$,
    for any nonnegative reals $r_1,\ldots,r_n$ with $r_1+\cdots+r_n=1$,
    if $r_1m_1+\cdots+r_nm_n=\pi(\bullet|h)$,
    then one of the following is true:
    \begin{enumerate}
        \item For each $i=1,\ldots,n$, $\Upsilon(\pi^{h\mapsto m_i})=\Upsilon(\pi)$.
        \item For some $i=1,\ldots,n$, $\Upsilon(\pi^{h\mapsto m_i})>\Upsilon(\pi)$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    If not, then for each $i=1,\ldots,n$, $\Upsilon(\pi^{h\mapsto m_i})\leq\Upsilon(\pi)$,
    and for some $i$, $\Upsilon(\pi^{h\mapsto m_i})<\Upsilon(\pi)$.
    This implies
    \begin{align*}
        \Upsilon(\pi)
            &= \Upsilon(r_1\pi^{h\mapsto m_1}\oplus\cdots\oplus r_n\pi^{h\mapsto m_n})
                &\mbox{(Proposition \ref{longproposition})}\\
            &= r_1\Upsilon(\pi^{h\mapsto m_1})+\cdots+r_n\Upsilon(\pi^{h\mapsto m_n})
                &\mbox{(Theorem \ref{maintheorem})}\\
            &< r_1\Upsilon(\pi)+\cdots+r_n\Upsilon(\pi)
                &\mbox{(Assumption)}\\
            &= \Upsilon(\pi),
                &\mbox{($r_1+\cdots+r_n=1$)}
    \end{align*}
    absurd.
\end{proof}

Theorem \ref{pointwisegenericnessthm} is interesting because it implies that for
any intelligent agent $\pi$, for any history $h$, if $\pi(\bullet|h)$ is not deterministic,
then the intelligence of $\pi$ does not critically depend on the specific probabilities
which $\pi(\bullet|h)$ assigns to different actions. For any decomposition
$\pi(\bullet|h)=r_1m_1+\cdots+r_nm_n$ of $\pi(\bullet|h)$ into competing probability
distributions $m_i$ (with $r_1+\cdots+r_m=1$), either $\pi^{h\mapsto m_i}$ is more intelligent
than $\pi$ for some $i$, or else every $\pi^{h\mapsto m_i}$ has the same intelligence as
$\pi$. This is counter-intuitive because one might imagine that the specific probability
distribution $\pi(\bullet|h)$ was carefully chosen and optimized to maximize the intelligence
of $\pi$.

\begin{example}
\label{genericnessexample}
    Suppose $\pi$ is an agent and $h_0$ is some percept-action history ending in a
    percept. Assume $\mathcal A=\{a_1,a_2,a_3\}$.
    Suppose $\pi(a|h_0)=\frac13$ for every $a\in\mathcal A$.
    Define probability distributions $m_1,m_2,m_3$ on $\mathcal A$ by
    \begin{align*}
        m_1(a) &=
        \begin{cases}
            \frac16 &\mbox{if $a\in\{a_1,a_2\}$}\\
            \frac23 &\mbox{if $a=a_3$}
        \end{cases}\\
        m_2(a) &=
        \begin{cases}
            \frac16 &\mbox{if $a\in\{a_1,a_3\}$}\\
            \frac23 &\mbox{if $a=a_2$}
        \end{cases}\\
        m_3(a) &=
        \begin{cases}
            \frac16 &\mbox{if $a\in\{a_2,a_3\}$}\\
            \frac23 &\mbox{if $a=a_1$}
        \end{cases}
    \end{align*}
    Let $r_1=r_2=r_3=\frac13$. Then $r_1+r_2+r_3=1$ and
    it is easy to check
    $(r_1m_1+r_2m_2+r_3m_3)(a)=\frac13$ for every $a\in\mathcal A$,
    i.e., $r_1m_1+r_2m_2+r_3m_3=\pi(\bullet|h_0)$.
    By Theorem \ref{pointwisegenericnessthm},
    for any weighted performance averager $\Upsilon$,
    one of the following statements is true:
    \begin{enumerate}
        \item
        For all $i=1,2,3$, $\pi$'s intelligence would not change if we
        changed the value of $\pi(\bullet|h_0)$ to $m_i$. Or,
        \item
        For some $i=1,2,3$, $\pi$'s intelligence would increase if
        we changed the value of $\pi(\bullet|h_0)$ to $m_i$.
    \end{enumerate}
\end{example}

In Example \ref{genericnessexample}, Theorem \ref{pointwisegenericnessthm} shows
that the intelligence of $\pi$ does not crucially depend on the specific value
of $\pi(\bullet|h_0)$. This is counter-intuitive because, a priori, it seems
plausible that the value of $\pi(\bullet|h_0)$ could have been determined
experimentally in order to maximize $\pi$'s intelligence. We could imagine ourselves
saying: ``This particular value of $\pi(\bullet|h_0)$ is critical to $\pi$'s
performance. Any other value would make $\pi$ sub-optimal.''
Theorem \ref{pointwisegenericnessthm} shows that no such statement can be true.

\begin{corollary}
\label{nondeterminismcorollary}
    (Pointwise Unnecessariness of Non-Determinism)
    Suppose $\Upsilon$ is a weighted performance averager, $\pi$ is an agent,
    and $h_0$ is a percept-action history ending in a percept.
    Let $a_1,\ldots,a_n$ enumerate $\{a\in\mathcal A\,:\,\pi(a|h_0)\not=0\}$.
    For each $i=1,\ldots,n$, let $m_i$ be the
    deterministic $\mathcal A$-probability distribution
    \[
        m_i(a) = \begin{cases}
            1 &\mbox{if $a=a_i$,}\\
            0 &\mbox{if $a\not=a_i$.}
        \end{cases}
    \]
    Then one of the following is true:
    \begin{enumerate}
        \item
        For each $i=1,\ldots,n$, $\Upsilon(\pi^{h_0\mapsto m_i})=\Upsilon(\pi)$.
        \item
        For some $i=1,\ldots,n$, $\Upsilon(\pi^{h_0\mapsto m_i})>\Upsilon(\pi)$.
    \end{enumerate}
\end{corollary}

\begin{proof}
    By Theorem \ref{pointwisegenericnessthm} with
    $r_i=\pi(a_i|h_0)$ for $i=1,\ldots,n$.
\end{proof}

Corollary \ref{nondeterminismcorollary} shows that for any individual
history $h_0$, the intelligence of $\pi$ cannot crucially depend on $\pi(\bullet|h_0)$
being non-deterministic. This is counter-intuitive because we could imagine saying:
``In response to such-and-such history, it would be optimal for our agent
to assume the environment is playing Paper-Rock-Scissors and therefore
assign uniform probabilities of $1/3$ to each of Paper, Rock, and Scissors;
certainly, it would be sub-optimal for our agent to instead assign probability $1$ to
any of Paper, Rock, or Scissors.'' Corollary \ref{nondeterminismcorollary} shows
that no such statement can be true.

The ``pointwise'' nature of Corollary \ref{nondeterminismcorollary}
is essential: by applying the corollary repeatedly, one can remove non-determinism
from $\pi$ at any finite number of points without ever decreasing $\pi$'s intelligence,
but one cannot conclude from this that non-determinism can be removed at infinitely
many points without decreasing $\pi$'s intelligence. The following example makes this
clear.

\begin{example}
    (Intelligence Discontinuity)
    Fix distinct actions $a_0,a_1\in\mathcal A$.
    Let $\mu$ be an environment such that in every agent-environment interaction:
    \begin{enumerate}
        \item If the agent always takes action $a_0$, then the agent gets total reward $-1$.
        \item If the agent initially takes action $a_0$ exactly $k$ times and
            then takes a different action, then the agent either
            gets total reward $1$ (with probability $1-2^{-k}$)
            or total reward $0$ (with probability $2^{-k}$)---so in expected
            value, the agent gets total reward $1-2^{-k}$.
    \end{enumerate}
    Let $\Upsilon$ be a weighted performance averager in which $\mu$ has weight $2$,
    $\overline\mu$ has weight $1$, and such that for every
    environment $\nu\not\in\{\mu,\overline{\mu}\}$, $\nu$ and $\overline{\nu}$ have
    equal weight.
    By Corollary 6 of \cite{alexander2021reward}, for any Janus agent $\pi$ and
    any environment $\nu$, $V^\pi_{\overline\nu}=-V^\pi_{\nu}$, thus for any Janus agent $\pi$,
    in the sum $\Upsilon(\pi)$, for any environment $\nu\not\in\{\mu,\overline{\mu}\}$,
    the contributions from $\nu$ and $\overline\nu$ are equal-weight multiples of
    $V^\pi_{\nu}$ and $-V^\pi_{\nu}$,
    respectively, and so cancel each other
    (terms of the infinite series can be regrouped due the absolute convergence
    in Definition \ref{performanceaveragerdefn}).
    Thus for any Janus agent $\pi$,
    $\Upsilon(\pi)$ consists solely of the contributions from $\mu$ and $\overline\mu$,
    i.e.,
    \[
        \Upsilon(\pi)=2V^\pi_\mu+1V^\pi_{\overline\mu}
        =2V^\pi_\mu-1V^\pi_{\mu}=V^\pi_\mu.
    \]
    For every $k\in\mathbb N$, let $\pi_k$ be the agent which ignores the environment,
    blindly taking action $a_0$ exactly $k$ times, and then forever thereafter,
    randomly taking action $a_0$ with probability $1/2$ or action $a_1$ with probability
    $1/2$.
    For every $k$,
    \begin{align*}
        V^{\pi_k}_\mu
            &= (1-2^{-(k+0)})\cdot \mbox{$\frac12$}
                +(1-2^{-(k+1)})\cdot\mbox{$\frac14$} + \cdots
                    &\mbox{(Basic probability)}\\
            &= 1-2^{-k},
                    &\mbox{(Geometric series)}
    \end{align*}
    so $\Upsilon(\pi_k)=1-2^{-k}$ (as $\pi_k$ is a Janus agent).
    Thus:
    \begin{enumerate}
        \item For each $k$, transforming $\pi_k$ into $\pi_{k+1}$
            (by changing finitely many $50\%$-probability-$a_0$
            actions into $100\%$-probability-$a_0$ actions)
            \emph{increases} intelligence.
        \item However, if we start this process with $\pi_0$ and
            repeat it to infinity, even though each individual step
            \emph{increases} intelligence, we finally end up with
            the completely deterministic
            Janus agent $\pi_{\infty}$ who always takes action $a_0$ and who is
            \emph{less} intelligent than all $\pi_k$'s:
            $\Upsilon(\pi_{\infty})=V^{\pi_\infty}_\mu=-1$.
    \end{enumerate}
\end{example}

\bibliographystyle{alpha}
\bibliography{main}

\end{document}