%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%COLT header%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\documentclass[anon,12pt]{COLT2022/colt2022} % Anonymized submission
%\usepackage{times}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%AISTATS header%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[twoside]{article}
\usepackage{AISTATS2023PaperPack/aistats2023}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% not for COLT %%%%%%%%%%%%%%%
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amsthm}
% \smartqed  % flush right qed marks, e.g. at end of proof
% \usepackage{graphicx}
%\newtheorem{definition}[definition]{Definition}
%
% \pagenumbering{gobble}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newtheorem{principle}[theorem]{Principle}
\newtheorem{question}[theorem]{Question}

\begin{document}

%\twocolumn[
%\aistatstitle{Instructions for Paper Submissions to AISTATS 2023}
%\aistatsauthor{ Author 1 \And Author 2 \And  Author 3 }
%\aistatsaddress{ Institution 1 \And  Institution 2 \And Institution 3 } ]

 \title{Agent mixtures and the genericness of non-deterministic intelligence}
% \titlerunning{Agent mixtures and genericness}
\author{Samuel Allen Alexander \& Len Du \& Marcus Hutter}

% \institute{The U.S.\ Securities and Exchange Commission
% \email{samuelallenalexander@gmail.com}
% \url{https://philpeople.org/profiles/samuel-alexander/publications}}

\maketitle

\begin{abstract}
    We introduce a weighted mixture operation on
    reinforcement learning agents. The mixture of several weighted agents is
    an agent with the
    following property: the expected total reward the mixture agent
    gets in any environment is the corresponding weighted average
    of the expected total rewards the original agents get in that
    environment. We use mixture agents to formalize and
    strengthen an informal result of Alexander and Hutter. We also use mixture
    agents to prove additional results, including a surprising result
    which we call the genericness of non-deterministic intelligence. Loosely:
    if some history causes an agent to assign nonzero probabilities to multiple
    actions, then those nonzero probabilities can be modified without making
    the agent less intelligent (in the sense of weighted average performance).
\end{abstract}

\section{Introduction}

In reinforcement learning (RL), an agent $\pi$ interacts with an environment $\mu$.
The agent and the environment take turns.
\begin{itemize}
\item
On $\pi$'s turn, $\pi$
outputs a probability distribution over a fixed action-set.
Based on this distribution, an action is sampled
and is transmitted to $\pi$ and $\mu$.
\item
On $\mu$'s turn, $\mu$
outputs a probability distribution over a fixed percept-set,
where every percept includes an observation (thought of as
the agent's view of the world) and a numerical reward.
Based on this distribution, a percept is sampled and
is transmitted to $\pi$ and $\mu$.
\end{itemize}
These turns continue forever, and the whole sequence of turns
is called an agent-environment interaction.

If $\pi_1$ and $\pi_2$ are two agents, we can informally imagine a new agent
$\sigma$ (first described in \cite{alexander2021reward})
as follows. At the beginning of every agent-environment interaction,
$\sigma$ flips a fair coin. If the coin lands heads, then $\sigma$ transforms into
$\pi_1$; otherwise, $\sigma$ transforms into $\pi_2$. Note that the coin is only
flipped one time, at the very start of the agent-environment interaction:
it is not repeatedly flipped every turn. Intuitively, it seems like the
expected total reward in the agent-environment interaction
when $\sigma$ interacts with $\mu$, should be the average
of the corresponding expected total rewards when $\pi_1$ or $\pi_2$ interact
with $\mu$. But this is all quite informal, as the RL
framework does not actually provide any mechanism for such an initial
coin-flip.

More generally, given agents $\vec{\pi}=(\pi_1,\ldots,\pi_n)$ and positive real
numbers
$\vec{w}=(w_1,\ldots,w_n)$ with $w_1+\cdots+w_n=1$, we could imagine an agent $\sigma$
who, at the start of each agent-environment interaction, randomly chooses
an agent $\pi_i$ to act as (each candidate $\pi_i$ being chosen with probability
$w_i$). If each $\pi_i$ would get total expected reward $R_i$ from an environment
$\mu$, we would expect $\sigma$ should get total expected reward
$\vec{w}\cdot \vec{r}=w_1R_1+\cdots+w_nR_n$ from that environment.
But again, the RL
framework has no mechanism for such an agent $\sigma$. We will define
an agent $\vec{w}\cdot\vec{\pi}$, called a \emph{mixture agent},
within the constraints
of the RL framework, and prove that ``the expected total reward of the
weighted mixture is the weighted average of the expected total rewards''.
We will then use this mixture operation to prove a number of interesting results.

\section{Preliminaries}

In defining \emph{agent} and \emph{environment} below, we attempt to follow
Legg and Hutter \cite{legg2007universal} as closely as possible,
except that we permit environments to output rewards from $\mathbb Q \cap [-1,1]$
rather than just $\mathbb Q\cap [0,1]$ (and, accordingly, we modify which well-behaved
environments to restrict our attention to).

Throughout the paper, we implicitly
fix non-empty finite sets $\mathcal A$ of \emph{actions},
$\mathcal O$ of \emph{observations},
and $\mathcal R\subseteq \mathbb Q\cap [-1,1]$ of \emph{rewards},
with the constraint that
whenever $\mathcal R$ contains any reward $r$, then $\mathcal R$
also contains $-r$. In other words, $\mathcal R$ is closed under negations.
By $\epsilon$ we mean the empty sequence.
By $\mathcal E$ we mean $\mathcal O\times\mathcal R$ (the set of all observation-reward
pairs); elements of $\mathcal E$ are called \emph{percepts}.
By $\Delta\mathcal A$ (resp.\ $\Delta\mathcal E$) we mean the set of all $\mathbb Q$-valued
probability distributions on $\mathcal A$ (resp.\ on $\mathcal E$).

\begin{definition}
\label{omnibusdefn}
    (Agents, environments, etc.)
    \begin{enumerate}
        \item
        We denote the set of all finite sequences
        of alternating percept-action pairs $x_1y_1\ldots x_ty_t$
        by $(\mathcal E\mathcal A)^*$.
        We also include $\epsilon$ in $(\mathcal E\mathcal A)^*$.
        Nonempty elements of $(\mathcal E\mathcal A)^*$ have the
        form $x_1y_1\ldots x_ty_t$ where each $x_i$ is a percept and
        each $y_i$ is an action.
        \item
        % By $(\mathcal E\mathcal A)^* \mathcal E$ we mean
        We denote the set of all sequences of the form $sx$ (where
        $s\in (\mathcal E\mathcal A)^*$, $x\in\mathcal E$, and $sx$
        is the result of appending $x$ to $s$) by
        $(\mathcal E\mathcal A)^*\mathcal E$.
        Elements of $(\mathcal E\mathcal A)^* \mathcal E$
        of length $>1$ have the form
        $x_1y_1\ldots x_{t-1}y_{t-1}x_t$
        (each $x_i$ a percept, each $y_i$ an action).
        \item
        An \emph{agent} is defined to be a function
        $\pi:(\mathcal E\mathcal A)^*\mathcal E\to \Delta \mathcal A$.
        For any $h\in (\mathcal E\mathcal A)^*\mathcal E$,
        we write $\pi(\cdot|h)$ for the value of $\pi$ at $h$, and
        for any $y\in \mathcal A$, we write $\pi(y|h)$ for
        $(\pi(\cdot|h))(y)$.
        Intuitively, $\pi(y|h)$ is the probability that agent $\pi$
        will take action $y$ in response to history $h$.
        \item
        An \emph{environment} is defined to be a function
        $\mu:(\mathcal E\mathcal A)^*\to\Delta\mathcal E$.
        For every $h\in(\mathcal E\mathcal A)^*$, we write
        $\mu(\cdot|h)$ for the value of $\mu$ at $h$, and for any
        $x\in\mathcal E$, we write $\mu(x|h)$ for $(\mu(\cdot|h))(x)$.
        If $x=(o,r)$ ($o\in\mathcal O$, $r\in\mathcal R$), we may also
        write $\mu(o,r|h)$ for $(\mu(\cdot|h))(x)$.
        Intuitively, $\mu(o,r|h)$ is the probability that environment
        $\mu$ will issue percept $(o,r)$ (observation $o$ and reward $r$)
        to the agent in response to history $h$.
    \end{enumerate}
\end{definition}

\begin{remark}
\label{impossibleremark}
    Note that in Definition \ref{omnibusdefn} part 3, we require,
    e.g., $\pi(\cdot|x_1y_1x_2)$ to be defined even if
    $\pi(y_1|x_1)=0$, in which case the initial percept-action sequence $x_1y_1x_2$
    would have probability $0$ of ever occurring in any agent-environment
    interaction. Intuitively: an agent must be able to choose actions even
    in response to histories that would never occur with nonzero probability.
    For example, even if, for its initial action, an agent assigns
    probability $0$ to action $A$, the agent must nevertheless
    be defined on histories where $A$ occurred as initial action.
    This convention (in which we follow \cite{legg2007universal}) simplifies most
    definitions, though it will complicate our definition of mixture agents
    (Definition \ref{maindefn} below). The agent's outputs on such
    $0$-probability inputs have no effect on the agent's performance
    in any environment, but these outputs may have implications for introspection
    and self-reflection, see \cite{extendedenvironmentspaper}.
\end{remark}

\begin{definition}
    By $\mathcal H$ we mean
    $((\mathcal E\mathcal A)^*)\cup((\mathcal E\mathcal A)^*\mathcal E)$,
    in other words, $\mathcal H$ is the set of alternating percept-action
    sequences that are empty or else start with a percept and can end with
    either a percept or an action.
    We refer to elements $h$ of $\mathcal H$ as \emph{histories} (a history
    may terminate with either a percept or an action).
\end{definition}

\begin{definition}
\label{pullbackdef}
    \begin{enumerate}
        \item
        For every agent $\pi$, for every $h\in\mathcal H$, we define a real number
        $P^\pi(h)\in[0,1]$ as follows.
        \begin{itemize}
            \item
            If $h=\epsilon$ then $P^\pi(h)=1$.
            \item
            If $h=gx$ for some $x\in\mathcal E$ then $P^\pi(h)=P^\pi(g)$.
            \item
            If $h=gy$ for some $y\in\mathcal A$ then $P^\pi(h)=P^\pi(g)\pi(y|g)$.
        \end{itemize}
        The intended intuition is that $P^\pi(h)$ can be thought of as the
        conditional probability that when $\pi$
        and any environment
        $\mu$ interact, the interaction will begin with initial segment
        $h$, \emph{assuming}
        that $\mu$ produces the percepts in $h$.
        \item
        For every environment $\mu$, for every $h\in\mathcal H$, we define a real number
        $P_\mu(h)\in[0,1]$ as follows.
        \begin{itemize}
            \item
            If $h=\epsilon$ then $P_\mu(h)=1$.
            \item
            If $h=gx$ for some $x\in\mathcal E$ then $P_\mu(h)=P_\mu(g)\mu(x|g)$.
            \item
            If $h=gy$ for some $y\in\mathcal A$ then $P_\mu(h)=P_\mu(g)$.
        \end{itemize}
        The intended intuition is that $P_\mu(h)$ can be thought of as the
        conditional probability that when any agent $\pi$ and $\mu$ interact,
        the interaction will begin with initial segment $h$, \emph{assuming} that
        $\pi$ produces the actions in $h$.
        \item
        For every agent $\pi$, environment $\mu$, and $h\in\mathcal H$, we define a
        real number $P^\pi_\mu(h)\in[0,1]$ by induction as follows.
        \begin{itemize}
            \item
            If $h=\epsilon$ then $P^\pi_\mu(h)=1$.
            \item
            If $h=gx$ for some $x\in\mathcal E$ then
            $P^\pi_\mu(h)=P^\pi_\mu(g)\mu(x|g)$.
            \item
            If $h=gy$ for some $y\in\mathcal A$ then
            $P^\pi_\mu(h)=P^\pi_\mu(g)\pi(y|g)$.
        \end{itemize}
        The intended intuition is that $P^\pi_\mu(h)$ can be thought of as
        the probability that when $\pi$ and $\mu$ interact, the interaction
        will begin with initial segment $h$.
    \end{enumerate}
\end{definition}

Some authors, such as \cite{hutter2009discrete}, would write $P(h)$ or a variation thereof
for $P^\pi_\mu$, provided $\pi$ and $\mu$ are clear from context.

One could alternately more directly define
\[
    P^\pi(x_1y_1\ldots x_ty_t)
    = \pi(y_1|x_1)\pi(y_2|x_1y_1x_2)\cdots \pi(y_t|x_1y_1\ldots x_t),
\]
and similarly define $P^\pi(x_1y_1\ldots x_t)$,
and likewise for $P_\mu$ and for $P^\pi_\mu$.

\begin{lemma}
\label{factorizationlemma}
    For all $h$, $\pi$, $\mu$ as in Definition \ref{pullbackdef},
    \[
        P^\pi_\mu(h) = P^\pi(h)P_\mu(h).
    \]
\end{lemma}

\begin{proof}
    By induction on $h$.

    Case 1: $h=\epsilon$. Then the lemma is trivial.

    Case 2: $h=gx$ for some $x\in\mathcal E$.
        Then
        \begin{align*}
            P^\pi_\mu(h)
                &= P^\pi_\mu(g)\mu(x|g)
                    &\mbox{(Definition \ref{pullbackdef} part 3)}\\
                &= P^\pi(g)P_\mu(g)\mu(x|g)
                    &\mbox{(Induction)}\\
                &= P^\pi(h)P_\mu(g)\mu(x|g)
                    &\mbox{(Definition \ref{pullbackdef} part 1)}\\
                &= P^\pi(h)P_\mu(h).
                    &\mbox{(Definition \ref{pullbackdef} part 2)}
        \end{align*}

    Case 3: $h=gy$ for some $y\in\mathcal A$.
        Similar to Case 2.
\end{proof}

In the following definition (and the rest of the paper),
$\mathbb N$ denotes the set $\{0,1,2,\ldots\}$ of non-negative
integers.

\begin{definition}
\label{performancedefn}
    (Performance in an environment)
    Let $\pi$ be an agent, $\mu$ an environment.
    \begin{enumerate}
    \item
        For every $t\in\mathbb N$,
        we define
        \[
            V^\pi_{\mu,t}=\sum_{h\in X_t}P^\pi_\mu(h)R(h)
        \]
        where $X_t\subseteq\mathcal H$ is the set of all
        length-$2t$ histories (i.e., all $h\in\mathcal H$ of the form
        $x_1y_1\ldots x_ty_t$ (each $x_i\in\mathcal E$, each $y_i\in\mathcal A$)
        provided $t>0$) and $R(h)$ is the sum of the rewards in $h$.
        Intuitively, $V^\pi_{\mu,t}$ is the expected total reward
        if $\pi$ were to interact with $\mu$ for $t$ steps.
        Note that $X_0=\{\epsilon\}$ and so $V^\pi_{\mu,0}=0$.
    \item
        We define $V^\pi_\mu=\lim_{t\to\infty}V^\pi_{\mu,t}$,
        provided the limit converges to a real number.
        Intuitively, $V^\pi_\mu$ is the expected total reward which $\pi$ would extract
        from $\mu$.
    \end{enumerate}
\end{definition}

Note that it is possible for $V^\pi_\mu$ to be undefined.
For example, if $\mu$ is an environment which always issues
reward $(-1)^t$ in response to the agent's $t$th action $y_t$,
then $V^\pi_\mu$ is undefined for every agent $\pi$.
But, following \cite{legg2007universal}, we will only be interested in
environments $\mu$ such that $V^\pi_\mu$
is always defined. Note also that, following \cite{legg2007universal},
we delegate any possible reward discounting to the environments themselves,
rather than build a fixed reward discounting factor into the definition
of $V^\pi_\mu$.

\begin{definition}
\label{wellbehaveddefn}
    An environment $\mu$ is \emph{well-behaved} if $\mu$ is Turing
    computable and the following
    condition holds: for every agent $\pi$, $V^\pi_\mu$ exists and
    $-1\leq V^\pi_\mu\leq 1$. Let $W$ be the set of all well-behaved environments.
\end{definition}

\begin{definition}
\label{performanceaveragerdefn}
    By a \emph{weighted intelligence measure}, we mean a function
    $\Upsilon:\Pi\to \mathbb R$ (where $\Pi$ is the set of all agents)
    such that there exist non-negative reals $\{w_\mu\}_{\mu\in W}$ such that:
    \begin{enumerate}
        \item
        $\sum_{\mu\in W}w_\mu$ converges.
        \item
        For every agent $\pi$, $\Upsilon(\pi)=\sum_{\mu\in W}w_\mu V^\pi_\mu$.
    \end{enumerate}
\end{definition}

The prototypical weighted intelligence measure is the Legg-Hutter intelligence
measure $\Upsilon$ introduced in \cite{legg2007universal},
where each well-behaved $\mu$ is
weighed using the universal prior \cite{li2008introduction}, i.e.,
given weight $2^{-K(\mu)}$
where $K$ denotes Kolmogorov complexity ($K(\mu)$ exists because of the Turing
computability requirement in Definition \ref{wellbehaveddefn}).
This depends on a background universal
Turing machine, the choice of which is highly nontrivial
\cite{leike2015bad}.

We will repeatedly write proofs by induction on histories $h\in\mathcal H$
(rather than on the usual induction on natural
numbers), implicitly using the following lemma without mention.

\begin{lemma}
\label{trivialinductionlemma}
    Suppose $\mathscr P$ is a property of histories. In order to prove
    that $\mathscr P(h)$ is true
    for all $h\in\mathcal H$, it suffices to prove the following three cases:
    \begin{enumerate}
        \item $\mathscr P(\epsilon)$.
        \item If $h=gy$ for some $y\in\mathcal A$, if $\mathscr P(g)$, then $\mathscr P(h)$.
        \item If $h=gx$ for some $x\in\mathcal E$, if $\mathscr P(g)$, then $\mathscr P(h)$.
    \end{enumerate}
    In proving (2) and (3), we refer to the hypothesis ``$\mathscr P(g)$'' as the
    ``Induction Hypothesis'' (or just ``Induction'').
\end{lemma}

\begin{proof}
    By induction on the length $\ell$ of $h\in\mathcal H$.
    For the base case, if $\ell=0$ then $h=\epsilon$ so $\mathscr P(h)$ holds by (1).
    For the induction step, assume $\ell>0$.
    If $h\in(\mathcal E\mathcal A)^*$ then this implies
    $h=gy$ for some $y\in\mathcal A$ such that $g$ has length $\ell-1$.
    By induction, $\mathscr P(g)$. Thus $\mathscr P(h)$ by (2).
    Otherwise, $h\in(\mathcal E\mathcal A)^*\mathcal A$, which implies
    $h=gx$ for some $x\in\mathcal E$ such that $g$ has length $\ell-1$.
    By induction, $\mathscr P(g)$, thus $\mathscr P(h)$ by (3).
\end{proof}

\section{Mixture agents}

Before defining mixture agents, we will first extend some of the above
definitions to vectors of agents.

\begin{definition}
\label{vectorizationdefn}
    Suppose $\vec\pi=(\pi_1,\ldots,\pi_n)$ is a vector of agents, $\mu$ is an environment,
    $h\in\mathcal H$, $t\in\mathbb N$, and $\Upsilon$ is a weighted
    intelligence measure. We define:
    \begin{itemize}
        \item ${P^{\vec\pi}}(h)=(P^{\pi_1}(h),\ldots,P^{\pi_n}(h))$.
        \item $P^{\vec\pi}_\mu(h)=(P^{\pi_1}_\mu(h),\ldots,P^{\pi_n}_\mu(h))$.
        \item $V^{\vec\pi}_{\mu,t}=(V^{\pi_1}_{\mu,t},\ldots,V^{\pi_n}_{\mu,t})$.
        \item $V^{\vec\pi}_\mu=(V^{\pi_1}_\mu,\ldots,V^{\pi_n}_\mu)$,
            provided $V^{\pi_1}_\mu,\ldots,V^{\pi_n}_\mu$ are defined.
        \item $\Upsilon(\vec\pi)=(\Upsilon(\pi_1),\ldots,\Upsilon(\pi_n))$.
    \end{itemize}
\end{definition}

We also recall the operation of dot product:

\begin{definition}
    If $\vec u=(u_1,\ldots,u_n)$ and
    $\vec v=(v_1,\ldots,v_n)$ are any two equal-length
    vectors of real numbers, then their \emph{dot product}
    is defined to be $\vec u\cdot \vec v=u_1v_1+\cdots+u_nv_n$.
\end{definition}

Now we are ready to define mixture agents.

\begin{definition}
\label{maindefn}
    (Mixture agents)
    Suppose $\vec\pi=(\pi_1,\ldots,\pi_n)$ are agents and $\vec w=(w_1,\ldots,w_n)$
    are positive real numbers with $w_1+\cdots+w_n=1$.
    Define the \emph{mixture agent} $\vec w\cdot\vec\pi$ as follows: for all
    $h\in (\mathcal E\mathcal A)^*\mathcal E$, $y\in\mathcal A$, let
    \[
        (\vec w\cdot\vec\pi)(y|h)
        =
        \begin{cases}
            \dfrac{\vec w\cdot {P^{\vec\pi}}(hy)}{\vec w\cdot {P^{\vec\pi}}(h)}
            &\mbox{if $\vec w\cdot {P^{\vec\pi}}(h)\not=0$,}\\
            1/|\mathcal{A}| &\mbox{otherwise.}
        \end{cases}
    \]
\end{definition}

\begin{remark}
    Definition \ref{maindefn} is complicated by the way
    Definition \ref{omnibusdefn} part 3
    forces us to include the case
    $(\vec w\cdot\vec\pi)(y|h)=1/|\mathcal A|$ when
    $\vec w\cdot {P^{\vec\pi}}(h)=0$
    (see Remark \ref{impossibleremark}): we are obligated to specify how
    $\vec w\cdot\vec\pi$ chooses actions even in response to histories that
    are ``impossible'' for $\vec w\cdot\vec\pi$ (histories which contain actions that
    $\vec w\cdot\vec\pi$ would never take in those circumstances).
    The value $1/|\mathcal A|$ in this case is completely arbitrary;
    any other probabilities would be just as good,
    provided $\vec w\cdot\vec pi(h)\in\Delta\mathcal A$.
\end{remark}

\begin{lemma}
\label{mixturereallyisanagent}
    If $\vec\pi$ and $\vec w$ are as in Definition \ref{maindefn}
    then the mixture agent $\vec w\cdot\vec\pi$ is an agent
    (per Definition \ref{omnibusdefn} part 3).
\end{lemma}

\begin{proof}
    Let $h\in(\mathcal E\mathcal A)^*\mathcal E$.
    Clearly $(\vec w\cdot\vec\pi)(y|h)\geq 0$ for all $y\in\mathcal A$.
    It remains to show
    $\sum_{y\in\mathcal A}(\vec w\cdot\vec\pi)(y|h)=1$.

    Case 1: $\vec w\cdot {P^{\vec\pi}}(h)=0$. Then
    each $(\vec w\cdot\vec\pi)(y|h)=1/|\mathcal A|$ so the
    claim is immediate.

    Case 2: $\vec w\cdot {P^{\vec\pi}}(h)\not=0$. Then
    \begin{align*}
        &{} \sum_{y\in\mathcal A}(\vec w\cdot\vec\pi)(y|h)\\
            &= \sum_{y\in\mathcal A}
                \frac{\vec w\cdot {P^{\vec\pi}}(hy)}{\vec w\cdot {P^{\vec\pi}}(h)}
                &\mbox{(Definition \ref{maindefn})}\\
            &= \sum_{y\in\mathcal A}
                \frac
                {\vec w\cdot(P^{\pi_1}(hy),\ldots,P^{\pi_n}(hy))}
                {\vec w\cdot {P^{\vec\pi}}(h)}
                &\mbox{(Definition \ref{vectorizationdefn})}\\
            &= \sum_{y\in\mathcal A}
                \frac
                {w_1 P^{\pi_1}(h)\pi_1(y|h)+\cdots+w_n P^{\pi_n}(h)\pi_n(y|h)}
                {\vec w\cdot {P^{\vec\pi}}(h)}
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= \frac{
                w_1 P^{\pi_1}(h)\left(\mbox{$\sum_{y\in\mathcal A}\pi_1(y|h)$}\right)
                +\cdots+
                w_n P^{\pi_n}(h)\left(\mbox{$\sum_{y\in\mathcal A}\pi_n(y|h)$}\right)
                }
                {\vec w\cdot {P^{\vec\pi}}(h)}
                &\mbox{(Algebra)}\\
            &= \frac
                {w_1 P^{\pi_1}(h)\cdot 1 + \cdots + w_n P^{\pi_n}(h)\cdot1}
                {\vec w\cdot {P^{\vec\pi}}(h)}
                =\frac{\vec w\cdot {P^{\vec\pi}}(h)}{\vec w\cdot {P^{\vec\pi}}(h)}=1.
                &\mbox{($\pi_i$ are agents)}
    \end{align*}
\end{proof}

We will frequently use Lemma \ref{mixturereallyisanagent} without explicit mention.
For example, the lemma allows us to speak of $P^{\vec w\cdot \vec\pi}(h)$
(Definition \ref{pullbackdefn}), $V^{\vec w\cdot\vec\pi}_{\mu}$
(Definition \ref{performancedefn}), etc., and we will freely do so without
explicitly citing Lemma \ref{mixturereallyisanagent}.

\begin{theorem}
\label{maintheorem}
    (Commutativity of $\vec w$)
    Let $\vec\pi=(\pi_1,\ldots,\pi_n)$ be agents.
    Let $\vec w=(w_1,\ldots,w_n)$ be positive reals with
    $w_1+\cdots+w_n=1$. Let $\mu$ be any environment.
    Then:
    \begin{enumerate}
        \item
        For any $h\in\mathcal H$,
        $P^{\vec w\cdot \vec\pi}(h)=\vec w\cdot {P^{\vec\pi}}(h)$.
        \item
        For any $h\in\mathcal H$,
        $P^{\vec w\cdot \vec\pi}_\mu(h)=\vec w \cdot P^{\vec\pi}_\mu(h)$.
        \item
        For any $t\in\mathbb N$,
        $V^{\vec w\cdot \vec\pi}_{\mu,t}=\vec w\cdot V^{\vec\pi}_{\mu,t}$.
        \item
        (``The expected reward of a weighted mixture is the weighted
        average of the expected rewards'')
        If $V^{\vec\pi}_\mu$ converges, then $V^{\vec w\cdot\vec\pi}_\mu$
        converges and $V^{\vec w\cdot\vec\pi}_\mu=\vec w\cdot V^{\vec\pi}_\mu$.
        \item
        (``The intelligence of a weighted mixture is the weighted average
        of the intelligences'')
        For any weighted intelligence measure $\Upsilon$,
        $\Upsilon(\vec w\cdot\vec\pi)=\vec w\cdot\Upsilon(\vec\pi)$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    (1) By induction on $h$ (using Lemma \ref{trivialinductionlemma}).

    Case 1: $h=\epsilon$. Then
    $P^{\vec w\cdot\vec\pi}(h)=1=w_1\cdot 1+\cdots+w_n\cdot 1
    =\vec w\cdot ({P^{\vec\pi}}(h))$.

    Case 2: $h=gx$ for some $x\in\mathcal E$. Then
    \begin{align*}
        P^{\vec w\cdot\vec\pi}(h)
            &= P^{\vec w\cdot\vec\pi}(g)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= \vec w\cdot({P^{\vec\pi}}(g))
                &\mbox{(Induction)}\\
            &= \vec w\cdot({P^{\vec\pi}}(gx))
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= \vec w\cdot({P^{\vec\pi}}(h)).
    \end{align*}

    Case 3: $h=gy$ for some $y\in\mathcal A$.

    Subcase 3.1: $P^{\vec w\cdot\vec\pi}(g)=0$.
        By induction $\vec w\cdot({P^{\vec\pi}}(g))=0$.
        Since the $w_i$ are positive, this implies
        each $P^{\pi_i}(g)=0$.
        Thus each
        \begin{align*}
            w_i P^{\pi_i}(gy)
                &= w_i P^{\pi_i}(g)\pi_i(y|g)
                    &\mbox{(Definition \ref{pullbackdef})}\\
                &= 0w_i\pi_i(y|g)=0,
        \end{align*}
        i.e., $\vec w\cdot({P^{\vec\pi}}(gy))=0$.
        And
        \begin{align*}
            P^{\vec w\cdot\vec\pi}(gy)
                &= P^{\vec w\cdot\vec\pi}(g)(\vec w\cdot\vec\pi)(y|g)
                    &\mbox{(Definition \ref{pullbackdef})}\\
                &= 0(\vec w\cdot\vec\pi)(y|g) = 0,
        \end{align*}
        so $P^{\vec w\cdot\vec\pi}(h)=\vec w\cdot P^{\vec\pi}(h)=0$.

    Subcase 3.2: ${P^{\vec w\cdot\vec\pi}}(g)\not=0$. Then
    \begin{align*}
        P^{\vec w\cdot\vec\pi}(h)
            &= P^{\vec w\cdot\vec\pi}(g)(\vec w\cdot\vec\pi)(y|g)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= P^{\vec w\cdot\vec\pi}(g)
                \frac
                {\vec w\cdot {P^{\vec\pi}}(gy)}
                {\vec w\cdot {P^{\vec\pi}}(g)}
                &\mbox{(Definition \ref{maindefn})}\\
            &= \vec w\cdot {P^{\vec\pi}}(g)
                \frac
                {\vec w\cdot {P^{\vec\pi}}(gy)}
                {\vec w\cdot {P^{\vec\pi}}(g)}
                &\mbox{(Induction)}\\
            &= \vec w\cdot {P^{\vec\pi}}(gy)
                &\mbox{(Basic Algebra)}\\
            &= \vec w\cdot {P^{\vec\pi}}(h).
    \end{align*}

    (2) Follows from (1) and Lemma \ref{factorizationlemma}.

    (3) Follows from (2) and Definition \ref{performancedefn} part 1.

    (4) Follows from (3) and Definition \ref{performancedefn} part 2.

    (5) Follows from (4) and Definition \ref{performanceaveragerdefn}.
\end{proof}

\section{Duality and self-dual agents:
Strengthening and formalizing a result of Alexander and Hutter}

\begin{definition}
\label{dualagentsdefn}
(Dual Agents and Dual Environments)
\begin{enumerate}
    \item
    For each $h\in\mathcal H$,
    we define the \emph{dual} of $h$, denoted $\overline h$, to be
    the sequence obtained
    by replacing every percept $(o,r)$ in $h$ by $(o,-r)$ (in other words:
    replacing every reward $r$ in $h$ by $-r$).
    \item
    Suppose $\pi$ is an agent.
    We define the \emph{dual} of $\pi$, denoted $\overline \pi$, as follows:
    for each $h\in (\mathcal E\mathcal A)^*\mathcal E$,
    for each action $y\in\mathcal A$,
    \[\overline\pi(y|h)=\pi(y|\overline h).\]
    \item
    Suppose $\mu$ is an environment.
    We define the \emph{dual} of $\mu$, denoted $\overline\mu$, as follows:
    for each $h\in (\mathcal E\mathcal A)^*$,
    for each $(o,r)\in\mathcal E$,
    \[\overline\mu(o,r|h)=\mu(o,-r|\overline h).\]
\end{enumerate}
\end{definition}

\begin{lemma}
\label{doublenegationlemma}
    If $x$ is any agent, environment, or history,
    then $\overline{\overline x}=x$.
\end{lemma}

\begin{proof}
    Trivial as $-(-r)=r$ for all real $r$.
\end{proof}

\begin{lemma}
\label{asteriskcommuteswithoverlinelemma}
    For any agent $\pi$ and any $h\in(\mathcal E\mathcal A)^*\mathcal E$,
    $P^\pi(\overline h)=P^{\overline{\pi}}(h)$.
\end{lemma}

\begin{proof}
    By induction on $h$.
\end{proof}

\begin{definition}
    (Self-dual agents)
    An agent $\pi$ is \emph{self-dual} if $\overline{\pi}=\pi$.
\end{definition}

We will use self-dual agents to strengthen and formalize an informal result
from \cite{alexander2021reward}.
But first, we will use mixture agents to characterize
self-dual agents (up to equivalence modulo a certain natural equivalence relation).

\begin{definition}
\label{equivdefn}
    If $\pi$ and $\rho$ are agents, we say $\pi\equiv\rho$ if the
    following conditions hold:
    \begin{enumerate}
        \item For all $h\in\mathcal H$, $P^\pi(h)=0$ iff $P^\rho(h)=0$.
        \item For all $h\in(\mathcal E\mathcal A)^*\mathcal E$,
            if $P^\pi(h)\not=0$ then for all $y\in\mathcal A$,
            $\pi(y|h)=\rho(y|h)$.
    \end{enumerate}
\end{definition}

\begin{remark}
    Intuitively, Definition \ref{equivdefn} says that $\pi\equiv\rho$
    iff the histories which are ``possible'' for $\pi$ (in the sense of
    Remark \ref{impossibleremark}) are exactly the histories which are
    ``possible'' for $\rho$, and $\pi=\rho$ on those histories.
\end{remark}

\begin{lemma}
\label{equivrelationlemma}
    The relation $\equiv$ of Definition \ref{equivdefn} is an equivalence
    relation.
\end{lemma}

\begin{proof}
    Straightforward.
\end{proof}

\begin{lemma}
\label{piopluspilemma}
    Let $\vec w=(w_1,\ldots,w_n)$ be positive reals,
    $w_1+\cdots+w_n=1$. For every agent $\pi$,
    $\pi\equiv\vec w\cdot (\pi,\ldots,\pi)$ (where
    $(\pi,\ldots,\pi)$ has length $n$).
\end{lemma}

\begin{proof}
    Write $\vec\pi$ for $(\pi,\ldots,\pi)$.
    We prove conditions 1 and 2 of Definition \ref{equivdefn}
    simultaneously by induction on $h$.
 
    Case 1: $h=\epsilon$. Then
    $P^\pi(h)=\vec w\cdot{P^{\vec\pi}}(h)=1\not=0$, so
    vacuously $P^\pi(h)=0$ iff $\vec w\cdot{P^{\vec\pi}}(h)=0$
    (proving condition 1).
    For condition 2, there is nothing to check, since
    $\epsilon\not\in(\mathcal E\mathcal A)^*\mathcal E$.

    Case 2: $h=h_0y_0$ for some
        $h_0\in(\mathcal E\mathcal A)^*\mathcal E$, $y_0\in\mathcal A$.
        For condition 2, there is nothing to prove, since
        $h\not\in(\mathcal E\mathcal A)^*\mathcal E$.
        For condition 1, we consider two cases.

        Subcase 2.1: $P^\pi(h_0)=0$.
        By induction, condition 1 holds for $h_0$, so
        $\vec w\cdot{P^{\vec\pi}}(h_0)=0$.
        By Definition \ref{pullbackdef},
        $P^\pi(h)=0\pi(y_0|h_0)=0$
        and $P^{\vec w\cdot\vec\pi}(h)=0(\vec w\cdot\vec\pi)(y_0|h_0)=0$.
        So $P^\pi(h)=0$ iff $P^{\vec w\cdot\vec\pi}(h)=0$.

        Subcase 2.2: $P^\pi(h_0)\not=0$.
        Then
        \begin{align*}
            P^{\vec w\cdot \vec\pi}(h)
                &= \vec w\cdot{P^{\vec\pi}}(h)
                    &\mbox{(Theorem \ref{maintheorem})}\\
                &= w_1 P^\pi(h)+\cdots+w_n P^\pi(h)
                    &\mbox{(Def.\ of $\vec w$ and $\vec\pi$)}\\
                &= P^\pi(h)
                    &\mbox{($w_1+\cdots+w_n=1$)}\\
                &= P^\pi(h_0)\pi(y_0|h_0).
                    &\mbox{(Definition \ref{pullbackdef})}
        \end{align*}
        Since $P^\pi(h_0)\not=0$, and since $\mathbb R$ has the zero-product property,
        it follows that
        $P^{\vec w\cdot\vec\pi}(h)=0$ iff $P^\pi(h)=0$ iff $\pi(y_0|h_0)=0$.

    Case 3: $h=h_0x$ for some $h_0\in (\mathcal E\mathcal A)^*$,
        $x\in\mathcal E$.
        By induction, conditions 1 and 2 hold for $h_0$.
        By Definition \ref{pullbackdef},
        $P^\pi(h)=P^\pi(h_0)$ and
        $P^{\vec w\cdot\vec\pi}(h)=P^{\vec w\cdot\vec\pi}(h_0)$,
        so condition 1 for $h$ follows.

        For condition 2,
        assume $P^\pi(h)\not=0$ and let $y\in\mathcal A$.
        By choice of $\vec w$ and $\vec\pi$,
        $\vec w\cdot{P^{\vec\pi}}(h)=w_1P^\pi(h)+\cdots+w_nP^\pi(h)=P^\pi(h)$.
        So, since $P^\pi(h)\not=0$, $\vec w\cdot{P^{\vec\pi}}(h)\not=0$.
        Thus
        \begin{align*}
            (\vec w\cdot\vec\pi)(y|h)
                &= \frac{\vec w\cdot {P^{\vec\pi}}(hy)}{\vec w\cdot{P^{\vec\pi}}(h)}
                    &\mbox{(Definition \ref{maindefn})}\\
                &= \frac{w_1P^\pi(hy)+\cdots+w_nP^\pi(hy)}
                    {w_1P^\pi(h)+\cdots+w_nP^\pi(h)}
                    &\mbox{(Def.\ of $\vec w$ and $\vec\pi$)}\\
                &= \frac{w_1P^\pi(h)+\cdots+w_nP^\pi(h)}{w_1P^\pi(h)+\cdots+w_nP^\pi(h)}
                    \pi(y|h)
                    &\mbox{(Definition \ref{pullbackdef})}\\
                &= \pi(y|h).
        \end{align*}
\end{proof}

\begin{lemma}
\label{reflectionmakesjanuslemma}
    For any agent $\pi$,
    $(\frac12,\frac12)\cdot(\pi,\overline\pi)$ is self-dual.
\end{lemma}

\begin{proof}
    Let $\vec w=(\frac12,\frac12)$.
    For any $h\in(\mathcal E\mathcal A)^*\mathcal E$ and $y\in\mathcal A$,
    we claim
    $\overline{\vec w\cdot(\pi,\overline{\pi})}(y|h)
    =(\vec w\cdot(\pi,\overline{\pi}))(y|h)$.
    Noting that
    $\vec w\cdot P^{(\pi,\overline{\pi})}(h)
    =\frac12P^\pi(h)+\frac12P^{\overline\pi}(h)$
    and
    $\vec w\cdot P^{(\pi,\overline{\pi})}(\overline h)
    =\frac12P^\pi(\overline h)+\frac12P^{\overline\pi}(\overline h)$,
    Lemmas \ref{doublenegationlemma} and \ref{asteriskcommuteswithoverlinelemma}
    imply that
    $\vec w\cdot P^{(\pi,\overline{\pi})}(h)
    =\vec w\cdot P^{(\pi,\overline{\pi})}(\overline h)$.
    So if
    $\vec w\cdot P^{(\pi,\overline{\pi})}(\overline h)=0$
    then $\vec w\cdot P^{(\pi,\overline{\pi})}(h)=0$
    and it follows from
    Definition \ref{maindefn}
    and Lemma \ref{asteriskcommuteswithoverlinelemma} that
    $\overline{\vec w\cdot(\pi,\overline{\pi})}(y|h)
    =(\vec w\cdot(\pi,\overline{\pi}))(y|h)=1/|\mathcal A|$.
    But assume $\vec w\cdot P^{(\pi,\overline{\pi})}(\overline h)\not=0$.
    Then:
    \begin{align*}
        \overline{\vec w\cdot(\pi,\overline{\pi})}(y|h)
        &= (\vec w\cdot(\pi,\overline{\pi}))(y|\overline h)
            &\mbox{(Definition \ref{dualagentsdefn})}\\
        &= \frac
            {\frac12P^\pi(\overline hy)+\frac12P^{\overline\pi}(\overline hy)}
            {\frac12P^\pi(\overline h)+\frac12P^{\overline\pi}(\overline h)}
            &\mbox{(Definition \ref{maindefn})}\\
        &= \frac
            {\frac12P^\pi(\overline{hy})+\frac12P^{\overline\pi}(\overline{hy})}
            {\frac12P^\pi(\overline h)+\frac12P^{\overline\pi}(\overline h)}
            &\mbox{(Clearly $\overline hy=\overline{hy}$)}\\
        &= \frac
            {\frac12P^{\overline\pi}(hy)+\frac12P^{\overline{\overline\pi}}(hy)}
            {\frac12P^{\overline\pi}(h)+\frac12P^{\overline{\overline\pi}}(h)}
            &\mbox{(Lemma \ref{asteriskcommuteswithoverlinelemma})}\\
        &= (\vec w\cdot(\overline{\overline\pi},\overline\pi))(y|h)
            &\mbox{(Definition \ref{maindefn})}\\
        &= (\vec w\cdot(\pi,\overline\pi))(y|h).
            &\mbox{(Lemma \ref{doublenegationlemma})}
    \end{align*}
\end{proof}

\begin{proposition}
    (Characterization of self-dual agents modulo $\equiv$)
    For any agent $\pi$, the following are equivalent:
    \begin{enumerate}
        \item $\pi\equiv\rho$ for some self-dual agent $\rho$.
        \item $\pi\equiv(\frac12,\frac12)\cdot(\rho,\overline{\rho})$
            for some agent $\rho$.
    \end{enumerate}
\end{proposition}

\begin{proof}
    ($\Rightarrow$)
    Assume $\pi\equiv\rho$ for some self-dual agent $\rho$.
    Then $\rho=\overline{\rho}$, so
    $\rho\equiv (\frac12,\frac12)\cdot(\rho,\overline{\rho})$ by
    Lemma \ref{piopluspilemma}, thus, by Lemma \ref{equivrelationlemma},
    $\pi\equiv(\frac12,\frac12)\cdot(\rho,\overline{\rho})$.

    ($\Leftarrow$)
    Assume $\pi\equiv(\frac12,\frac12)\cdot(\rho,\overline{\rho})$ for some agent $\rho$.
    By Lemma \ref{reflectionmakesjanuslemma},
    $(\frac12,\frac12)\cdot(\rho,\overline{\rho})$ is self-dual, so we are done.
\end{proof}

The following theorem formalizes and strengthens an informal result
from \cite{alexander2021reward}.

\begin{theorem}
    Suppose $\Upsilon$ is a weighted intelligence measure.
    If $\Upsilon(\pi)=0$ for every self-dual agent $\pi$,
    then $\Upsilon(\overline{\pi})=-\Upsilon(\pi)$
    for every agent $\pi$.
\end{theorem}

\begin{proof}
    Let $\pi$ be any agent.
    By Lemma \ref{reflectionmakesjanuslemma},
    $(\frac12,\frac12)\cdot(\pi,\overline\pi)$ is self-dual.
    So by assumption,
    $\Upsilon((\frac12,\frac12)\cdot(\pi,\overline\pi))=0$.
    Thus by Theorem \ref{maintheorem},
    \[
        (\mbox{$\frac12$},\mbox{$\frac12$})\cdot\Upsilon((\pi,\overline\pi))
        =\mbox{$\frac12$}\Upsilon(\pi)+\mbox{$\frac12$}\Upsilon(\overline\pi)=0.
    \]
    So $\Upsilon(\overline{\pi})=-\Upsilon(\pi)$.
\end{proof}


\section{Detectability of sets of agents}

In this section, we give another application of mixture agents.

\begin{definition}
\label{incentivizabilitydefn}
    A set $\Pi$ of agents is \emph{detectable} if there exists
    an environment $\mu$ such that for every agent $\pi$:
    \begin{enumerate}
        \item
        $V^\pi_\mu$ exists.
        \item
        $V^\pi_\mu>0$ iff $\pi\in\Pi$.
    \end{enumerate}
\end{definition}

\begin{definition}
\label{mixtureclosuredef}
    A set $\Pi$ is \emph{closed under mixtures} if the following
    condition holds: for all positive reals $\vec w=(w_1,\ldots,w_n)$
    with $w_1+\cdots+w_n=1$,
    for all agents $\vec\pi=(\pi_1,\ldots,\pi_n)$,
    if $\pi_i\in \Pi$ for every $i=1,\ldots,n$, then
    $\vec w\cdot\vec\pi\in\Pi$.
\end{definition}

\begin{theorem}
\label{closuretheorem}
    (Necessary Conditition for Detectability)
    Let $\Pi$ be any set of agents.
    If $\Pi$ is detectable, then $\Pi$ is closed under mixtures, and so
    is its complement $\Pi^c$.
\end{theorem}

\begin{proof}
    Assume $\Pi$ is detectable.
    Let $\mu$ be as in
    Definition \ref{incentivizabilitydefn}.
    To see $\Pi$ is closed under mixtures, let $\vec w$, $\vec\pi=(\pi_1,\ldots,\pi_n)$ be
    as in Definition \ref{mixtureclosuredef}.
    Assume $\pi_i\in\Pi$ for all $i=1,\ldots,n$.
    By choice of $\mu$, $V^{\pi_i}_\mu>0$ for all $i=1,\ldots,n$.
    Thus
    \begin{align*}
        V^{\vec w\cdot\vec\pi}_\mu
            &= \vec w\cdot V^{\vec\pi}_\mu
                &\mbox{(Theorem \ref{maintheorem})}\\
            &> w_1\cdot 0 + \cdots + w_n\cdot 0 = 0,
    \end{align*}
    so by choice of $\mu$, $\vec w\cdot \vec\pi\in \Pi$.
    A similar argument shows that $\Pi^c$ is closed under mixtures.
\end{proof}


\section{Genericness of non-deterministic agents}

\begin{definition}
\label{modifyagentatoneplace}
    If $\pi$ is an agent, $h_0\in(\mathcal E\mathcal A)^*\mathcal E$,
    and $m$ is a probability distribution on $\mathcal A$,
    we write $\pi^{h_0\mapsto m}$ for the agent which is identical to $\pi$
    except that it maps $h_0$ to $m$, in other words:
    \[
        \pi^{h_0\mapsto m}(y|h)
        =
        \begin{cases}
            \pi(y|h) &\mbox{if $h\not=h_0$,}\\
            m(y) &\mbox{if $h=h_0$.}
        \end{cases}
    \]
\end{definition}

The following three lemmas are technical lemmas needed to prove
some interesting results.

\begin{lemma}
\label{firsttechlemmaforgenericity}
    Suppose $\pi$, $h_0$, $m$ are as in Definition \ref{modifyagentatoneplace}.
    Let $h\in\mathcal H$ be such that
    for every $y\in\mathcal A$,
    $h_0y$ is not an initial segment of $h$.
    Then $P^{\pi^{h_0\mapsto m}}(h)=P^\pi(h)$.
\end{lemma}

\begin{proof}
    By induction on $h$.
\end{proof}

\begin{lemma}
\label{thirdtechlemmaforgenericity}
    Suppose $\pi$, $h_0$, $m$ are as in Definition \ref{modifyagentatoneplace}.
    For any $y\in\mathcal A$,
    $P^{\pi^{h_0\mapsto m}}(h_0y)=P^\pi(h_0)m(y)$.
\end{lemma}

\begin{proof}
    Immediate by Definition \ref{pullbackdef} and Lemma \ref{firsttechlemmaforgenericity}.
\end{proof}

\begin{lemma}
\label{secondtechlemmaforgenericity}
    Suppose $\pi$, $h_0$, $m$ are as in Definition \ref{modifyagentatoneplace}.
    Assume $h\in\mathcal H$, $y_0\in\mathcal A$, and $h_0y_0$ is
    an initial segment of $h$. Assume $\pi(y_0|h_0)\not=0$. Then
    $P^{\pi^{h_0\mapsto m}}(h) = \frac{P^\pi(h)m(y_0)}{\pi(y_0|h_0)}$.
\end{lemma}

\begin{proof}
    By induction on $h$.

    Case 1: $h=h_0y_0$. Then
    \begin{align*}
        P^{\pi^{h_0\mapsto m}}(h)
        &= P^{\pi^{h_0\mapsto m}}(h_0)\pi^{h_0\mapsto m}(y_0|h_0)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= P^\pi(h_0)\pi^{h_0\mapsto m}(y_0|h_0)
            &\mbox{(Lemma \ref{firsttechlemmaforgenericity})}\\
        &= P^\pi(h_0)m(y_0)
            &\mbox{(Definition \ref{modifyagentatoneplace})}\\
        &= P^\pi(h_0)\pi(y_0|h_0)m(y_0)/\pi(y_0|h_0)
            &\mbox{(Basic Algebra)}\\
        &= P^\pi(h)m(y_0)/\pi(y_0|h_0).
            &\mbox{(Definition \ref{pullbackdef})}
    \end{align*}

    Case 2: $h=h_0 y_0 h_1 x$ for some $h_1\in\mathcal H$
        and $x\in\mathcal E$. Then
    \begin{align*}
        P^{\pi^{h_0\mapsto m}}(h)
        &= P^{\pi^{h_0\mapsto m}}(h_0 y_0 h_1)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= P^\pi(h_0 y_0 h_1)m(y_0)/\pi(y_0|h_0)
            &\mbox{(Induction)}\\
        &= P^\pi(h_0 a_0 h_1 x)m(y_0)/\pi(y_0|h_0)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= P^\pi(h)m(y_0)/\pi(y_0|h_0).
    \end{align*}

    Case 3: $h=h_0 y_0 h_1 y$ for some $h_1\in\mathcal H$ and
        $y\in\mathcal A$. Then
    \begin{align*}
        P^{\pi^{h_0\mapsto m}}(h)
        &= P^{\pi^{h_0\mapsto m}}(h_0 y_0 h_1)
            \pi^{h_0\mapsto m}(y|h_0 y_0 h_1)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= P^{\pi^{h_0\mapsto m}}(h_0 y_0 h_1)
            \pi(y|h_0 a h_1)
            &\mbox{(Definition \ref{modifyagentatoneplace})}\\
        &= P^\pi(h_0 y_0 h_1)\pi(y|h_0 y_0 h_1)m(y_0)/\pi(y_0|h_0)
            &\mbox{(Induction)}\\
        &= P^\pi(h_0 y_0 h_1 y)m(y_0)/\pi(y_0|h_0)
            &\mbox{(Definition \ref{pullbackdef})}\\
        &= P^\pi(h)m(y_0)/\pi(y_0|h_0).
    \end{align*}
\end{proof}

\begin{definition}
\label{sumofdistros}
    Suppose $\vec m=(m_1,\ldots,m_n)$ are probability distributions on $\mathcal A$
    and $\vec w=(w_1,\ldots,w_n)$ are positive reals with
    $w_1+\cdots+w_n=1$. By $\vec w\cdot\vec m$ we mean the probability distribution
    on $\mathcal A$ defined by
    \[
        (\vec w\cdot\vec m)(y) = w_1m_1(y) + \cdots + w_nm_n(y).
    \]
\end{definition}

\begin{lemma}
    If $\vec m$, $\vec w$ are as in Definition \ref{sumofdistros}
    then $\vec w\cdot\vec m$ really is a probability distribution on $\mathcal A$.
\end{lemma}

\begin{proof}
    Clearly for every $y\in\mathcal A$,
    $(\vec w\cdot\vec m)(y) = w_1m_1(y) + \cdots + w_nm_n(y)$ is a nonnegative
    real. It remains to show $\sum_{y\in\mathcal A}(\vec w\cdot\vec m)(y)=1$.
    We compute:
    \begin{align*}
        &{} \sum_{y\in\mathcal A}(\vec w\cdot\vec m)(y)\\
        &=
        \sum_{y\in\mathcal A} w_1m_1(y) + \cdots + w_nm_n(y)
            &\mbox{(Definition \ref{sumofdistros})}\\
        &=
        w_1\sum_{y\in\mathcal A} m_1(y) + \cdots + w_n\sum_{a\in\mathcal A}m_n(y)
            &\mbox{(Basic Algebra)}\\
        &= w_1 + \cdots + w_n
            &\mbox{($m_1,\ldots,m_n$ are probability distr's)}\\
        &= 1.
    \end{align*}
\end{proof}

\begin{definition}
    For any agent $\pi$, for any $h\in(\mathcal E\mathcal A)^*\mathcal E$,
    for any probability distributions $\vec m=(m_1,\ldots,m_n)$ on $\mathcal A$,
    let $\pi^{h\mapsto \vec m}=(\pi^{h\mapsto m_1},\ldots,\pi^{h\mapsto m_n})$.
\end{definition}

The following proposition shows that
for any particular history $h$ and agent $\pi$,
for any decomposition of $\pi(\cdot|h)$ into a weighted sum
of probability distributions $m_1,\ldots,m_n$,
$\pi$ has the same intelligence as the weighted mixture of the corresponding $n$ agents
$\pi^{h\mapsto \vec m}$.

\begin{proposition}
\label{longproposition}
    Let $\Upsilon$ be any weighted intelligence measure, let $\pi$ be any agent,
    and let $h\in(\mathcal E\mathcal A)^*\mathcal E$.
    Suppose $\vec m$ and $\vec w$ are as in Definition \ref{sumofdistros}.
    If $\vec w\cdot\vec m = \pi(\cdot|h)$, then
    $
        \Upsilon(\pi)
        =
        \Upsilon(\vec w\cdot \pi^{h\mapsto \vec m}).
    $
\end{proposition}

\begin{proof}
    For brevity, write $\vec\pi$ for $\pi^{h\mapsto \vec m}$. It suffices to
    show that for every well-behaved $\mu$ and every $t\in\mathbb N$,
    $
        V^{\pi}_{\mu,t}
        =
        V^{\vec w\cdot \vec\pi}_{\mu,t}.
    $
    By Definition \ref{performancedefn}, it suffices to show that for every
    well-behaved $\mu$ and every $g\in\mathcal H$,
    $
    P^\pi_\mu(g)
    =
    P^{\vec w\cdot\vec\pi}_\mu(g)
    $.
    By Lemma \ref{factorizationlemma}, it suffices
    to show that for every $g\in\mathcal H$,
    $P^\pi(g)=P^{\vec w\cdot\vec\pi}(g)$.
    We prove this by induction on $g$.

    Case 1: $g=\epsilon$.
    Then $P^\pi(g)=1
    =P^{\vec w\cdot\vec\pi}(g)$.

    Case 2: $g=fx$ for some $x\in\mathcal E$.
    Then
    \begin{align*}
        P^\pi(g)
            &= P^\pi(f)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= P^{\vec w\cdot\vec\pi}(f)
                &\mbox{(Induction)}\\
            &= P^{\vec w\cdot\vec\pi}(g).
                &\mbox{(Definition \ref{pullbackdef})}
    \end{align*}

    Case 3: $g=fy$ for some $y\in\mathcal A$.

    Subcase 3.1: $P^\pi(f)=0$.
    Then
    \begin{align*}
        P^{\vec w\cdot\vec\pi}(g)
            &= P^{\vec w\cdot\vec\pi}(f)
            (\vec w\cdot\vec\pi)(y|f)
                &\mbox{(Definition \ref{pullbackdef})}\\
            &= P^\pi(f)(\vec w\cdot\vec\pi)(y|f)
                &\mbox{(Induction)}\\
            &= 0.
    \end{align*}
    Similarly, $P^\pi(g)=0$. So $P^{\vec w\cdot\vec\pi}(g)=P^\pi(g)$.

    Subcase 3.2: $P^\pi(f)\not=0$ and $f=h$. Then:
    \begin{align*}
        P^{\vec w\cdot\vec\pi}(g)
            &= P^{\vec w\cdot\vec\pi}(hy)\\
            &= \vec w\cdot{P^{\vec\pi}}(hy)
                    &\mbox{(Theorem \ref{maintheorem})}\\
            &= w_1P^\pi(h)m_1(y)+\cdots+w_nP^\pi(h)m_n(y)
                    &\mbox{(Lemma \ref{thirdtechlemmaforgenericity})}\\
            &= P^\pi(h)\pi(y|h)
                    &\mbox{($\vec w\cdot\vec m=\pi(\cdot|h)$)}\\
            &= P^\pi(hy) = P^\pi(g).
                    &\mbox{(Definition \ref{pullbackdef})}
    \end{align*}

    Subcase 3.3: $P^\pi(f)\not=0$, $f\not=h$, and
    $f$ has an initial segment $h y_0$ ($y_0\in\mathcal A$).

    Then $\pi(y_0|h)\not=0$, lest
    we would have $P^\pi(f)=0$. Thus:
    \begin{align*}
        P^{\vec w\cdot\vec\pi}(g)
            &= P^{\vec w\cdot \vec\pi}(fy)\\
            &= \vec w\cdot{P^{\vec\pi}}(fy)
                    &\mbox{(Theorem \ref{maintheorem})}\\
            &= w_1\frac{P^\pi(fy)m_1(y_0)}{\pi(y_0|h)}
                +\cdots+w_n\frac{P^\pi(fy)m_n(y_0)}{\pi(y_0|h)}
                    &\mbox{(Lemma \ref{secondtechlemmaforgenericity})}\\
            &= \frac{P^\pi(fy)}{\pi(y_0|h)}(w_1m_1(y_0)+\cdots+w_nm_n(y_0))
                    &\mbox{(Basic Algebra)}\\
            &= \frac{P^\pi(fy)}{\pi(y_0|h)}\pi(y_0|h)
                    &\mbox{($\vec w\cdot\vec m=\pi(\cdot|h)$)}\\
            &= P^\pi(fy) = P^\pi(g).
    \end{align*}

    Subcase 3.4: $P^\pi(f)\not=0$, $f\not=h$, and $f$ has no initial segment
        of the form $hy_0$. Then:
    \begin{align*}
        P^{\vec w\cdot\vec\pi}(g)
            &= P^{\vec w\cdot\vec\pi}(fy)\\
            &= \vec w\cdot{P^{\vec\pi}}(fy)
                    &\mbox{(Theorem \ref{maintheorem})}\\
            &= w_1P^\pi(fy)+\cdots+w_nP^\pi(fy)
                    &\mbox{(Lemma \ref{firsttechlemmaforgenericity})}\\
            &= P^\pi(fy) = P^\pi(g),
                    &\mbox{($w_1+\cdots+w_n=1$)}
    \end{align*}
    as desired.
\end{proof}

\begin{definition}
    Suppose $\pi$ and $\pi_1,\ldots,\pi_n$ are agents and $\Upsilon$ is a
    weighted intelligence measure. We say that $\pi$ \emph{strictly Pareto dominates}
    $\pi_1,\ldots,\pi_n$ (as measured by $\Upsilon$) if the following conditions hold:
    \begin{enumerate}
        \item $\Upsilon(\pi)\geq \Upsilon(\pi_i)$ for each $i=1,\ldots,n$.
        \item $\Upsilon(\pi)>\Upsilon(\pi_i)$ for some $i=1,\ldots,n$.
    \end{enumerate}
\end{definition}

\begin{theorem}
\label{pointwisegenericnessthm}
    (Pointwise Genericness of Non-Deterministic Intelligence)
    Let $\Upsilon$ be any weighted intelligence measure and let
    $\pi$ be an agent.
    Let $h\in (\mathcal E\mathcal A)^*\mathcal E$.
    For any probability distributions $\vec m=(m_1,\ldots,m_n)$ on $\mathcal A$,
    for any positive reals $\vec w=(w_1,\ldots,w_n)$ with $w_1+\cdots+w_n=1$,
    if $\vec w\cdot\vec m=\pi(\cdot|h)$,
    then $\pi$ does not strictly Pareto dominate
    $\pi^{h\mapsto m_1},\ldots,\pi^{h\mapsto m_n}$
    (as measured by $\Upsilon$).
\end{theorem}

\begin{proof}
    Assume $\pi$ strictly Pareto dominates $\pi^{h\mapsto m_1},\ldots,\pi^{h\mapsto m_n}$
    (as measured by $\Upsilon$).
    This implies
    \begin{align*}
        \Upsilon(\pi)
            &= \Upsilon(\vec w\cdot\pi^{h\mapsto\vec m})
                &\mbox{(Proposition \ref{longproposition})}\\
            &= \vec w\cdot\Upsilon(\pi^{h\mapsto\vec m})
                &\mbox{(Theorem \ref{maintheorem})}\\
            &< w_1\Upsilon(\pi)+\cdots+w_n\Upsilon(\pi)
                &\mbox{(Assumption)}\\
            &= \Upsilon(\pi),
                &\mbox{($w_1+\cdots+w_n=1$)}
    \end{align*}
    absurd.
\end{proof}

Theorem \ref{pointwisegenericnessthm} is interesting because it implies that for
any intelligent agent $\pi$, for any history $h$, if $\pi(\cdot|h)$ is not deterministic,
then the optimality of $\pi$ does not critically depend on the specific probabilities
which $\pi(\cdot|h)$ assigns to different actions. For any decomposition
$\pi(\cdot|h)=\vec w\cdot\vec m$ of $\pi(\cdot|h)$ into competing probability
distributions $\vec m$
(with $w_1+\cdots+w_n=1$), either $\pi^{h\mapsto m_i}$ is more intelligent
than $\pi$ for some $i$, or else every $\pi^{h\mapsto m_i}$ has the same intelligence as
$\pi$. This is counter-intuitive because one might imagine that the specific probability
distribution $\pi(\cdot|h)$ was carefully chosen and optimized to maximize the intelligence
of $\pi$.

\begin{example}
\label{genericnessexample}
    Suppose $\pi$ is an agent and $h_0\in(\mathcal E\mathcal A)^*\mathcal E$.
    Assume $\mathcal A=\{y_1,y_2,y_3\}$.
    Suppose $\pi(y|h_0)=\frac13$ for every $y\in\mathcal A$.
    Define probability distributions $\vec m=(m_1,m_2,m_3)$ on $\mathcal A$ by
    \begin{align*}
        m_1(y) &=
        \begin{cases}
            \frac16 &\mbox{if $y\in\{y_1,y_2\}$}\\
            \frac23 &\mbox{if $y=y_3$}
        \end{cases}\\
        m_2(y) &=
        \begin{cases}
            \frac16 &\mbox{if $y\in\{y_1,y_3\}$}\\
            \frac23 &\mbox{if $y=y_2$}
        \end{cases}\\
        m_3(y) &=
        \begin{cases}
            \frac16 &\mbox{if $y\in\{y_2,y_3\}$}\\
            \frac23 &\mbox{if $y=y_1$}
        \end{cases}
    \end{align*}
    Let $\vec w=(w_1,w_2,w_3)=(\frac13,\frac13,\frac13)$. Then $w_1+w_2+w_3=1$ and
    it is easy to check
    $(\vec w\cdot\vec m)(y)=\frac13$ for every $y\in\mathcal A$,
    i.e., $\vec w\cdot\vec m=\pi(\cdot|h_0)$.
    By Theorem \ref{pointwisegenericnessthm},
    for any weighted intelligence measure $\Upsilon$,
    $\pi$ does not strictly Pareto dominate (as measured by $\Upsilon$)
    the set of agents obtained by changing the value of $\pi(\cdot|h_0)$ to $m_i$
    ($i=1,\ldots,n$).
\end{example}

In Example \ref{genericnessexample}, Theorem \ref{pointwisegenericnessthm} shows
that the optimality of $\pi$ cannot crucially depend on the specific values
$\pi(y_1|h_0)=\pi(y_2|h_0)=\pi(y_3|h_0)=\frac13$ of $\pi$.
This is counter-intuitive because, a priori, it seems
plausible that these values could have been chosen
in order to maximize $\pi$'s intelligence. We could imagine ourselves
saying: ``These particular values $\pi(y_1|h_0)=\pi(y_2|h_0)=\pi(y_3|h_0)=\frac13$ are
critical to $\pi$'s
performance. Any other values would make $\pi$ sub-optimal.''
Theorem \ref{pointwisegenericnessthm} shows that no such statement can be true.

\begin{corollary}
\label{nondeterminismcorollary}
    (Pointwise Unnecessariness of Non-Determinism)
    Suppose $\Upsilon$ is a weighted intelligence measure, $\pi$ is an agent,
    and $h_0\in(\mathcal E\mathcal A)^*\mathcal E$.
    Let $y_1,\ldots,y_n$ enumerate $\{y\in\mathcal A\,:\,\pi(y|h_0)\not=0\}$.
    For each $i=1,\ldots,n$, let $m_i$ be the
    deterministic $\mathcal A$-probability distribution
    \[
        m_i(y) = \begin{cases}
            1 &\mbox{if $y=y_i$,}\\
            0 &\mbox{if $y\not=y_i$.}
        \end{cases}
    \]
    Then $\pi$ does not strictly Pareto dominate
    $\pi^{h_0\mapsto m_0},\ldots,\pi^{h_0\mapsto m_n}$
    (as measured by $\Upsilon$).
\end{corollary}

\begin{proof}
    By Theorem \ref{pointwisegenericnessthm} with
    $\vec m=(m_1,\ldots,m_n)$ and $\vec w=(w_1,\ldots,w_n)$ where each
    $w_i=\pi(a_i|h_0)$.
\end{proof}

Corollary \ref{nondeterminismcorollary} shows that for any individual
history $h_0$, the optimality of $\pi$ cannot crucially depend on $\pi(\cdot|h_0)$
being non-deterministic. This is counter-intuitive because we could imagine saying:
``In response to such-and-such history, it would be optimal for our agent
to assume the environment is such-and-such environment where it is optimal to
assign uniform probabilities to every action;
certainly, it would be sub-optimal for our agent to instead assign probability $1$ to
any particular action.'' Corollary \ref{nondeterminismcorollary} shows
that no such statement can be true.

The ``pointwise'' nature of Corollary \ref{nondeterminismcorollary}
is essential: by applying the corollary repeatedly, one can remove non-determinism
from $\pi$ at any finite number of points without ever decreasing $\pi$'s intelligence,
but one cannot conclude from this that non-determinism can be removed at infinitely
many points without decreasing $\pi$'s intelligence. The following example makes this
clear.

\begin{example}
    (Intelligence Discontinuity)
    Fix distinct actions $a_0,a_1\in\mathcal A$.
    Let $\mu$ be an environment such that in every agent-environment interaction:
    \begin{enumerate}
        \item If the agent always takes action $a_0$, then the agent gets total reward $-1$.
        \item If the agent initially takes action $a_0$ exactly $k$ times and
            then takes a different action, then the agent either
            gets total reward $1$ (with probability $1-2^{-k}$)
            or total reward $0$ (with probability $2^{-k}$)---so in expected
            value, the agent gets total reward $1-2^{-k}$.
    \end{enumerate}
    Let $\Upsilon$ be a weighted intelligence measure in which $\mu$ has weight $2$,
    $\overline\mu$ has weight $1$, and such that for every
    environment $\nu\not\in\{\mu,\overline{\mu}\}$, $\nu$ and $\overline{\nu}$ have
    equal weight.
    By Corollary 6 of \cite{alexander2021reward}, for any self-dual agent $\pi$ and
    any environment $\nu$, $V^\pi_{\overline\nu}=-V^\pi_{\nu}$, thus for any
    self-dual agent $\pi$,
    in the sum $\Upsilon(\pi)$, for any environment $\nu\not\in\{\mu,\overline{\mu}\}$,
    the contributions from $\nu$ and $\overline\nu$ are equal-weight multiples of
    $V^\pi_{\nu}$ and $-V^\pi_{\nu}$,
    respectively, and so cancel each other
    (terms of the infinite series can be regrouped because
    Condition 1 of Definition \ref{performanceaveragerdefn} implies
    the series defining $\Upsilon(\pi)$ is absolutely convergent).
    Thus for any self-dual agent $\pi$,
    $\Upsilon(\pi)$ consists solely of the contributions from $\mu$ and $\overline\mu$,
    i.e.,
    \[
        \Upsilon(\pi)=2V^\pi_\mu+1V^\pi_{\overline\mu}
        =2V^\pi_\mu-1V^\pi_{\mu}=V^\pi_\mu.
    \]
    For every $k\in\mathbb N$, let $\pi_k$ be the agent which ignores the environment,
    blindly taking action $a_0$ exactly $k$ times, and then forever thereafter,
    randomly taking action $a_0$ with probability $1/2$ or action $a_1$ with probability
    $1/2$.
    For every $k$,
    \begin{align*}
        V^{\pi_k}_\mu
            &= (1-2^{-(k+0)})\cdot \mbox{$\frac12$}
                +(1-2^{-(k+1)})\cdot\mbox{$\frac14$} + \cdots
                    &\mbox{(Basic probability)}\\
            &= 1-2^{-k},
                    &\mbox{(Geometric series)}
    \end{align*}
    so $\Upsilon(\pi_k)=1-2^{-k}$ (as $\pi_k$ is self-dual).
    Thus:
    \begin{enumerate}
        \item For each $k$, transforming $\pi_k$ into $\pi_{k+1}$
            (by changing finitely many $50\%$-probability-$a_0$
            actions into $100\%$-probability-$a_0$ actions)
            \emph{increases} intelligence.
        \item However, if we start this process with $\pi_0$ and
            repeat it to infinity, even though each individual step
            \emph{increases} intelligence, we finally end up with
            the completely deterministic
            self-dual agent $\pi_{\infty}$ who always takes action $a_0$ and who is
            \emph{less} intelligent than each $\pi_k$:
            $\Upsilon(\pi_{\infty})=V^{\pi_\infty}_\mu=-1$.
    \end{enumerate}
\end{example}

\bibliographystyle{alpha} % Not for COLT
\bibliography{main}

\appendix

\section{My Proof of Theorem 1}

This is a boring technical proof. ( Copied from template )

\section{My Proof of Theorem 2}

This is a complete version of a proof sketched in the main text.
( Copied from template )

 Alternative proofs of the same theorem can be added here as a few previous COLT papers did this.

\end{document}
